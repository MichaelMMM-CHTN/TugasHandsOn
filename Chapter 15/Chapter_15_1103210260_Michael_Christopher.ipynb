{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Chapter 15: Processing Sequences Using RNNs and CNNs\n",
        "\n",
        "## Comprehensive Study Guide with Theory, Implementation, and Exercises\n",
        "\n",
        "### Based on \"Hands-On Machine Learning\" by Aurélien Géron\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. **Introduction to Sequence Processing**\n",
        "2. **Recurrent Neurons and Layers**\n",
        "3. **Training RNNs - Backpropagation Through Time**\n",
        "4. **Time Series Forecasting**\n",
        "5. **Handling Long Sequences**\n",
        "6. **LSTM and GRU Cells**\n",
        "7. **1D Convolutional Layers for Sequences**\n",
        "8. **WaveNet Architecture**\n",
        "9. **Exercises and Solutions**\n",
        "10. **Mathematical Foundations**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "## Setup and Imports\n",
        "\n",
        "First, let's set up our environment with all necessary imports and configurations for Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-imports"
      },
      "source": [
        "# Google Colab setup\n",
        "!pip install -q tensorflow-datasets\n",
        "\n",
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Configuration\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Plotting configuration\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-theory"
      },
      "source": [
        "## 1. Introduction to Sequence Processing\n",
        "\n",
        "### Theoretical Foundation\n",
        "\n",
        "Sequences are everywhere in machine learning:\n",
        "- **Time series**: Stock prices, weather data, sensor readings\n",
        "- **Natural language**: Sentences, documents, speech\n",
        "- **Biological sequences**: DNA, protein sequences\n",
        "- **Video**: Sequences of frames\n",
        "\n",
        "**Key Challenge**: Traditional neural networks expect fixed-size inputs, but sequences have variable lengths and temporal dependencies.\n",
        "\n",
        "**Solution**: Recurrent Neural Networks (RNNs) can:\n",
        "1. Process sequences of arbitrary length\n",
        "2. Maintain memory of previous inputs\n",
        "3. Share parameters across time steps\n",
        "\n",
        "### Mathematical Motivation\n",
        "\n",
        "For a sequence $\\mathbf{x} = (x_1, x_2, ..., x_T)$, we want to model:\n",
        "\n",
        "$$P(x_1, x_2, ..., x_T) = \\prod_{t=1}^{T} P(x_t | x_1, ..., x_{t-1})$$\n",
        "\n",
        "This requires a model that can capture dependencies between distant time steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnn-theory"
      },
      "source": [
        "## 2. Recurrent Neurons and Layers\n",
        "\n",
        "### Basic RNN Architecture\n",
        "\n",
        "A recurrent neuron at time step $t$ receives:\n",
        "- Input vector $\\mathbf{x}^{(t)}$\n",
        "- Previous output $\\mathbf{y}^{(t-1)}$ (which becomes the hidden state)\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "For a single recurrent neuron:\n",
        "$$y^{(t)} = \\phi(\\mathbf{w}_x^T \\mathbf{x}^{(t)} + \\mathbf{w}_y^T \\mathbf{y}^{(t-1)} + b)$$\n",
        "\n",
        "For a layer of recurrent neurons:\n",
        "$$\\mathbf{y}^{(t)} = \\phi(\\mathbf{W}_x \\mathbf{x}^{(t)} + \\mathbf{W}_y \\mathbf{y}^{(t-1)} + \\mathbf{b})$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{W}_x$: weight matrix for inputs (shape: $n_{inputs} \\times n_{neurons}$)\n",
        "- $\\mathbf{W}_y$: weight matrix for previous outputs (shape: $n_{neurons} \\times n_{neurons}$)\n",
        "- $\\mathbf{b}$: bias vector\n",
        "- $\\phi$: activation function (typically tanh)\n",
        "\n",
        "### Memory Cells\n",
        "\n",
        "A memory cell preserves state across time steps:\n",
        "- **Hidden state** $\\mathbf{h}^{(t)}$: internal state of the cell\n",
        "- **Output** $\\mathbf{y}^{(t)}$: what the cell outputs (may equal hidden state)\n",
        "\n",
        "General form:\n",
        "$$\\mathbf{h}^{(t)} = f(\\mathbf{h}^{(t-1)}, \\mathbf{x}^{(t)})$$\n",
        "$$\\mathbf{y}^{(t)} = g(\\mathbf{h}^{(t)})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnn-implementation"
      },
      "source": [
        "# Demonstration of basic RNN concepts\n",
        "\n",
        "def visualize_rnn_unrolling():\n",
        "    \"\"\"\n",
        "    This function creates a visualization showing how an RNN is 'unrolled' through time.\n",
        "    It demonstrates the concept that the same RNN cell is applied at each time step.\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Folded RNN (left)\n",
        "    ax1.set_title(\"Folded RNN\", fontsize=14, fontweight='bold')\n",
        "    ax1.text(0.5, 0.7, \"RNN\\nCell\", ha='center', va='center',\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue'),\n",
        "             fontsize=12)\n",
        "    ax1.arrow(0.3, 0.4, 0, 0.2, head_width=0.02, head_length=0.02, fc='black')\n",
        "    ax1.text(0.3, 0.3, \"x(t)\", ha='center', fontsize=10)\n",
        "    ax1.arrow(0.5, 0.9, 0, 0.08, head_width=0.02, head_length=0.02, fc='black')\n",
        "    ax1.text(0.5, 1.0, \"y(t)\", ha='center', fontsize=10)\n",
        "    # Self-loop for recurrence\n",
        "    circle = plt.Circle((0.7, 0.7), 0.1, fill=False, linestyle='--')\n",
        "    ax1.add_patch(circle)\n",
        "    ax1.arrow(0.8, 0.7, 0.1, 0, head_width=0.02, head_length=0.02, fc='red')\n",
        "    ax1.text(0.95, 0.7, \"y(t-1)\", ha='left', fontsize=10, color='red')\n",
        "    ax1.set_xlim(0, 1.2)\n",
        "    ax1.set_ylim(0, 1.2)\n",
        "    ax1.axis('off')\n",
        "\n",
        "    # Unrolled RNN (right)\n",
        "    ax2.set_title(\"Unrolled RNN Through Time\", fontsize=14, fontweight='bold')\n",
        "    time_steps = 4\n",
        "    for t in range(time_steps):\n",
        "        x_pos = 0.2 + t * 0.2\n",
        "        # RNN cell\n",
        "        ax2.text(x_pos, 0.5, f\"RNN\\nCell\", ha='center', va='center',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.15\", facecolor='lightblue'),\n",
        "                fontsize=8)\n",
        "        # Input\n",
        "        ax2.arrow(x_pos, 0.3, 0, 0.1, head_width=0.01, head_length=0.01, fc='black')\n",
        "        ax2.text(x_pos, 0.25, f\"x({t})\", ha='center', fontsize=8)\n",
        "        # Output\n",
        "        ax2.arrow(x_pos, 0.65, 0, 0.1, head_width=0.01, head_length=0.01, fc='black')\n",
        "        ax2.text(x_pos, 0.8, f\"y({t})\", ha='center', fontsize=8)\n",
        "        # Connection to next time step\n",
        "        if t < time_steps - 1:\n",
        "            ax2.arrow(x_pos + 0.05, 0.5, 0.1, 0, head_width=0.02, head_length=0.01, fc='red')\n",
        "\n",
        "    ax2.text(0.5, 0.1, \"Time →\", ha='center', fontsize=12, fontweight='bold')\n",
        "    ax2.set_xlim(0, 1)\n",
        "    ax2.set_ylim(0, 1)\n",
        "    ax2.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_rnn_unrolling()\n",
        "\n",
        "print(\"Key Concepts Illustrated:\")\n",
        "print(\"1. Same RNN cell is reused at each time step\")\n",
        "print(\"2. Hidden state flows from one time step to the next\")\n",
        "print(\"3. Unrolling reveals the deep architecture through time\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "input-output-sequences"
      },
      "source": [
        "### Input and Output Sequences\n",
        "\n",
        "RNNs can handle different sequence patterns:\n",
        "\n",
        "1. **Sequence-to-Sequence**: Input sequence → Output sequence\n",
        "   - Example: Time series forecasting\n",
        "   - Use case: Predict next N values given previous M values\n",
        "\n",
        "2. **Sequence-to-Vector**: Input sequence → Single output\n",
        "   - Example: Sentiment analysis\n",
        "   - Use case: Classify entire sequence\n",
        "\n",
        "3. **Vector-to-Sequence**: Single input → Output sequence\n",
        "   - Example: Image captioning\n",
        "   - Use case: Generate sequence from single input\n",
        "\n",
        "4. **Encoder-Decoder**: Sequence → Vector → Sequence\n",
        "   - Example: Machine translation\n",
        "   - Use case: Transform one sequence type to another\n",
        "\n",
        "### Practical Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sequence-types-demo"
      },
      "source": [
        "# Demonstration of different sequence types with simple examples\n",
        "\n",
        "def create_sample_data():\n",
        "    \"\"\"\n",
        "    Creates sample data for different sequence processing tasks.\n",
        "    This helps understand the input/output shapes for different RNN architectures.\n",
        "    \"\"\"\n",
        "    # Sample time series data\n",
        "    time_steps = 50\n",
        "    n_samples = 1000\n",
        "\n",
        "    # Generate synthetic time series\n",
        "    np.random.seed(42)\n",
        "    t = np.linspace(0, 4*np.pi, time_steps)\n",
        "    series = []\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Different frequencies and phases for variety\n",
        "        freq1 = np.random.uniform(0.5, 2.0)\n",
        "        freq2 = np.random.uniform(0.1, 0.5)\n",
        "        phase1 = np.random.uniform(0, 2*np.pi)\n",
        "        phase2 = np.random.uniform(0, 2*np.pi)\n",
        "\n",
        "        # Combine sine waves with noise\n",
        "        signal = (0.5 * np.sin(freq1 * t + phase1) +\n",
        "                 0.3 * np.sin(freq2 * t + phase2) +\n",
        "                 0.1 * np.random.randn(time_steps))\n",
        "        series.append(signal)\n",
        "\n",
        "    return np.array(series)\n",
        "\n",
        "def demonstrate_sequence_types():\n",
        "    \"\"\"\n",
        "    Shows the data shapes and use cases for different sequence processing tasks.\n",
        "    \"\"\"\n",
        "    series_data = create_sample_data()\n",
        "\n",
        "    print(\"Original time series shape:\", series_data.shape)\n",
        "    print(\"Format: (samples, time_steps)\\n\")\n",
        "\n",
        "    # 1. Sequence-to-Sequence: Forecast next 10 steps\n",
        "    window_size = 30\n",
        "    forecast_steps = 10\n",
        "\n",
        "    X_seq2seq = []\n",
        "    y_seq2seq = []\n",
        "\n",
        "    for series in series_data:\n",
        "        for i in range(len(series) - window_size - forecast_steps + 1):\n",
        "            X_seq2seq.append(series[i:i+window_size])\n",
        "            y_seq2seq.append(series[i+window_size:i+window_size+forecast_steps])\n",
        "\n",
        "    X_seq2seq = np.array(X_seq2seq).reshape(-1, window_size, 1)\n",
        "    y_seq2seq = np.array(y_seq2seq)\n",
        "\n",
        "    print(\"1. SEQUENCE-TO-SEQUENCE (Time Series Forecasting)\")\n",
        "    print(f\"   Input shape: {X_seq2seq.shape} (samples, time_steps, features)\")\n",
        "    print(f\"   Output shape: {y_seq2seq.shape} (samples, forecast_steps)\")\n",
        "    print(f\"   Use case: Given {window_size} past values, predict next {forecast_steps} values\\n\")\n",
        "\n",
        "    # 2. Sequence-to-Vector: Classify trend\n",
        "    X_seq2vec = X_seq2seq[:1000]  # Use subset\n",
        "    # Create binary classification: upward vs downward trend\n",
        "    y_seq2vec = (X_seq2vec[:, -1, 0] > X_seq2vec[:, 0, 0]).astype(int)\n",
        "\n",
        "    print(\"2. SEQUENCE-TO-VECTOR (Trend Classification)\")\n",
        "    print(f\"   Input shape: {X_seq2vec.shape} (samples, time_steps, features)\")\n",
        "    print(f\"   Output shape: {y_seq2vec.shape} (samples,)\")\n",
        "    print(f\"   Use case: Classify if sequence shows upward (1) or downward (0) trend\\n\")\n",
        "\n",
        "    # 3. Vector-to-Sequence: Generate sequence from initial condition\n",
        "    X_vec2seq = series_data[:100, 0].reshape(-1, 1)  # Just first value\n",
        "    y_vec2seq = series_data[:100, 1:21]  # Next 20 values\n",
        "\n",
        "    print(\"3. VECTOR-TO-SEQUENCE (Sequence Generation)\")\n",
        "    print(f\"   Input shape: {X_vec2seq.shape} (samples, features)\")\n",
        "    print(f\"   Output shape: {y_vec2seq.shape} (samples, sequence_length)\")\n",
        "    print(f\"   Use case: Generate sequence given initial condition\\n\")\n",
        "\n",
        "    # Visualize examples\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Original series\n",
        "    axes[0, 0].plot(series_data[0])\n",
        "    axes[0, 0].set_title(\"Original Time Series\")\n",
        "    axes[0, 0].set_xlabel(\"Time Step\")\n",
        "    axes[0, 0].set_ylabel(\"Value\")\n",
        "\n",
        "    # Sequence-to-sequence example\n",
        "    example_idx = 5\n",
        "    input_seq = X_seq2seq[example_idx, :, 0]\n",
        "    target_seq = y_seq2seq[example_idx]\n",
        "\n",
        "    axes[0, 1].plot(range(len(input_seq)), input_seq, 'b-', label='Input', linewidth=2)\n",
        "    axes[0, 1].plot(range(len(input_seq), len(input_seq) + len(target_seq)),\n",
        "                   target_seq, 'r-', label='Target', linewidth=2)\n",
        "    axes[0, 1].axvline(x=len(input_seq)-1, color='gray', linestyle='--', alpha=0.7)\n",
        "    axes[0, 1].set_title(\"Sequence-to-Sequence Example\")\n",
        "    axes[0, 1].set_xlabel(\"Time Step\")\n",
        "    axes[0, 1].set_ylabel(\"Value\")\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "    # Trend distribution\n",
        "    axes[1, 0].hist(y_seq2vec, bins=2, alpha=0.7, edgecolor='black')\n",
        "    axes[1, 0].set_title(\"Trend Classification Distribution\")\n",
        "    axes[1, 0].set_xlabel(\"Trend (0=Down, 1=Up)\")\n",
        "    axes[1, 0].set_ylabel(\"Count\")\n",
        "    axes[1, 0].set_xticks([0, 1])\n",
        "\n",
        "    # Vector-to-sequence example\n",
        "    vec_example = 5\n",
        "    initial_val = X_vec2seq[vec_example, 0]\n",
        "    generated_seq = y_vec2seq[vec_example]\n",
        "\n",
        "    axes[1, 1].scatter(0, initial_val, color='red', s=100, label='Initial Value', zorder=5)\n",
        "    axes[1, 1].plot(range(1, len(generated_seq)+1), generated_seq, 'b-',\n",
        "                   label='Generated Sequence', linewidth=2)\n",
        "    axes[1, 1].set_title(\"Vector-to-Sequence Example\")\n",
        "    axes[1, 1].set_xlabel(\"Time Step\")\n",
        "    axes[1, 1].set_ylabel(\"Value\")\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return X_seq2seq, y_seq2seq, X_seq2vec, y_seq2vec, X_vec2seq, y_vec2seq\n",
        "\n",
        "# Run the demonstration\n",
        "X_seq2seq, y_seq2seq, X_seq2vec, y_seq2vec, X_vec2seq, y_vec2seq = demonstrate_sequence_types()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bptt-theory"
      },
      "source": [
        "## 3. Training RNNs - Backpropagation Through Time (BPTT)\n",
        "\n",
        "### Theoretical Foundation\n",
        "\n",
        "Training RNNs requires a modified version of backpropagation called **Backpropagation Through Time (BPTT)**.\n",
        "\n",
        "### The Process\n",
        "\n",
        "1. **Forward Pass**: Unroll the RNN and compute outputs for all time steps\n",
        "2. **Compute Loss**: Calculate loss using all relevant outputs\n",
        "3. **Backward Pass**: Propagate gradients backward through time\n",
        "4. **Update Parameters**: Use accumulated gradients to update weights\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "For an RNN with hidden state $\\mathbf{h}^{(t)} = f(\\mathbf{W}\\mathbf{x}^{(t)} + \\mathbf{U}\\mathbf{h}^{(t-1)} + \\mathbf{b})$:\n",
        "\n",
        "**Forward Pass:**\n",
        "$$\\mathbf{h}^{(t)} = f(\\mathbf{W}\\mathbf{x}^{(t)} + \\mathbf{U}\\mathbf{h}^{(t-1)} + \\mathbf{b})$$\n",
        "$$\\mathbf{y}^{(t)} = g(\\mathbf{V}\\mathbf{h}^{(t)} + \\mathbf{c})$$\n",
        "\n",
        "**Loss Function:**\n",
        "$$L = \\sum_{t=1}^{T} L^{(t)}(\\mathbf{y}^{(t)}, \\hat{\\mathbf{y}}^{(t)})$$\n",
        "\n",
        "**Gradient Computation:**\n",
        "The gradient of the loss with respect to $\\mathbf{U}$ involves the chain rule across time:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\mathbf{U}} = \\sum_{t=1}^{T} \\sum_{k=1}^{t} \\frac{\\partial L^{(t)}}{\\partial \\mathbf{h}^{(t)}} \\frac{\\partial \\mathbf{h}^{(t)}}{\\partial \\mathbf{h}^{(k)}} \\frac{\\partial \\mathbf{h}^{(k)}}{\\partial \\mathbf{U}}$$\n",
        "\n",
        "Where:\n",
        "$$\\frac{\\partial \\mathbf{h}^{(t)}}{\\partial \\mathbf{h}^{(k)}} = \\prod_{i=k+1}^{t} \\frac{\\partial \\mathbf{h}^{(i)}}{\\partial \\mathbf{h}^{(i-1)}} = \\prod_{i=k+1}^{t} \\mathbf{U}^T \\text{diag}(f'(\\mathbf{a}^{(i)}))$$\n",
        "\n",
        "### Challenges\n",
        "\n",
        "1. **Vanishing Gradients**: $\\prod_{i=k+1}^{t} \\mathbf{U}^T \\text{diag}(f'(\\mathbf{a}^{(i)}))$ can become very small\n",
        "2. **Exploding Gradients**: The product can become very large\n",
        "3. **Computational Complexity**: $O(T)$ memory and computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bptt-implementation"
      },
      "source": [
        "# Demonstration of BPTT concepts and gradient flow\n",
        "\n",
        "def demonstrate_gradient_flow():\n",
        "    \"\"\"\n",
        "    This function demonstrates how gradients flow through time in RNNs\n",
        "    and illustrates the vanishing gradient problem.\n",
        "    \"\"\"\n",
        "    # Simulate gradient magnitudes through time\n",
        "    time_steps = 20\n",
        "\n",
        "    # Different scenarios\n",
        "    scenarios = {\n",
        "        'Vanishing (tanh, small weights)': {\n",
        "            'weight_scale': 0.5,\n",
        "            'activation': 'tanh'\n",
        "        },\n",
        "        'Exploding (tanh, large weights)': {\n",
        "            'weight_scale': 2.0,\n",
        "            'activation': 'tanh'\n",
        "        },\n",
        "        'Stable (proper initialization)': {\n",
        "            'weight_scale': 1.0,\n",
        "            'activation': 'tanh'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    for idx, (scenario_name, params) in enumerate(scenarios.items()):\n",
        "        gradients = [1.0]  # Start with gradient of 1.0 at the end\n",
        "\n",
        "        # Simulate backward propagation\n",
        "        for t in range(time_steps - 1):\n",
        "            # Simplified gradient computation\n",
        "            # For tanh: derivative is approximately 1 - tanh²(x)\n",
        "            if params['activation'] == 'tanh':\n",
        "                activation_grad = 0.25  # Average derivative for tanh\n",
        "\n",
        "            # Gradient through recurrent connection\n",
        "            weight_contribution = params['weight_scale']\n",
        "            new_gradient = gradients[-1] * weight_contribution * activation_grad\n",
        "            gradients.append(new_gradient)\n",
        "\n",
        "        # Plot gradients (reverse order for backward flow)\n",
        "        time_points = list(range(time_steps, 0, -1))\n",
        "        axes[idx].semilogy(time_points, gradients, 'o-', linewidth=2, markersize=6)\n",
        "        axes[idx].set_title(f'{scenario_name}', fontweight='bold')\n",
        "        axes[idx].set_xlabel('Time Steps Back')\n",
        "        axes[idx].set_ylabel('Gradient Magnitude (log scale)')\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "        axes[idx].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Initial gradient')\n",
        "\n",
        "        # Add interpretation\n",
        "        final_gradient = gradients[-1]\n",
        "        if final_gradient < 0.01:\n",
        "            interpretation = \"Vanishing!\"\n",
        "            color = 'red'\n",
        "        elif final_gradient > 100:\n",
        "            interpretation = \"Exploding!\"\n",
        "            color = 'orange'\n",
        "        else:\n",
        "            interpretation = \"Stable\"\n",
        "            color = 'green'\n",
        "\n",
        "        axes[idx].text(0.7, 0.1, f'Final: {final_gradient:.6f}\\n{interpretation}',\n",
        "                      transform=axes[idx].transAxes,\n",
        "                      bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.3),\n",
        "                      fontweight='bold')\n",
        "\n",
        "        if idx == 0:\n",
        "            axes[idx].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Gradient Flow Analysis:\")\n",
        "    print(\"• Vanishing: Gradients decrease exponentially → early layers don't learn\")\n",
        "    print(\"• Exploding: Gradients increase exponentially → unstable training\")\n",
        "    print(\"• Stable: Gradients remain in reasonable range → effective learning\")\n",
        "\n",
        "def implement_simple_bptt():\n",
        "    \"\"\"\n",
        "    Demonstrates the BPTT algorithm with a simple RNN implementation.\n",
        "    This is educational - in practice, use TensorFlow's automatic differentiation.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SIMPLE BPTT IMPLEMENTATION (Educational)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Simple RNN for binary sequence classification\n",
        "    class SimpleRNN:\n",
        "        def __init__(self, input_size, hidden_size, output_size):\n",
        "            # Initialize weights (small values to avoid exploding gradients)\n",
        "            self.Wxh = np.random.randn(input_size, hidden_size) * 0.1\n",
        "            self.Whh = np.random.randn(hidden_size, hidden_size) * 0.1\n",
        "            self.Why = np.random.randn(hidden_size, output_size) * 0.1\n",
        "            self.bh = np.zeros((1, hidden_size))\n",
        "            self.by = np.zeros((1, output_size))\n",
        "\n",
        "        def forward(self, X):\n",
        "            \"\"\"\n",
        "            Forward pass through the RNN\n",
        "            X: input sequence (seq_len, input_size)\n",
        "            \"\"\"\n",
        "            seq_len, _ = X.shape\n",
        "\n",
        "            # Store activations for backward pass\n",
        "            self.h = np.zeros((seq_len + 1, self.Whh.shape[0]))\n",
        "            self.y = np.zeros((seq_len, self.Why.shape[1]))\n",
        "\n",
        "            # Forward through time\n",
        "            for t in range(seq_len):\n",
        "                self.h[t+1] = np.tanh(X[t:t+1] @ self.Wxh + self.h[t:t+1] @ self.Whh + self.bh)\n",
        "                self.y[t] = self.h[t+1] @ self.Why + self.by\n",
        "\n",
        "            return self.y\n",
        "\n",
        "        def backward(self, X, y_true, learning_rate=0.01):\n",
        "            \"\"\"\n",
        "            Backward pass (BPTT)\n",
        "            \"\"\"\n",
        "            seq_len = X.shape[0]\n",
        "\n",
        "            # Initialize gradients\n",
        "            dWxh = np.zeros_like(self.Wxh)\n",
        "            dWhh = np.zeros_like(self.Whh)\n",
        "            dWhy = np.zeros_like(self.Why)\n",
        "            dbh = np.zeros_like(self.bh)\n",
        "            dby = np.zeros_like(self.by)\n",
        "\n",
        "            # Gradient w.r.t. output\n",
        "            dy = self.y - y_true\n",
        "\n",
        "            # Backward through time\n",
        "            dh_next = np.zeros_like(self.h[0:1])\n",
        "\n",
        "            for t in reversed(range(seq_len)):\n",
        "                # Output layer gradients\n",
        "                dWhy += self.h[t+1].T @ dy[t:t+1]\n",
        "                dby += dy[t:t+1]\n",
        "\n",
        "                # Hidden layer gradients\n",
        "                dh = dy[t:t+1] @ self.Why.T + dh_next\n",
        "                dh_raw = (1 - self.h[t+1] ** 2) * dh  # tanh derivative\n",
        "\n",
        "                # Weight gradients\n",
        "                dWxh += X[t:t+1].T @ dh_raw\n",
        "                dWhh += self.h[t:t+1].T @ dh_raw\n",
        "                dbh += dh_raw\n",
        "\n",
        "                # Gradient for next iteration\n",
        "                dh_next = dh_raw @ self.Whh.T\n",
        "\n",
        "            # Update weights\n",
        "            self.Wxh -= learning_rate * dWxh\n",
        "            self.Whh -= learning_rate * dWhh\n",
        "            self.Why -= learning_rate * dWhy\n",
        "            self.bh -= learning_rate * dbh\n",
        "            self.by -= learning_rate * dby\n",
        "\n",
        "            return np.mean(dy ** 2)  # MSE loss\n",
        "\n",
        "    # Create sample data\n",
        "    seq_len = 10\n",
        "    input_size = 1\n",
        "    hidden_size = 5\n",
        "    output_size = 1\n",
        "\n",
        "    # Simple task: sum of sequence\n",
        "    X = np.random.randn(seq_len, input_size)\n",
        "    y_true = np.array([[np.sum(X)]] * seq_len)  # Target is sum at each step\n",
        "\n",
        "    # Initialize RNN\n",
        "    rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "    print(f\"Training simple RNN on sequence summation task...\")\n",
        "    print(f\"Input sequence length: {seq_len}\")\n",
        "    print(f\"Hidden size: {hidden_size}\")\n",
        "    print(f\"Target: sum of sequence = {np.sum(X):.3f}\")\n",
        "\n",
        "    # Training loop\n",
        "    losses = []\n",
        "    for epoch in range(100):\n",
        "        y_pred = rnn.forward(X)\n",
        "        loss = rnn.backward(X, y_true, learning_rate=0.01)\n",
        "        losses.append(loss)\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch:3d}: Loss = {loss:.6f}, Final prediction = {y_pred[-1, 0]:.3f}\")\n",
        "\n",
        "    # Plot training progress\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(losses)\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.yscale('log')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    final_pred = rnn.forward(X)\n",
        "    plt.plot(range(seq_len), y_true[:, 0], 'b-', label='Target', linewidth=2)\n",
        "    plt.plot(range(seq_len), final_pred[:, 0], 'r--', label='Prediction', linewidth=2)\n",
        "    plt.title('Final Predictions vs Targets')\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run demonstrations\n",
        "demonstrate_gradient_flow()\n",
        "implement_simple_bptt()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "time-series-theory"
      },
      "source": [
        "## 4. Time Series Forecasting\n",
        "\n",
        "### Theoretical Background\n",
        "\n",
        "Time series forecasting involves predicting future values based on past observations. The goal is to model:\n",
        "\n",
        "$$x_{t+h} = f(x_t, x_{t-1}, ..., x_{t-n+1}) + \\epsilon_{t+h}$$\n",
        "\n",
        "Where:\n",
        "- $h$ is the forecast horizon\n",
        "- $n$ is the lookback window\n",
        "- $\\epsilon$ is the error term\n",
        "\n",
        "### Types of Time Series\n",
        "\n",
        "1. **Univariate**: Single variable (e.g., stock price)\n",
        "2. **Multivariate**: Multiple variables (e.g., weather data with temperature, humidity, pressure)\n",
        "\n",
        "### Key Challenges\n",
        "\n",
        "1. **Trend**: Long-term increase or decrease\n",
        "2. **Seasonality**: Regular patterns (daily, weekly, yearly)\n",
        "3. **Stationarity**: Statistical properties change over time\n",
        "4. **Noise**: Random fluctuations\n",
        "\n",
        "### Baseline Methods\n",
        "\n",
        "Before using complex models, establish baselines:\n",
        "\n",
        "1. **Naive Forecast**: $\\hat{x}_{t+1} = x_t$\n",
        "2. **Moving Average**: $\\hat{x}_{t+1} = \\frac{1}{k}\\sum_{i=0}^{k-1} x_{t-i}$\n",
        "3. **Linear Regression**: $\\hat{x}_{t+1} = \\alpha + \\beta t$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "time-series-generation"
      },
      "source": [
        "# Time series generation function (as described in the book)\n",
        "\n",
        "def generate_time_series(batch_size, n_steps):\n",
        "    \"\"\"\n",
        "    Generates synthetic time series as described in the book.\n",
        "    Each series is a combination of two sine waves with different frequencies\n",
        "    and phases, plus some noise.\n",
        "\n",
        "    Args:\n",
        "        batch_size: Number of time series to generate\n",
        "        n_steps: Number of time steps in each series\n",
        "\n",
        "    Returns:\n",
        "        NumPy array of shape [batch_size, n_steps, 1]\n",
        "    \"\"\"\n",
        "    # Generate random frequencies and offsets for each series\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "\n",
        "    # Create time axis\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "\n",
        "    # Generate series as sum of two sine waves plus noise\n",
        "    series = (0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) +     # wave 1\n",
        "              0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) +     # wave 2\n",
        "              0.1 * (np.random.rand(batch_size, n_steps) - 0.5))        # noise\n",
        "\n",
        "    return series[..., np.newaxis].astype(np.float32)\n",
        "\n",
        "def demonstrate_time_series_concepts():\n",
        "    \"\"\"\n",
        "    Demonstrates key time series concepts with visualizations.\n",
        "    \"\"\"\n",
        "    print(\"DEMONSTRATING TIME SERIES CONCEPTS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Generate sample data\n",
        "    n_steps = 50\n",
        "    series = generate_time_series(10000, n_steps + 1)\n",
        "\n",
        "    # Split into train/validation/test\n",
        "    X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "    X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "    X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
        "\n",
        "    print(f\"Dataset shapes:\")\n",
        "    print(f\"Training: X={X_train.shape}, y={y_train.shape}\")\n",
        "    print(f\"Validation: X={X_valid.shape}, y={y_valid.shape}\")\n",
        "    print(f\"Test: X={X_test.shape}, y={y_test.shape}\")\n",
        "\n",
        "    # Visualize examples\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "    # Show different series\n",
        "    for i in range(6):\n",
        "        row, col = i // 3, i % 3\n",
        "        full_series = np.concatenate([X_train[i, :, 0], y_train[i:i+1]])\n",
        "        axes[row, col].plot(range(len(X_train[i, :, 0])), X_train[i, :, 0], 'b-', linewidth=2, label='Input')\n",
        "        axes[row, col].plot(len(X_train[i, :, 0]), y_train[i], 'ro', markersize=8, label='Target')\n",
        "        axes[row, col].set_title(f'Time Series {i+1}')\n",
        "        axes[row, col].set_xlabel('Time Step')\n",
        "        axes[row, col].set_ylabel('Value')\n",
        "        axes[row, col].legend()\n",
        "        axes[row, col].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
        "\n",
        "def baseline_methods(X_valid, y_valid):\n",
        "    \"\"\"\n",
        "    Implements baseline forecasting methods as described in the book.\n",
        "    \"\"\"\n",
        "    print(\"\\nBASELINE FORECASTING METHODS\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # 1. Naive forecast (last value)\n",
        "    naive_pred = X_valid[:, -1, 0]  # Last value in sequence\n",
        "    naive_mse = np.mean((y_valid - naive_pred) ** 2)\n",
        "\n",
        "    print(f\"1. Naive Forecast (last value): MSE = {naive_mse:.6f}\")\n",
        "\n",
        "    # 2. Moving average\n",
        "    window_sizes = [3, 5, 10]\n",
        "    for window in window_sizes:\n",
        "        ma_pred = np.mean(X_valid[:, -window:, 0], axis=1)\n",
        "        ma_mse = np.mean((y_valid - ma_pred) ** 2)\n",
        "        print(f\"2. Moving Average (window={window}): MSE = {ma_mse:.6f}\")\n",
        "\n",
        "    # 3. Linear regression baseline\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "\n",
        "    # Flatten sequences for linear regression\n",
        "    X_flat = X_valid.reshape(X_valid.shape[0], -1)\n",
        "\n",
        "    lr_model = LinearRegression()\n",
        "    # Use training data for fitting\n",
        "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "    lr_model.fit(X_train_flat, y_train)\n",
        "\n",
        "    lr_pred = lr_model.predict(X_flat)\n",
        "    lr_mse = mean_squared_error(y_valid, lr_pred)\n",
        "\n",
        "    print(f\"3. Linear Regression: MSE = {lr_mse:.6f}\")\n",
        "\n",
        "    # Visualize predictions\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Sample indices for visualization\n",
        "    sample_indices = np.random.choice(len(y_valid), 100, replace=False)\n",
        "\n",
        "    methods = [\n",
        "        ('Naive', naive_pred[sample_indices], naive_mse),\n",
        "        ('Moving Avg (5)', np.mean(X_valid[sample_indices, -5:, 0], axis=1),\n",
        "         np.mean((y_valid[sample_indices] - np.mean(X_valid[sample_indices, -5:, 0], axis=1)) ** 2)),\n",
        "        ('Linear Regression', lr_pred[sample_indices], lr_mse)\n",
        "    ]\n",
        "\n",
        "    for i, (method_name, predictions, mse) in enumerate(methods):\n",
        "        axes[i].scatter(y_valid[sample_indices], predictions, alpha=0.6, s=20)\n",
        "        axes[i].plot([y_valid.min(), y_valid.max()], [y_valid.min(), y_valid.max()],\n",
        "                    'r--', linewidth=2, label='Perfect prediction')\n",
        "        axes[i].set_xlabel('True Values')\n",
        "        axes[i].set_ylabel('Predicted Values')\n",
        "        axes[i].set_title(f'{method_name}\\nMSE = {mse:.6f}')\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'naive': naive_mse,\n",
        "        'moving_avg_5': np.mean((y_valid - np.mean(X_valid[:, -5:, 0], axis=1)) ** 2),\n",
        "        'linear_regression': lr_mse\n",
        "    }\n",
        "\n",
        "# Generate data and run baseline analysis\n",
        "X_train, y_train, X_valid, y_valid, X_test, y_test = demonstrate_time_series_concepts()\n",
        "baseline_results = baseline_methods(X_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simple-rnn-implementation"
      },
      "source": [
        "### Simple RNN Implementation\n",
        "\n",
        "Now let's implement a simple RNN for time series forecasting, following the book's approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "simple-rnn-forecasting"
      },
      "source": [
        "# Simple RNN for time series forecasting\n",
        "\n",
        "def build_simple_rnn(input_shape, units=1):\n",
        "    \"\"\"\n",
        "    Builds a simple RNN model as described in the book.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Shape of input sequences (time_steps, features)\n",
        "        units: Number of RNN units\n",
        "\n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.SimpleRNN(units, input_shape=input_shape)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "def build_deep_rnn(input_shape, units=[20, 20, 1]):\n",
        "    \"\"\"\n",
        "    Builds a deep RNN model as described in the book.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Shape of input sequences\n",
        "        units: List of units for each layer\n",
        "\n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # First layer\n",
        "    model.add(keras.layers.SimpleRNN(\n",
        "        units[0],\n",
        "        return_sequences=True,\n",
        "        input_shape=input_shape\n",
        "    ))\n",
        "\n",
        "    # Hidden layers\n",
        "    for i in range(1, len(units)-1):\n",
        "        model.add(keras.layers.SimpleRNN(\n",
        "            units[i],\n",
        "            return_sequences=True\n",
        "        ))\n",
        "\n",
        "    # Output layer\n",
        "    if len(units) > 1:\n",
        "        model.add(keras.layers.SimpleRNN(units[-1]))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "def build_dense_output_rnn(input_shape, rnn_units=[20, 20], dense_units=1):\n",
        "    \"\"\"\n",
        "    Builds RNN with Dense output layer (as recommended in the book).\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # RNN layers\n",
        "    for i, units in enumerate(rnn_units):\n",
        "        return_seq = i < len(rnn_units) - 1  # Return sequences except for last layer\n",
        "        if i == 0:\n",
        "            model.add(keras.layers.SimpleRNN(\n",
        "                units,\n",
        "                return_sequences=return_seq,\n",
        "                input_shape=input_shape\n",
        "            ))\n",
        "        else:\n",
        "            model.add(keras.layers.SimpleRNN(\n",
        "                units,\n",
        "                return_sequences=return_seq\n",
        "            ))\n",
        "\n",
        "    # Dense output layer\n",
        "    model.add(keras.layers.Dense(dense_units))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_models():\n",
        "    \"\"\"\n",
        "    Trains and evaluates different RNN architectures.\n",
        "    \"\"\"\n",
        "    print(\"\\nTRAINING RNN MODELS\")\n",
        "    print(\"=\"*30)\n",
        "\n",
        "    input_shape = [None, 1]  # Variable length sequences with 1 feature\n",
        "\n",
        "    # Build models\n",
        "    models = {\n",
        "        'Simple RNN (1 unit)': build_simple_rnn(input_shape, units=1),\n",
        "        'Deep RNN (20-20-1)': build_deep_rnn(input_shape, units=[20, 20, 1]),\n",
        "        'RNN + Dense': build_dense_output_rnn(input_shape, rnn_units=[20, 20], dense_units=1)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    histories = {}\n",
        "\n",
        "    # Training parameters\n",
        "    epochs = 20\n",
        "    batch_size = 32\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        print(f\"Model parameters: {model.count_params():,}\")\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(X_valid, y_valid),\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        train_loss = model.evaluate(X_train, y_train, verbose=0)[0]\n",
        "        val_loss = model.evaluate(X_valid, y_valid, verbose=0)[0]\n",
        "        test_loss = model.evaluate(X_test, y_test, verbose=0)[0]\n",
        "\n",
        "        results[name] = {\n",
        "            'train_mse': train_loss,\n",
        "            'val_mse': val_loss,\n",
        "            'test_mse': test_loss,\n",
        "            'params': model.count_params()\n",
        "        }\n",
        "\n",
        "        histories[name] = history\n",
        "\n",
        "        print(f\"Results: Train MSE = {train_loss:.6f}, Val MSE = {val_loss:.6f}, Test MSE = {test_loss:.6f}\")\n",
        "\n",
        "    # Compare with baselines\n",
        "    print(f\"\\nCOMPARISON WITH BASELINES:\")\n",
        "    print(f\"Naive forecast MSE: {baseline_results['naive']:.6f}\")\n",
        "    print(f\"Moving average MSE: {baseline_results['moving_avg_5']:.6f}\")\n",
        "    print(f\"Linear regression MSE: {baseline_results['linear_regression']:.6f}\")\n",
        "\n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Training curves\n",
        "    ax1 = axes[0, 0]\n",
        "    for name, history in histories.items():\n",
        "        ax1.plot(history.history['loss'], label=f'{name} (train)', alpha=0.7)\n",
        "        ax1.plot(history.history['val_loss'], label=f'{name} (val)', linestyle='--', alpha=0.7)\n",
        "    ax1.set_title('Training Curves')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('MSE Loss')\n",
        "    ax1.legend()\n",
        "    ax1.set_yscale('log')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Performance comparison\n",
        "    ax2 = axes[0, 1]\n",
        "    model_names = list(results.keys()) + ['Naive', 'Moving Avg', 'Linear Reg']\n",
        "    mse_values = ([results[name]['test_mse'] for name in results.keys()] +\n",
        "                 [baseline_results['naive'], baseline_results['moving_avg_5'], baseline_results['linear_regression']])\n",
        "\n",
        "    colors = ['skyblue'] * len(results) + ['orange'] * 3\n",
        "    bars = ax2.bar(range(len(model_names)), mse_values, color=colors, alpha=0.7)\n",
        "    ax2.set_title('Test MSE Comparison')\n",
        "    ax2.set_xlabel('Model')\n",
        "    ax2.set_ylabel('MSE')\n",
        "    ax2.set_xticks(range(len(model_names)))\n",
        "    ax2.set_xticklabels(model_names, rotation=45, ha='right')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, mse_values):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0001,\n",
        "                f'{value:.4f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    # Model complexity vs performance\n",
        "    ax3 = axes[1, 0]\n",
        "    param_counts = [results[name]['params'] for name in results.keys()]\n",
        "    test_mses = [results[name]['test_mse'] for name in results.keys()]\n",
        "\n",
        "    ax3.scatter(param_counts, test_mses, s=100, alpha=0.7)\n",
        "    for i, name in enumerate(results.keys()):\n",
        "        ax3.annotate(name, (param_counts[i], test_mses[i]),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "    ax3.set_title('Model Complexity vs Performance')\n",
        "    ax3.set_xlabel('Number of Parameters')\n",
        "    ax3.set_ylabel('Test MSE')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # Prediction examples\n",
        "    ax4 = axes[1, 1]\n",
        "    best_model_name = min(results.keys(), key=lambda x: results[x]['test_mse'])\n",
        "    best_model = models[best_model_name]\n",
        "\n",
        "    # Get predictions for a few test samples\n",
        "    sample_indices = [0, 1, 2]\n",
        "    for i, idx in enumerate(sample_indices):\n",
        "        # Plot input sequence\n",
        "        input_seq = X_test[idx, :, 0]\n",
        "        true_next = y_test[idx]\n",
        "        pred_next = best_model.predict(X_test[idx:idx+1], verbose=0)[0, 0]\n",
        "\n",
        "        time_steps = range(len(input_seq))\n",
        "        ax4.plot(time_steps, input_seq, 'b-', alpha=0.7, linewidth=1)\n",
        "        ax4.scatter(len(input_seq), true_next, color='red', s=60, alpha=0.8, label='True' if i == 0 else '')\n",
        "        ax4.scatter(len(input_seq), pred_next, color='orange', s=60, alpha=0.8, marker='x',\n",
        "                   label='Predicted' if i == 0 else '')\n",
        "\n",
        "    ax4.set_title(f'Predictions: {best_model_name}')\n",
        "    ax4.set_xlabel('Time Step')\n",
        "    ax4.set_ylabel('Value')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results, models\n",
        "\n",
        "# Train and evaluate models\n",
        "model_results, trained_models = train_and_evaluate_models()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lstm-theory"
      },
      "source": [
        "## 5. LSTM and GRU Cells\n",
        "\n",
        "### Long Short-Term Memory (LSTM)\n",
        "\n",
        "LSTM cells address the vanishing gradient problem in RNNs through a sophisticated gating mechanism.\n",
        "\n",
        "#### Mathematical Formulation\n",
        "\n",
        "LSTM maintains two states:\n",
        "- **Cell state** $\\mathbf{c}^{(t)}$: Long-term memory\n",
        "- **Hidden state** $\\mathbf{h}^{(t)}$: Short-term memory/output\n",
        "\n",
        "**Gate Equations:**\n",
        "$$\\mathbf{i}^{(t)} = \\sigma(\\mathbf{W}_{xi}\\mathbf{x}^{(t)} + \\mathbf{W}_{hi}\\mathbf{h}^{(t-1)} + \\mathbf{b}_i)$$ (Input gate)\n",
        "$$\\mathbf{f}^{(t)} = \\sigma(\\mathbf{W}_{xf}\\mathbf{x}^{(t)} + \\mathbf{W}_{hf}\\mathbf{h}^{(t-1)} + \\mathbf{b}_f)$$ (Forget gate)\n",
        "$$\\mathbf{o}^{(t)} = \\sigma(\\mathbf{W}_{xo}\\mathbf{x}^{(t)} + \\mathbf{W}_{ho}\\mathbf{h}^{(t-1)} + \\mathbf{b}_o)$$ (Output gate)\n",
        "\n",
        "**Candidate Values:**\n",
        "$$\\mathbf{g}^{(t)} = \\tanh(\\mathbf{W}_{xg}\\mathbf{x}^{(t)} + \\mathbf{W}_{hg}\\mathbf{h}^{(t-1)} + \\mathbf{b}_g)$$\n",
        "\n",
        "**State Updates:**\n",
        "$$\\mathbf{c}^{(t)} = \\mathbf{f}^{(t)} \\odot \\mathbf{c}^{(t-1)} + \\mathbf{i}^{(t)} \\odot \\mathbf{g}^{(t)}$$\n",
        "$$\\mathbf{h}^{(t)} = \\mathbf{o}^{(t)} \\odot \\tanh(\\mathbf{c}^{(t)})$$\n",
        "\n",
        "Where $\\odot$ denotes element-wise multiplication and $\\sigma$ is the sigmoid function.\n",
        "\n",
        "### Gated Recurrent Unit (GRU)\n",
        "\n",
        "GRU is a simplified version of LSTM with fewer parameters:\n",
        "\n",
        "**Gate Equations:**\n",
        "$$\\mathbf{z}^{(t)} = \\sigma(\\mathbf{W}_{xz}\\mathbf{x}^{(t)} + \\mathbf{W}_{hz}\\mathbf{h}^{(t-1)} + \\mathbf{b}_z)$$ (Update gate)\n",
        "$$\\mathbf{r}^{(t)} = \\sigma(\\mathbf{W}_{xr}\\mathbf{x}^{(t)} + \\mathbf{W}_{hr}\\mathbf{h}^{(t-1)} + \\mathbf{b}_r)$$ (Reset gate)\n",
        "\n",
        "**Candidate State:**\n",
        "$$\\mathbf{g}^{(t)} = \\tanh(\\mathbf{W}_{xg}\\mathbf{x}^{(t)} + \\mathbf{W}_{hg}(\\mathbf{r}^{(t)} \\odot \\mathbf{h}^{(t-1)}) + \\mathbf{b}_g)$$\n",
        "\n",
        "**State Update:**\n",
        "$$\\mathbf{h}^{(t)} = \\mathbf{z}^{(t)} \\odot \\mathbf{h}^{(t-1)} + (1 - \\mathbf{z}^{(t)}) \\odot \\mathbf{g}^{(t)}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lstm-gru-implementation"
      },
      "source": [
        "# LSTM and GRU implementation and comparison\n",
        "\n",
        "def visualize_lstm_architecture():\n",
        "    \"\"\"\n",
        "    Creates a visual representation of LSTM cell architecture.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
        "\n",
        "    # LSTM cell components\n",
        "    ax.text(0.5, 0.9, 'LSTM Cell Architecture', ha='center', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Cell state flow (top)\n",
        "    ax.arrow(0.1, 0.7, 0.8, 0, head_width=0.02, head_length=0.02, fc='blue', linewidth=3)\n",
        "    ax.text(0.5, 0.75, 'Cell State C(t-1) → C(t)', ha='center', fontsize=12, color='blue', fontweight='bold')\n",
        "\n",
        "    # Gates\n",
        "    gates = [\n",
        "        (0.2, 0.5, 'Forget\\nGate', 'lightcoral'),\n",
        "        (0.4, 0.5, 'Input\\nGate', 'lightgreen'),\n",
        "        (0.6, 0.5, 'Candidate\\nValues', 'lightyellow'),\n",
        "        (0.8, 0.5, 'Output\\nGate', 'lightblue')\n",
        "    ]\n",
        "\n",
        "    for x, y, label, color in gates:\n",
        "        rect = plt.Rectangle((x-0.05, y-0.08), 0.1, 0.16,\n",
        "                           facecolor=color, edgecolor='black', linewidth=1)\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x, y, label, ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "    # Input connections\n",
        "    ax.arrow(0.2, 0.2, 0, 0.22, head_width=0.01, head_length=0.01, fc='black')\n",
        "    ax.arrow(0.4, 0.2, 0, 0.22, head_width=0.01, head_length=0.01, fc='black')\n",
        "    ax.arrow(0.6, 0.2, 0, 0.22, head_width=0.01, head_length=0.01, fc='black')\n",
        "    ax.arrow(0.8, 0.2, 0, 0.22, head_width=0.01, head_length=0.01, fc='black')\n",
        "\n",
        "    ax.text(0.5, 0.15, 'x(t) and h(t-1)', ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Mathematical operations\n",
        "    ax.text(0.25, 0.35, '×', ha='center', fontsize=20, fontweight='bold')  # Forget\n",
        "    ax.text(0.45, 0.35, '×', ha='center', fontsize=20, fontweight='bold')  # Input\n",
        "    ax.text(0.5, 0.7, '+', ha='center', fontsize=20, fontweight='bold', color='blue')  # Add\n",
        "    ax.text(0.75, 0.35, '×', ha='center', fontsize=20, fontweight='bold')  # Output\n",
        "\n",
        "    # Output\n",
        "    ax.arrow(0.8, 0.58, 0, 0.1, head_width=0.01, head_length=0.01, fc='red', linewidth=2)\n",
        "    ax.text(0.85, 0.65, 'h(t)', ha='left', fontsize=12, color='red', fontweight='bold')\n",
        "\n",
        "    # Key equations\n",
        "    equations = [\n",
        "        'f(t) = σ(Wf·[h(t-1), x(t)] + bf)',\n",
        "        'i(t) = σ(Wi·[h(t-1), x(t)] + bi)',\n",
        "        'g(t) = tanh(Wg·[h(t-1), x(t)] + bg)',\n",
        "        'o(t) = σ(Wo·[h(t-1), x(t)] + bo)',\n",
        "        'C(t) = f(t)⊙C(t-1) + i(t)⊙g(t)',\n",
        "        'h(t) = o(t)⊙tanh(C(t))'\n",
        "    ]\n",
        "\n",
        "    for i, eq in enumerate(equations):\n",
        "        ax.text(0.02, 0.02 + i*0.04, eq, fontsize=9, fontfamily='monospace')\n",
        "\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def build_lstm_models():\n",
        "    \"\"\"\n",
        "    Builds LSTM models with different configurations.\n",
        "    \"\"\"\n",
        "    input_shape = [None, 1]\n",
        "\n",
        "    models = {}\n",
        "\n",
        "    # Simple LSTM\n",
        "    models['Simple LSTM'] = keras.Sequential([\n",
        "        keras.layers.LSTM(20, input_shape=input_shape),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Deep LSTM\n",
        "    models['Deep LSTM'] = keras.Sequential([\n",
        "        keras.layers.LSTM(20, return_sequences=True, input_shape=input_shape),\n",
        "        keras.layers.LSTM(20, return_sequences=True),\n",
        "        keras.layers.LSTM(20),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Simple GRU\n",
        "    models['Simple GRU'] = keras.Sequential([\n",
        "        keras.layers.GRU(20, input_shape=input_shape),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Deep GRU\n",
        "    models['Deep GRU'] = keras.Sequential([\n",
        "        keras.layers.GRU(20, return_sequences=True, input_shape=input_shape),\n",
        "        keras.layers.GRU(20, return_sequences=True),\n",
        "        keras.layers.GRU(20),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # LSTM with dropout\n",
        "    models['LSTM + Dropout'] = keras.Sequential([\n",
        "        keras.layers.LSTM(20, return_sequences=True, input_shape=input_shape,\n",
        "                         dropout=0.2, recurrent_dropout=0.2),\n",
        "        keras.layers.LSTM(20, dropout=0.2, recurrent_dropout=0.2),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Compile all models\n",
        "    for model in models.values():\n",
        "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "    return models\n",
        "\n",
        "def compare_architectures():\n",
        "    \"\"\"\n",
        "    Compares different RNN architectures (SimpleRNN, LSTM, GRU).\n",
        "    \"\"\"\n",
        "    print(\"\\nCOMPARING RNN ARCHITECTURES\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    models = build_lstm_models()\n",
        "\n",
        "    results = {}\n",
        "    training_times = {}\n",
        "\n",
        "    epochs = 15\n",
        "    batch_size = 32\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        print(f\"Parameters: {model.count_params():,}\")\n",
        "\n",
        "        # Time the training\n",
        "        import time\n",
        "        start_time = time.time()\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(X_valid, y_valid),\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "        training_times[name] = training_time\n",
        "\n",
        "        # Evaluate\n",
        "        test_loss = model.evaluate(X_test, y_test, verbose=0)[0]\n",
        "        val_loss = min(history.history['val_loss'])\n",
        "\n",
        "        results[name] = {\n",
        "            'test_mse': test_loss,\n",
        "            'best_val_mse': val_loss,\n",
        "            'params': model.count_params(),\n",
        "            'training_time': training_time,\n",
        "            'history': history\n",
        "        }\n",
        "\n",
        "        print(f\"Test MSE: {test_loss:.6f}\")\n",
        "        print(f\"Best Val MSE: {val_loss:.6f}\")\n",
        "        print(f\"Training time: {training_time:.1f}s\")\n",
        "\n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    # Training curves\n",
        "    ax1 = axes[0, 0]\n",
        "    for name, result in results.items():\n",
        "        history = result['history']\n",
        "        ax1.plot(history.history['val_loss'], label=name, linewidth=2)\n",
        "    ax1.set_title('Validation Loss Curves')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('MSE Loss')\n",
        "    ax1.legend()\n",
        "    ax1.set_yscale('log')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Performance vs Parameters\n",
        "    ax2 = axes[0, 1]\n",
        "    params = [result['params'] for result in results.values()]\n",
        "    test_mses = [result['test_mse'] for result in results.values()]\n",
        "    names = list(results.keys())\n",
        "\n",
        "    ax2.scatter(params, test_mses, s=100, alpha=0.7)\n",
        "    for i, name in enumerate(names):\n",
        "        ax2.annotate(name, (params[i], test_mses[i]),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "    ax2.set_title('Performance vs Model Complexity')\n",
        "    ax2.set_xlabel('Number of Parameters')\n",
        "    ax2.set_ylabel('Test MSE')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Training time comparison\n",
        "    ax3 = axes[0, 2]\n",
        "    times = [result['training_time'] for result in results.values()]\n",
        "    bars = ax3.bar(range(len(names)), times, alpha=0.7)\n",
        "    ax3.set_title('Training Time Comparison')\n",
        "    ax3.set_xlabel('Model')\n",
        "    ax3.set_ylabel('Training Time (seconds)')\n",
        "    ax3.set_xticks(range(len(names)))\n",
        "    ax3.set_xticklabels(names, rotation=45, ha='right')\n",
        "\n",
        "    # Performance comparison bar chart\n",
        "    ax4 = axes[1, 0]\n",
        "    bars = ax4.bar(range(len(names)), test_mses, alpha=0.7)\n",
        "    ax4.set_title('Test MSE Comparison')\n",
        "    ax4.set_xlabel('Model')\n",
        "    ax4.set_ylabel('Test MSE')\n",
        "    ax4.set_xticks(range(len(names)))\n",
        "    ax4.set_xticklabels(names, rotation=45, ha='right')\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, test_mses):\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0001,\n",
        "                f'{value:.4f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    # Memory usage visualization (parameter count breakdown)\n",
        "    ax5 = axes[1, 1]\n",
        "    # Group by architecture type\n",
        "    lstm_models = [name for name in names if 'LSTM' in name]\n",
        "    gru_models = [name for name in names if 'GRU' in name]\n",
        "\n",
        "    lstm_params = [results[name]['params'] for name in lstm_models]\n",
        "    gru_params = [results[name]['params'] for name in gru_models]\n",
        "\n",
        "    x_pos = np.arange(max(len(lstm_models), len(gru_models)))\n",
        "    width = 0.35\n",
        "\n",
        "    if lstm_params:\n",
        "        ax5.bar(x_pos[:len(lstm_params)] - width/2, lstm_params, width,\n",
        "               label='LSTM variants', alpha=0.7)\n",
        "    if gru_params:\n",
        "        ax5.bar(x_pos[:len(gru_params)] + width/2, gru_params, width,\n",
        "               label='GRU variants', alpha=0.7)\n",
        "\n",
        "    ax5.set_title('Parameter Count by Architecture')\n",
        "    ax5.set_xlabel('Model Variant')\n",
        "    ax5.set_ylabel('Number of Parameters')\n",
        "    ax5.legend()\n",
        "\n",
        "    # Efficiency plot (performance vs time)\n",
        "    ax6 = axes[1, 2]\n",
        "    ax6.scatter(times, test_mses, s=100, alpha=0.7)\n",
        "    for i, name in enumerate(names):\n",
        "        ax6.annotate(name, (times[i], test_mses[i]),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "    ax6.set_title('Efficiency: Performance vs Training Time')\n",
        "    ax6.set_xlabel('Training Time (seconds)')\n",
        "    ax6.set_ylabel('Test MSE')\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Summary table\n",
        "    print(\"\\nSUMMARY TABLE:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"{'Model':<20} {'Test MSE':<12} {'Parameters':<12} {'Time (s)':<10} {'Efficiency':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for name, result in results.items():\n",
        "        efficiency = result['training_time'] / (1 / result['test_mse'])  # Lower is better\n",
        "        print(f\"{name:<20} {result['test_mse']:<12.6f} {result['params']:<12,} {result['training_time']:<10.1f} {efficiency:<10.2f}\")\n",
        "\n",
        "    return results, models\n",
        "\n",
        "# Visualize LSTM architecture and compare models\n",
        "visualize_lstm_architecture()\n",
        "lstm_results, lstm_models = compare_architectures()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "multi-step-forecasting"
      },
      "source": [
        "### Multi-Step Forecasting\n",
        "\n",
        "The book discusses two approaches for forecasting multiple time steps:\n",
        "\n",
        "1. **Iterative Approach**: Predict one step, add to input, repeat\n",
        "2. **Direct Approach**: Train model to output multiple steps at once\n",
        "3. **Sequence-to-Sequence**: Use all time steps for training\n",
        "\n",
        "#### Mathematical Framework\n",
        "\n",
        "For forecasting $h$ steps ahead:\n",
        "\n",
        "**Iterative:**\n",
        "$$\\hat{x}_{t+1} = f(x_t, x_{t-1}, ..., x_{t-n+1})$$\n",
        "$$\\hat{x}_{t+2} = f(\\hat{x}_{t+1}, x_t, ..., x_{t-n+2})$$\n",
        "$$\\vdots$$\n",
        "\n",
        "**Direct:**\n",
        "$$[\\hat{x}_{t+1}, \\hat{x}_{t+2}, ..., \\hat{x}_{t+h}] = f(x_t, x_{t-1}, ..., x_{t-n+1})$$\n",
        "\n",
        "**Sequence-to-Sequence:**\n",
        "Each time step predicts the next $h$ values, providing more training signal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "multi-step-forecasting-implementation"
      },
      "source": [
        "# Multi-step forecasting implementation\n",
        "\n",
        "def prepare_multistep_data(series, n_steps, forecast_horizon):\n",
        "    \"\"\"\n",
        "    Prepares data for multi-step forecasting.\n",
        "\n",
        "    Args:\n",
        "        series: Time series data\n",
        "        n_steps: Number of input time steps\n",
        "        forecast_horizon: Number of steps to forecast\n",
        "\n",
        "    Returns:\n",
        "        X, y for multi-step forecasting\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(len(series) - n_steps - forecast_horizon + 1):\n",
        "        X.append(series[i:i + n_steps])\n",
        "        y.append(series[i + n_steps:i + n_steps + forecast_horizon])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def iterative_forecasting(model, initial_sequence, forecast_steps):\n",
        "    \"\"\"\n",
        "    Implements iterative forecasting approach.\n",
        "    \"\"\"\n",
        "    sequence = initial_sequence.copy()\n",
        "    predictions = []\n",
        "\n",
        "    for step in range(forecast_steps):\n",
        "        # Predict next value\n",
        "        next_pred = model.predict(sequence[-50:].reshape(1, -1, 1), verbose=0)[0, 0]\n",
        "        predictions.append(next_pred)\n",
        "\n",
        "        # Add prediction to sequence for next iteration\n",
        "        sequence = np.append(sequence, next_pred)\n",
        "\n",
        "    return np.array(predictions)\n",
        "\n",
        "def build_multistep_models(input_shape, forecast_horizon):\n",
        "    \"\"\"\n",
        "    Builds models for different multi-step forecasting approaches.\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "\n",
        "    # 1. Direct multi-output model\n",
        "    models['Direct Multi-Output'] = keras.Sequential([\n",
        "        keras.layers.LSTM(20, return_sequences=True, input_shape=input_shape),\n",
        "        keras.layers.LSTM(20),\n",
        "        keras.layers.Dense(forecast_horizon)  # Output all future steps at once\n",
        "    ])\n",
        "\n",
        "    # 2. Sequence-to-sequence model\n",
        "    models['Sequence-to-Sequence'] = keras.Sequential([\n",
        "        keras.layers.LSTM(20, return_sequences=True, input_shape=input_shape),\n",
        "        keras.layers.LSTM(20, return_sequences=True),\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(forecast_horizon))\n",
        "    ])\n",
        "\n",
        "    # Compile models\n",
        "    for model in models.values():\n",
        "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "    return models\n",
        "\n",
        "def demonstrate_multistep_forecasting():\n",
        "    \"\"\"\n",
        "    Demonstrates different approaches to multi-step forecasting.\n",
        "    \"\"\"\n",
        "    print(\"\\nMULTI-STEP FORECASTING DEMONSTRATION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Parameters\n",
        "    n_steps = 50\n",
        "    forecast_horizon = 10\n",
        "\n",
        "    # Generate extended series for multi-step forecasting\n",
        "    series = generate_time_series(1000, n_steps + forecast_horizon)\n",
        "\n",
        "    # Prepare data for different approaches\n",
        "\n",
        "    # 1. For iterative approach (single-step model)\n",
        "    X_single, y_single = series[:, :n_steps], series[:, n_steps]\n",
        "\n",
        "    # 2. For direct multi-output approach\n",
        "    X_multi, y_multi = prepare_multistep_data(series[0, :, 0], n_steps, forecast_horizon)\n",
        "\n",
        "    # Expand to all series\n",
        "    X_multi_all, y_multi_all = [], []\n",
        "    for i in range(series.shape[0]):\n",
        "        X_temp, y_temp = prepare_multistep_data(series[i, :, 0], n_steps, forecast_horizon)\n",
        "        if len(X_temp) > 0:\n",
        "            X_multi_all.extend(X_temp)\n",
        "            y_multi_all.extend(y_temp)\n",
        "\n",
        "    X_multi_all = np.array(X_multi_all).reshape(-1, n_steps, 1)\n",
        "    y_multi_all = np.array(y_multi_all)\n",
        "\n",
        "    # 3. For sequence-to-sequence approach\n",
        "    # Create targets where each time step predicts next forecast_horizon values\n",
        "    Y_seq2seq = np.zeros((len(X_multi_all), n_steps, forecast_horizon))\n",
        "    for i in range(len(X_multi_all)):\n",
        "        for t in range(n_steps):\n",
        "            if t + forecast_horizon < n_steps:\n",
        "                # At each time step, predict the next forecast_horizon steps\n",
        "                start_idx = t + 1\n",
        "                end_idx = min(t + 1 + forecast_horizon, n_steps)\n",
        "                pred_length = end_idx - start_idx\n",
        "                Y_seq2seq[i, t, :pred_length] = X_multi_all[i, start_idx:end_idx, 0]\n",
        "\n",
        "    print(f\"Data shapes:\")\n",
        "    print(f\"Single-step: X={X_single.shape}, y={y_single.shape}\")\n",
        "    print(f\"Multi-output: X={X_multi_all.shape}, y={y_multi_all.shape}\")\n",
        "    print(f\"Seq2Seq: X={X_multi_all.shape}, Y={Y_seq2seq.shape}\")\n",
        "\n",
        "    # Split data\n",
        "    train_size = int(0.7 * len(X_multi_all))\n",
        "    val_size = int(0.2 * len(X_multi_all))\n",
        "\n",
        "    X_train_multi = X_multi_all[:train_size]\n",
        "    y_train_multi = y_multi_all[:train_size]\n",
        "    Y_train_seq2seq = Y_seq2seq[:train_size]\n",
        "\n",
        "    X_val_multi = X_multi_all[train_size:train_size + val_size]\n",
        "    y_val_multi = y_multi_all[train_size:train_size + val_size]\n",
        "    Y_val_seq2seq = Y_seq2seq[train_size:train_size + val_size]\n",
        "\n",
        "    X_test_multi = X_multi_all[train_size + val_size:]\n",
        "    y_test_multi = y_multi_all[train_size + val_size:]\n",
        "    Y_test_seq2seq = Y_seq2seq[train_size + val_size:]\n",
        "\n",
        "    # Train models\n",
        "    print(\"\\nTraining models...\")\n",
        "\n",
        "    # 1. Train single-step model for iterative approach\n",
        "    single_model = keras.Sequential([\n",
        "        keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "        keras.layers.LSTM(20),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "    single_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Use original single-step data\n",
        "    train_idx = int(0.7 * len(X_single))\n",
        "    val_idx = int(0.9 * len(X_single))\n",
        "\n",
        "    single_model.fit(\n",
        "        X_single[:train_idx], y_single[:train_idx],\n",
        "        epochs=10, batch_size=32,\n",
        "        validation_data=(X_single[train_idx:val_idx], y_single[train_idx:val_idx]),\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # 2. Train multi-output model\n",
        "    multi_model = keras.Sequential([\n",
        "        keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "        keras.layers.LSTM(20),\n",
        "        keras.layers.Dense(forecast_horizon)\n",
        "    ])\n",
        "    multi_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    multi_model.fit(\n",
        "        X_train_multi, y_train_multi,\n",
        "        epochs=10, batch_size=32,\n",
        "        validation_data=(X_val_multi, y_val_multi),\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # 3. Custom metric for sequence-to-sequence (only last time step matters for evaluation)\n",
        "    def last_time_step_mse(y_true, y_pred):\n",
        "        return keras.metrics.mean_squared_error(y_true[:, -1], y_pred[:, -1])\n",
        "\n",
        "    seq2seq_model = keras.Sequential([\n",
        "        keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "        keras.layers.LSTM(20, return_sequences=True),\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(forecast_horizon))\n",
        "    ])\n",
        "    seq2seq_model.compile(optimizer='adam', loss='mse', metrics=[last_time_step_mse])\n",
        "\n",
        "    seq2seq_model.fit(\n",
        "        X_train_multi, Y_train_seq2seq,\n",
        "        epochs=10, batch_size=32,\n",
        "        validation_data=(X_val_multi, Y_val_seq2seq),\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate approaches\n",
        "    print(\"\\nEvaluating approaches...\")\n",
        "\n",
        "    # Test on a few examples\n",
        "    test_examples = 5\n",
        "    results = {}\n",
        "\n",
        "    for i in range(test_examples):\n",
        "        input_seq = X_test_multi[i, :, 0]\n",
        "        true_future = y_test_multi[i]\n",
        "\n",
        "        # 1. Iterative approach\n",
        "        iter_pred = iterative_forecasting(single_model, input_seq, forecast_horizon)\n",
        "        iter_mse = np.mean((true_future - iter_pred) ** 2)\n",
        "\n",
        "        # 2. Direct multi-output\n",
        "        multi_pred = multi_model.predict(X_test_multi[i:i+1], verbose=0)[0]\n",
        "        multi_mse = np.mean((true_future - multi_pred) ** 2)\n",
        "\n",
        "        # 3. Sequence-to-sequence (use last time step output)\n",
        "        seq2seq_pred = seq2seq_model.predict(X_test_multi[i:i+1], verbose=0)[0, -1, :]\n",
        "        seq2seq_mse = np.mean((true_future - seq2seq_pred) ** 2)\n",
        "\n",
        "        results[f'Example_{i+1}'] = {\n",
        "            'iterative_mse': iter_mse,\n",
        "            'multi_output_mse': multi_mse,\n",
        "            'seq2seq_mse': seq2seq_mse,\n",
        "            'predictions': {\n",
        "                'iterative': iter_pred,\n",
        "                'multi_output': multi_pred,\n",
        "                'seq2seq': seq2seq_pred,\n",
        "                'true': true_future\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # Visualize results\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "    # Plot examples\n",
        "    for i in range(min(3, test_examples)):\n",
        "        ax = axes[0, i]\n",
        "        preds = results[f'Example_{i+1}']['predictions']\n",
        "\n",
        "        # Input sequence\n",
        "        input_seq = X_test_multi[i, :, 0]\n",
        "        ax.plot(range(len(input_seq)), input_seq, 'b-', linewidth=2, label='Input')\n",
        "\n",
        "        # Predictions\n",
        "        future_range = range(len(input_seq), len(input_seq) + forecast_horizon)\n",
        "        ax.plot(future_range, preds['true'], 'k-', linewidth=3, label='True', alpha=0.8)\n",
        "        ax.plot(future_range, preds['iterative'], 'r--', linewidth=2, label='Iterative')\n",
        "        ax.plot(future_range, preds['multi_output'], 'g--', linewidth=2, label='Multi-output')\n",
        "        ax.plot(future_range, preds['seq2seq'], 'm--', linewidth=2, label='Seq2Seq')\n",
        "\n",
        "        ax.axvline(x=len(input_seq)-0.5, color='gray', linestyle=':', alpha=0.7)\n",
        "        ax.set_title(f'Example {i+1}')\n",
        "        ax.set_xlabel('Time Step')\n",
        "        ax.set_ylabel('Value')\n",
        "        if i == 0:\n",
        "            ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Performance comparison\n",
        "    ax = axes[1, 0]\n",
        "    methods = ['Iterative', 'Multi-output', 'Seq2Seq']\n",
        "    avg_mses = []\n",
        "\n",
        "    for method_key in ['iterative_mse', 'multi_output_mse', 'seq2seq_mse']:\n",
        "        mses = [results[f'Example_{i+1}'][method_key] for i in range(test_examples)]\n",
        "        avg_mses.append(np.mean(mses))\n",
        "\n",
        "    bars = ax.bar(methods, avg_mses, alpha=0.7, color=['red', 'green', 'magenta'])\n",
        "    ax.set_title('Average MSE Comparison')\n",
        "    ax.set_ylabel('MSE')\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, avg_mses):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "                f'{value:.4f}', ha='center', va='bottom')\n",
        "\n",
        "    # MSE distribution\n",
        "    ax = axes[1, 1]\n",
        "    all_mses = {'Iterative': [], 'Multi-output': [], 'Seq2Seq': []}\n",
        "\n",
        "    for i in range(test_examples):\n",
        "        all_mses['Iterative'].append(results[f'Example_{i+1}']['iterative_mse'])\n",
        "        all_mses['Multi-output'].append(results[f'Example_{i+1}']['multi_output_mse'])\n",
        "        all_mses['Seq2Seq'].append(results[f'Example_{i+1}']['seq2seq_mse'])\n",
        "\n",
        "    ax.boxplot(all_mses.values(), labels=all_mses.keys())\n",
        "    ax.set_title('MSE Distribution')\n",
        "    ax.set_ylabel('MSE')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Error by forecast step\n",
        "    ax = axes[1, 2]\n",
        "    step_errors = {'Iterative': [], 'Multi-output': [], 'Seq2Seq': []}\n",
        "\n",
        "    for step in range(forecast_horizon):\n",
        "        iter_errors = []\n",
        "        multi_errors = []\n",
        "        seq2seq_errors = []\n",
        "\n",
        "        for i in range(test_examples):\n",
        "            preds = results[f'Example_{i+1}']['predictions']\n",
        "            true_val = preds['true'][step]\n",
        "\n",
        "            iter_errors.append((preds['iterative'][step] - true_val) ** 2)\n",
        "            multi_errors.append((preds['multi_output'][step] - true_val) ** 2)\n",
        "            seq2seq_errors.append((preds['seq2seq'][step] - true_val) ** 2)\n",
        "\n",
        "        step_errors['Iterative'].append(np.mean(iter_errors))\n",
        "        step_errors['Multi-output'].append(np.mean(multi_errors))\n",
        "        step_errors['Seq2Seq'].append(np.mean(seq2seq_errors))\n",
        "\n",
        "    steps = range(1, forecast_horizon + 1)\n",
        "    ax.plot(steps, step_errors['Iterative'], 'r-o', label='Iterative', linewidth=2)\n",
        "    ax.plot(steps, step_errors['Multi-output'], 'g-s', label='Multi-output', linewidth=2)\n",
        "    ax.plot(steps, step_errors['Seq2Seq'], 'm-^', label='Seq2Seq', linewidth=2)\n",
        "\n",
        "    ax.set_title('MSE by Forecast Step')\n",
        "    ax.set_xlabel('Forecast Step')\n",
        "    ax.set_ylabel('MSE')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nSUMMARY:\")\n",
        "    print(f\"Average MSE - Iterative: {avg_mses[0]:.6f}\")\n",
        "    print(f\"Average MSE - Multi-output: {avg_mses[1]:.6f}\")\n",
        "    print(f\"Average MSE - Seq2Seq: {avg_mses[2]:.6f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run multi-step forecasting demonstration\n",
        "multistep_results = demonstrate_multistep_forecasting()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "handling-long-sequences"
      },
      "source": [
        "## 6. Handling Long Sequences\n",
        "\n",
        "### Challenges with Long Sequences\n",
        "\n",
        "1. **Vanishing Gradients**: As sequences get longer, gradients become exponentially smaller\n",
        "2. **Limited Memory**: RNNs forget early information in long sequences\n",
        "3. **Computational Complexity**: Memory and computation scale with sequence length\n",
        "\n",
        "### Mathematical Analysis of Gradient Flow\n",
        "\n",
        "For an RNN with hidden state $\\mathbf{h}^{(t)} = f(\\mathbf{W}\\mathbf{h}^{(t-1)} + \\mathbf{U}\\mathbf{x}^{(t)})$:\n",
        "\n",
        "The gradient of loss with respect to early parameters involves:\n",
        "$$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\sum_{t=1}^{T} \\sum_{k=1}^{t} \\frac{\\partial L^{(t)}}{\\partial \\mathbf{h}^{(t)}} \\frac{\\partial \\mathbf{h}^{(t)}}{\\partial \\mathbf{h}^{(k)}} \\frac{\\partial \\mathbf{h}^{(k)}}{\\partial \\mathbf{W}}$$\n",
        "\n",
        "The problematic term is:\n",
        "$$\\frac{\\partial \\mathbf{h}^{(t)}}{\\partial \\mathbf{h}^{(k)}} = \\prod_{i=k+1}^{t} \\frac{\\partial \\mathbf{h}^{(i)}}{\\partial \\mathbf{h}^{(i-1)}} = \\prod_{i=k+1}^{t} \\mathbf{W}^T \\text{diag}(f'(\\mathbf{h}^{(i-1)}))$$\n",
        "\n",
        "If the largest eigenvalue of $\\mathbf{W}^T \\text{diag}(f'())$ is:\n",
        "- **< 1**: Gradients vanish exponentially\n",
        "- **> 1**: Gradients explode exponentially\n",
        "\n",
        "### Solutions\n",
        "\n",
        "1. **LSTM/GRU**: Gating mechanisms control information flow\n",
        "2. **Gradient Clipping**: Prevent exploding gradients\n",
        "3. **Layer Normalization**: Normalize activations\n",
        "4. **Residual Connections**: Skip connections for gradient flow\n",
        "5. **1D Convolutions**: Process sequences more efficiently"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gradient-clipping-layer-norm"
      },
      "source": [
        "# Advanced techniques for handling long sequences\n",
        "\n",
        "class LayerNormSimpleRNNCell(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom RNN cell with Layer Normalization as described in the book.\n",
        "    This demonstrates how to create custom cells with advanced techniques.\n",
        "    \"\"\"\n",
        "    def __init__(self, units, activation='tanh', **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.state_size = units\n",
        "        self.output_size = units\n",
        "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None)\n",
        "        self.layer_norm = keras.layers.LayerNormalization()\n",
        "        self.activation = keras.activations.get(activation)\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
        "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
        "        return norm_outputs, [norm_outputs]\n",
        "\n",
        "def demonstrate_gradient_techniques():\n",
        "    \"\"\"\n",
        "    Demonstrates gradient clipping and layer normalization.\n",
        "    \"\"\"\n",
        "    print(\"\\nDEMONSTRATING GRADIENT TECHNIQUES\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create longer sequences to demonstrate the problem\n",
        "    n_steps = 100  # Longer sequences\n",
        "    series_long = generate_time_series(1000, n_steps + 1)\n",
        "\n",
        "    X_long = series_long[:, :n_steps]\n",
        "    y_long = series_long[:, -1]\n",
        "\n",
        "    # Split data\n",
        "    train_size = int(0.7 * len(X_long))\n",
        "    val_size = int(0.2 * len(X_long))\n",
        "\n",
        "    X_train_long = X_long[:train_size]\n",
        "    y_train_long = y_long[:train_size]\n",
        "    X_val_long = X_long[train_size:train_size + val_size]\n",
        "    y_val_long = y_long[train_size:train_size + val_size]\n",
        "    X_test_long = X_long[train_size + val_size:]\n",
        "    y_test_long = y_long[train_size + val_size:]\n",
        "\n",
        "    print(f\"Long sequence data shapes:\")\n",
        "    print(f\"X_train: {X_train_long.shape}, y_train: {y_train_long.shape}\")\n",
        "\n",
        "    # Build models with different techniques\n",
        "    models = {}\n",
        "\n",
        "    # 1. Standard LSTM\n",
        "    models['Standard LSTM'] = keras.Sequential([\n",
        "        keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "        keras.layers.LSTM(20),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # 2. LSTM with gradient clipping\n",
        "    models['LSTM + Grad Clip'] = keras.Sequential([\n",
        "        keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "        keras.layers.LSTM(20),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # 3. LSTM with dropout\n",
        "    models['LSTM + Dropout'] = keras.Sequential([\n",
        "        keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1],\n",
        "                         dropout=0.2, recurrent_dropout=0.2),\n",
        "        keras.layers.LSTM(20, dropout=0.2, recurrent_dropout=0.2),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # 4. Custom cell with Layer Normalization\n",
        "    models['Layer Norm RNN'] = keras.Sequential([\n",
        "        keras.layers.RNN(LayerNormSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]),\n",
        "        keras.layers.RNN(LayerNormSimpleRNNCell(20)),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Compile models with different optimizers\n",
        "    optimizers = {\n",
        "        'Standard LSTM': keras.optimizers.Adam(learning_rate=0.001),\n",
        "        'LSTM + Grad Clip': keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),  # Gradient clipping\n",
        "        'LSTM + Dropout': keras.optimizers.Adam(learning_rate=0.001),\n",
        "        'Layer Norm RNN': keras.optimizers.Adam(learning_rate=0.001)\n",
        "    }\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.compile(optimizer=optimizers[name], loss='mse', metrics=['mae'])\n",
        "\n",
        "    # Custom training loop to track gradients\n",
        "    def train_with_gradient_monitoring(model, name, X_train, y_train, X_val, y_val, epochs=15):\n",
        "        \"\"\"\n",
        "        Trains model while monitoring gradient norms.\n",
        "        \"\"\"\n",
        "        history = {'loss': [], 'val_loss': [], 'gradient_norm': []}\n",
        "\n",
        "        # Prepare datasets\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_losses = []\n",
        "            epoch_grad_norms = []\n",
        "\n",
        "            for batch_x, batch_y in train_dataset:\n",
        "                with tf.GradientTape() as tape:\n",
        "                    predictions = model(batch_x, training=True)\n",
        "                    loss = keras.losses.mse(batch_y, predictions)\n",
        "                    loss = tf.reduce_mean(loss)\n",
        "\n",
        "                # Compute gradients\n",
        "                gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "                # Calculate gradient norm\n",
        "                grad_norm = tf.sqrt(sum([tf.reduce_sum(tf.square(g)) for g in gradients if g is not None]))\n",
        "                epoch_grad_norms.append(float(grad_norm))\n",
        "\n",
        "                # Apply gradients\n",
        "                model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "                epoch_losses.append(float(loss))\n",
        "\n",
        "            # Validation loss\n",
        "            val_predictions = model(X_val, training=False)\n",
        "            val_loss = tf.reduce_mean(keras.losses.mse(y_val, val_predictions))\n",
        "\n",
        "            history['loss'].append(np.mean(epoch_losses))\n",
        "            history['val_loss'].append(float(val_loss))\n",
        "            history['gradient_norm'].append(np.mean(epoch_grad_norms))\n",
        "\n",
        "            if epoch % 5 == 0:\n",
        "                print(f\"{name} - Epoch {epoch}: Loss={history['loss'][-1]:.6f}, Val Loss={history['val_loss'][-1]:.6f}, Grad Norm={history['gradient_norm'][-1]:.4f}\")\n",
        "\n",
        "        return history\n",
        "\n",
        "    # Train models and collect results\n",
        "    results = {}\n",
        "    histories = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        history = train_with_gradient_monitoring(\n",
        "            model, name, X_train_long, y_train_long, X_val_long, y_val_long\n",
        "        )\n",
        "        histories[name] = history\n",
        "\n",
        "        # Evaluate final performance\n",
        "        test_pred = model(X_test_long, training=False)\n",
        "        test_mse = float(tf.reduce_mean(keras.losses.mse(y_test_long, test_pred)))\n",
        "\n",
        "        results[name] = {\n",
        "            'test_mse': test_mse,\n",
        "            'final_grad_norm': history['gradient_norm'][-1],\n",
        "            'gradient_stability': np.std(history['gradient_norm'])\n",
        "        }\n",
        "\n",
        "    # Visualize results\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "    # Training loss curves\n",
        "    ax = axes[0, 0]\n",
        "    for name, history in histories.items():\n",
        "        ax.plot(history['loss'], label=f'{name} (train)', alpha=0.7)\n",
        "        ax.plot(history['val_loss'], label=f'{name} (val)', linestyle='--', alpha=0.7)\n",
        "    ax.set_title('Training Curves')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('MSE Loss')\n",
        "    ax.set_yscale('log')\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Gradient norm evolution\n",
        "    ax = axes[0, 1]\n",
        "    for name, history in histories.items():\n",
        "        ax.plot(history['gradient_norm'], label=name, linewidth=2)\n",
        "    ax.set_title('Gradient Norm Evolution')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Gradient Norm')\n",
        "    ax.set_yscale('log')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Final performance comparison\n",
        "    ax = axes[0, 2]\n",
        "    names = list(results.keys())\n",
        "    test_mses = [results[name]['test_mse'] for name in names]\n",
        "\n",
        "    bars = ax.bar(range(len(names)), test_mses, alpha=0.7)\n",
        "    ax.set_title('Test MSE Comparison')\n",
        "    ax.set_xlabel('Model')\n",
        "    ax.set_ylabel('Test MSE')\n",
        "    ax.set_xticks(range(len(names)))\n",
        "    ax.set_xticklabels(names, rotation=45, ha='right')\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, test_mses):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0001,\n",
        "                f'{value:.4f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # Gradient stability analysis\n",
        "    ax = axes[1, 0]\n",
        "    grad_stabilities = [results[name]['gradient_stability'] for name in names]\n",
        "\n",
        "    bars = ax.bar(range(len(names)), grad_stabilities, alpha=0.7, color='orange')\n",
        "    ax.set_title('Gradient Stability (Lower = Better)')\n",
        "    ax.set_xlabel('Model')\n",
        "    ax.set_ylabel('Gradient Norm Std Dev')\n",
        "    ax.set_xticks(range(len(names)))\n",
        "    ax.set_xticklabels(names, rotation=45, ha='right')\n",
        "\n",
        "    # Performance vs Gradient Stability\n",
        "    ax = axes[1, 1]\n",
        "    ax.scatter(grad_stabilities, test_mses, s=100, alpha=0.7)\n",
        "    for i, name in enumerate(names):\n",
        "        ax.annotate(name, (grad_stabilities[i], test_mses[i]),\n",
        "                   xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "    ax.set_xlabel('Gradient Stability (Std Dev)')\n",
        "    ax.set_ylabel('Test MSE')\n",
        "    ax.set_title('Performance vs Gradient Stability')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Convergence speed analysis\n",
        "    ax = axes[1, 2]\n",
        "    for name, history in histories.items():\n",
        "        # Find epoch where validation loss stabilizes (minimum reached)\n",
        "        min_val_loss_epoch = np.argmin(history['val_loss'])\n",
        "        ax.scatter(min_val_loss_epoch, min(history['val_loss']),\n",
        "                  label=name, s=100, alpha=0.7)\n",
        "\n",
        "    ax.set_xlabel('Epochs to Best Val Loss')\n",
        "    ax.set_ylabel('Best Val Loss')\n",
        "    ax.set_title('Convergence Speed vs Final Performance')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed analysis\n",
        "    print(\"\\nDETAILED ANALYSIS:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"{'Model':<20} {'Test MSE':<12} {'Grad Stability':<15} {'Final Grad Norm':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for name in names:\n",
        "        print(f\"{name:<20} {results[name]['test_mse']:<12.6f} {results[name]['gradient_stability']:<15.4f} {results[name]['final_grad_norm']:<15.4f}\")\n",
        "\n",
        "    return results, histories\n",
        "\n",
        "# Run gradient techniques demonstration\n",
        "gradient_results, gradient_histories = demonstrate_gradient_techniques()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d-cnn-theory"
      },
      "source": [
        "## 7. 1D Convolutional Layers for Sequence Processing\n",
        "\n",
        "### Theoretical Foundation\n",
        "\n",
        "1D Convolutional layers can effectively process sequences by:\n",
        "1. **Local Pattern Detection**: Each filter detects specific patterns\n",
        "2. **Computational Efficiency**: Parallel processing vs sequential RNNs\n",
        "3. **Long-Range Dependencies**: Dilated convolutions extend receptive field\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "For a 1D convolution with input $\\mathbf{x} \\in \\mathbb{R}^{T \\times d}$ and filter $\\mathbf{w} \\in \\mathbb{R}^{k \\times d}$:\n",
        "\n",
        "$$y_t = \\sum_{i=0}^{k-1} \\sum_{j=0}^{d-1} w_{i,j} \\cdot x_{t+i-\\lfloor k/2 \\rfloor, j} + b$$\n",
        "\n",
        "With stride $s$ and dilation $r$:\n",
        "$$y_t = \\sum_{i=0}^{k-1} \\sum_{j=0}^{d-1} w_{i,j} \\cdot x_{t \\cdot s + i \\cdot r, j} + b$$\n",
        "\n",
        "### Receptive Field Analysis\n",
        "\n",
        "For a stack of $L$ layers with kernel size $k$:\n",
        "- **Without dilation**: Receptive field = $1 + L(k-1)$\n",
        "- **With dilation** $r_l$ at layer $l$: Receptive field = $1 + \\sum_{l=1}^{L} (k-1) \\cdot \\prod_{i=1}^{l} r_i$\n",
        "\n",
        "### Advantages over RNNs\n",
        "\n",
        "1. **Parallelization**: All outputs computed simultaneously\n",
        "2. **Stable Gradients**: No vanishing gradient problem\n",
        "3. **Flexible Receptive Fields**: Controlled via kernel size and dilation\n",
        "4. **Hierarchical Features**: Lower layers capture local, higher layers capture global patterns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d-cnn-implementation"
      },
      "source": [
        "# 1D Convolutional Networks for Sequence Processing\n",
        "\n",
        "def visualize_1d_convolution():\n",
        "    \"\"\"\n",
        "    Visualizes how 1D convolution works on sequences.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
        "\n",
        "    # Create sample sequence\n",
        "    sequence_length = 20\n",
        "    sequence = np.sin(np.linspace(0, 4*np.pi, sequence_length)) + 0.1 * np.random.randn(sequence_length)\n",
        "\n",
        "    # Different kernel sizes\n",
        "    kernel_sizes = [3, 5, 7]\n",
        "\n",
        "    for i, kernel_size in enumerate(kernel_sizes):\n",
        "        # Create different types of kernels\n",
        "        kernels = {\n",
        "            'Edge Detector': np.array([-1, 0, 1] + [0] * (kernel_size - 3)),\n",
        "            'Smoother': np.ones(kernel_size) / kernel_size\n",
        "        }\n",
        "\n",
        "        for j, (kernel_name, kernel) in enumerate(kernels.items()):\n",
        "            ax = axes[i, j]\n",
        "\n",
        "            # Apply convolution (simplified)\n",
        "            output = np.convolve(sequence, kernel, mode='same')\n",
        "\n",
        "            # Plot\n",
        "            ax.plot(sequence, 'b-', linewidth=2, label='Input', alpha=0.7)\n",
        "            ax.plot(output, 'r-', linewidth=2, label='Output')\n",
        "\n",
        "            # Show kernel\n",
        "            kernel_pos = len(sequence) // 4\n",
        "            kernel_x = np.arange(kernel_pos, kernel_pos + len(kernel))\n",
        "            ax.stem(kernel_x, kernel * 2 + np.mean(sequence),\n",
        "                   basefmt=' ', linefmt='g-', markerfmt='go', label='Kernel')\n",
        "\n",
        "            ax.set_title(f'{kernel_name} (kernel size {kernel_size})')\n",
        "            ax.set_xlabel('Time Step')\n",
        "            ax.set_ylabel('Value')\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def build_1d_cnn_models():\n",
        "    \"\"\"\n",
        "    Builds various 1D CNN architectures for sequence processing.\n",
        "    \"\"\"\n",
        "    input_shape = [None, 1]\n",
        "    models = {}\n",
        "\n",
        "    # 1. Simple 1D CNN\n",
        "    models['Simple 1D CNN'] = keras.Sequential([\n",
        "        keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
        "        keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "        keras.layers.GlobalMaxPooling1D(),\n",
        "        keras.layers.Dense(50, activation='relu'),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # 2. Deep 1D CNN with pooling\n",
        "    models['Deep 1D CNN'] = keras.Sequential([\n",
        "        keras.layers.Conv1D(filters=32, kernel_size=5, activation='relu', input_shape=input_shape),\n",
        "        keras.layers.Conv1D(filters=32, kernel_size=5, activation='relu'),\n",
        "        keras.layers.MaxPooling1D(pool_size=2),\n",
        "        keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "        keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "        keras.layers.GlobalMaxPooling1D(),\n",
        "        keras.layers.Dense(50, activation='relu'),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # 3. 1D CNN + RNN hybrid\n",
        "    models['CNN-RNN Hybrid'] = keras.Sequential([\n",
        "        keras.layers.Conv1D(filters=32, kernel_size=5, activation='relu', input_shape=input_shape),\n",
        "        keras.layers.Conv1D(filters=32, kernel_size=5, activation='relu'),\n",
        "        keras.layers.MaxPooling1D(pool_size=2),\n",
        "        keras.layers.LSTM(50, return_sequences=True),\n",
        "        keras.layers.LSTM(50),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # 4. Dilated 1D CNN (simplified WaveNet-style)\n",
        "    models['Dilated CNN'] = keras.Sequential([\n",
        "        keras.layers.Conv1D(filters=32, kernel_size=2, dilation_rate=1,\n",
        "                           activation='relu', padding='causal', input_shape=input_shape),\n",
        "        keras.layers.Conv1D(filters=32, kernel_size=2, dilation_rate=2,\n",
        "                           activation='relu', padding='causal'),\n",
        "        keras.layers.Conv1D(filters=32, kernel_size=2, dilation_rate=4,\n",
        "                           activation='relu', padding='causal'),\n",
        "        keras.layers.Conv1D(filters=32, kernel_size=2, dilation_rate=8,\n",
        "                           activation='relu', padding='causal'),\n",
        "        keras.layers.GlobalMaxPooling1D(),\n",
        "        keras.layers.Dense(50, activation='relu'),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Compile all models\n",
        "    for model in models.values():\n",
        "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "    return models\n",
        "\n",
        "def analyze_receptive_fields():\n",
        "    \"\"\"\n",
        "    Analyzes and visualizes receptive fields for different architectures.\n",
        "    \"\"\"\n",
        "    print(\"\\nRECEPTIVE FIELD ANALYSIS\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Calculate receptive fields for different architectures\n",
        "    architectures = {\n",
        "        'Single Conv (k=3)': [(3, 1, 1)],  # (kernel_size, dilation, stride)\n",
        "        'Two Conv (k=3)': [(3, 1, 1), (3, 1, 1)],\n",
        "        'Conv + Pool': [(3, 1, 1), (3, 1, 2)],  # pooling = stride 2\n",
        "        'Dilated Conv': [(2, 1, 1), (2, 2, 1), (2, 4, 1), (2, 8, 1)]\n",
        "    }\n",
        "\n",
        "    def calculate_receptive_field(layers):\n",
        "        \"\"\"Calculate receptive field for a sequence of layers.\"\"\"\n",
        "        rf = 1\n",
        "        jump = 1\n",
        "\n",
        "        for kernel_size, dilation, stride in layers:\n",
        "            rf += (kernel_size - 1) * jump * dilation\n",
        "            jump *= stride\n",
        "\n",
        "        return rf\n",
        "\n",
        "    rf_results = {}\n",
        "    for name, layers in architectures.items():\n",
        "        rf = calculate_receptive_field(layers)\n",
        "        rf_results[name] = rf\n",
        "        print(f\"{name:<20}: Receptive Field = {rf}\")\n",
        "\n",
        "    # Visualize receptive fields\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    sequence_length = 50\n",
        "    center_pos = sequence_length // 2\n",
        "\n",
        "    for i, (name, rf) in enumerate(rf_results.items()):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Create sequence\n",
        "        sequence = np.zeros(sequence_length)\n",
        "        sequence[center_pos] = 1  # Impulse at center\n",
        "\n",
        "        # Show receptive field\n",
        "        rf_start = max(0, center_pos - rf // 2)\n",
        "        rf_end = min(sequence_length, center_pos + rf // 2 + 1)\n",
        "\n",
        "        ax.plot(sequence, 'k-', linewidth=2, label='Input')\n",
        "        ax.axvspan(rf_start, rf_end, alpha=0.3, color='red', label=f'Receptive Field (size {rf})')\n",
        "        ax.axvline(center_pos, color='blue', linestyle='--', label='Output position')\n",
        "\n",
        "        ax.set_title(f'{name}')\n",
        "        ax.set_xlabel('Input Position')\n",
        "        ax.set_ylabel('Value')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return rf_results\n",
        "\n",
        "def compare_cnn_rnn_performance():\n",
        "    \"\"\"\n",
        "    Compares 1D CNN vs RNN performance on time series forecasting.\n",
        "    \"\"\"\n",
        "    print(\"\\nCOMPARING 1D CNN vs RNN PERFORMANCE\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Build comparison models\n",
        "    cnn_models = build_1d_cnn_models()\n",
        "\n",
        "    # Add RNN models for comparison\n",
        "    rnn_models = {\n",
        "        'Simple LSTM': keras.Sequential([\n",
        "            keras.layers.LSTM(32, return_sequences=True, input_shape=[None, 1]),\n",
        "            keras.layers.LSTM(32),\n",
        "            keras.layers.Dense(1)\n",
        "        ]),\n",
        "        'Deep LSTM': keras.Sequential([\n",
        "            keras.layers.LSTM(32, return_sequences=True, input_shape=[None, 1]),\n",
        "            keras.layers.LSTM(32, return_sequences=True),\n",
        "            keras.layers.LSTM(32),\n",
        "            keras.layers.Dense(1)\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    for model in rnn_models.values():\n",
        "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "    # Combine all models\n",
        "    all_models = {**cnn_models, **rnn_models}\n",
        "\n",
        "    # Training and evaluation\n",
        "    results = {}\n",
        "    training_times = {}\n",
        "\n",
        "    epochs = 10\n",
        "    batch_size = 32\n",
        "\n",
        "    for name, model in all_models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        print(f\"Parameters: {model.count_params():,}\")\n",
        "\n",
        "        # Measure training time\n",
        "        import time\n",
        "        start_time = time.time()\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(X_valid, y_valid),\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "        training_times[name] = training_time\n",
        "\n",
        "        # Evaluate\n",
        "        test_loss = model.evaluate(X_test, y_test, verbose=0)[0]\n",
        "\n",
        "        results[name] = {\n",
        "            'test_mse': test_loss,\n",
        "            'params': model.count_params(),\n",
        "            'training_time': training_time,\n",
        "            'final_val_loss': history.history['val_loss'][-1],\n",
        "            'history': history\n",
        "        }\n",
        "\n",
        "        print(f\"Test MSE: {test_loss:.6f}, Training time: {training_time:.1f}s\")\n",
        "\n",
        "    # Detailed comparison visualization\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "    # Separate CNN and RNN results\n",
        "    cnn_names = list(cnn_models.keys())\n",
        "    rnn_names = list(rnn_models.keys())\n",
        "\n",
        "    # Performance comparison\n",
        "    ax = axes[0, 0]\n",
        "    cnn_mses = [results[name]['test_mse'] for name in cnn_names]\n",
        "    rnn_mses = [results[name]['test_mse'] for name in rnn_names]\n",
        "\n",
        "    x_cnn = np.arange(len(cnn_names))\n",
        "    x_rnn = np.arange(len(rnn_names)) + len(cnn_names) + 0.5\n",
        "\n",
        "    ax.bar(x_cnn, cnn_mses, alpha=0.7, label='CNN Models', color='blue')\n",
        "    ax.bar(x_rnn, rnn_mses, alpha=0.7, label='RNN Models', color='red')\n",
        "\n",
        "    all_names = cnn_names + rnn_names\n",
        "    ax.set_xticks(list(x_cnn) + list(x_rnn))\n",
        "    ax.set_xticklabels(all_names, rotation=45, ha='right')\n",
        "    ax.set_title('Test MSE Comparison')\n",
        "    ax.set_ylabel('MSE')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Training time comparison\n",
        "    ax = axes[0, 1]\n",
        "    cnn_times = [results[name]['training_time'] for name in cnn_names]\n",
        "    rnn_times = [results[name]['training_time'] for name in rnn_names]\n",
        "\n",
        "    ax.bar(x_cnn, cnn_times, alpha=0.7, label='CNN Models', color='blue')\n",
        "    ax.bar(x_rnn, rnn_times, alpha=0.7, label='RNN Models', color='red')\n",
        "\n",
        "    ax.set_xticks(list(x_cnn) + list(x_rnn))\n",
        "    ax.set_xticklabels(all_names, rotation=45, ha='right')\n",
        "    ax.set_title('Training Time Comparison')\n",
        "    ax.set_ylabel('Time (seconds)')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Parameter count comparison\n",
        "    ax = axes[0, 2]\n",
        "    cnn_params = [results[name]['params'] for name in cnn_names]\n",
        "    rnn_params = [results[name]['params'] for name in rnn_names]\n",
        "\n",
        "    ax.bar(x_cnn, cnn_params, alpha=0.7, label='CNN Models', color='blue')\n",
        "    ax.bar(x_rnn, rnn_params, alpha=0.7, label='RNN Models', color='red')\n",
        "\n",
        "    ax.set_xticks(list(x_cnn) + list(x_rnn))\n",
        "    ax.set_xticklabels(all_names, rotation=45, ha='right')\n",
        "    ax.set_title('Parameter Count Comparison')\n",
        "    ax.set_ylabel('Number of Parameters')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Efficiency plot (performance vs time)\n",
        "    ax = axes[1, 0]\n",
        "    all_times = [results[name]['training_time'] for name in all_names]\n",
        "    all_mses = [results[name]['test_mse'] for name in all_names]\n",
        "    colors = ['blue'] * len(cnn_names) + ['red'] * len(rnn_names)\n",
        "\n",
        "    scatter = ax.scatter(all_times, all_mses, c=colors, s=100, alpha=0.7)\n",
        "    for i, name in enumerate(all_names):\n",
        "        ax.annotate(name, (all_times[i], all_mses[i]),\n",
        "                   xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "\n",
        "    ax.set_xlabel('Training Time (seconds)')\n",
        "    ax.set_ylabel('Test MSE')\n",
        "    ax.set_title('Efficiency: Performance vs Training Time')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Training curves for best models\n",
        "    ax = axes[1, 1]\n",
        "    best_cnn = min(cnn_names, key=lambda x: results[x]['test_mse'])\n",
        "    best_rnn = min(rnn_names, key=lambda x: results[x]['test_mse'])\n",
        "\n",
        "    ax.plot(results[best_cnn]['history'].history['val_loss'],\n",
        "           label=f'{best_cnn} (best CNN)', linewidth=2, color='blue')\n",
        "    ax.plot(results[best_rnn]['history'].history['val_loss'],\n",
        "           label=f'{best_rnn} (best RNN)', linewidth=2, color='red')\n",
        "\n",
        "    ax.set_title('Best Model Training Curves')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Validation Loss')\n",
        "    ax.legend()\n",
        "    ax.set_yscale('log')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Model complexity vs performance\n",
        "    ax = axes[1, 2]\n",
        "    all_params = [results[name]['params'] for name in all_names]\n",
        "\n",
        "    scatter = ax.scatter(all_params, all_mses, c=colors, s=100, alpha=0.7)\n",
        "    for i, name in enumerate(all_names):\n",
        "        ax.annotate(name, (all_params[i], all_mses[i]),\n",
        "                   xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "\n",
        "    ax.set_xlabel('Number of Parameters')\n",
        "    ax.set_ylabel('Test MSE')\n",
        "    ax.set_title('Model Complexity vs Performance')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Summary analysis\n",
        "    print(\"\\nSUMMARY ANALYSIS:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"{'Model':<20} {'Type':<8} {'Test MSE':<12} {'Time (s)':<10} {'Parameters':<12} {'Efficiency':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for name in all_names:\n",
        "        model_type = 'CNN' if name in cnn_names else 'RNN'\n",
        "        efficiency = results[name]['training_time'] / (1 / results[name]['test_mse'])\n",
        "        print(f\"{name:<20} {model_type:<8} {results[name]['test_mse']:<12.6f} {results[name]['training_time']:<10.1f} {results[name]['params']:<12,} {efficiency:<10.2f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run 1D CNN demonstrations\n",
        "print(\"1D CONVOLUTION VISUALIZATION:\")\n",
        "visualize_1d_convolution()\n",
        "\n",
        "print(\"\\nRECEPTIVE FIELD ANALYSIS:\")\n",
        "rf_analysis = analyze_receptive_fields()\n",
        "\n",
        "print(\"\\nCNN vs RNN PERFORMANCE COMPARISON:\")\n",
        "cnn_rnn_results = compare_cnn_rnn_performance()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wavenet-theory"
      },
      "source": [
        "## 8. WaveNet Architecture\n",
        "\n",
        "### Theoretical Foundation\n",
        "\n",
        "WaveNet, introduced by DeepMind, uses **dilated convolutions** to achieve exponentially large receptive fields efficiently.\n",
        "\n",
        "### Key Innovations\n",
        "\n",
        "1. **Dilated Convolutions**: Exponentially increasing dilation rates\n",
        "2. **Causal Convolutions**: No future information leakage\n",
        "3. **Residual Connections**: Skip connections for gradient flow\n",
        "4. **Gated Activation Units**: Similar to LSTM gates\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "**Dilated Convolution:**\n",
        "$$y_t = \\sum_{i=0}^{k-1} w_i \\cdot x_{t - i \\cdot d}$$\n",
        "\n",
        "Where $d$ is the dilation rate.\n",
        "\n",
        "**Gated Activation Unit:**\n",
        "$$z = \\tanh(W_{f,k} * x) \\odot \\sigma(W_{g,k} * x)$$\n",
        "\n",
        "Where $W_{f,k}$ and $W_{g,k}$ are filter and gate convolutions, $*$ denotes convolution, and $\\odot$ is element-wise multiplication.\n",
        "\n",
        "**Residual Connection:**\n",
        "$$y = z + x$$\n",
        "\n",
        "### Receptive Field Growth\n",
        "\n",
        "With $L$ layers and dilation rates $d_l = 2^l$:\n",
        "$$\\text{Receptive Field} = 1 + \\sum_{l=0}^{L-1} 2^l (k-1) = 1 + (k-1)(2^L - 1)$$\n",
        "\n",
        "For $k=2$ and $L=10$: RF = $1 + (2-1)(2^{10} - 1) = 1024$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wavenet-implementation"
      },
      "source": [
        "# WaveNet Implementation\n",
        "\n",
        "def build_wavenet_block(inputs, filters, kernel_size, dilation_rate, name):\n",
        "    \"\"\"\n",
        "    Builds a WaveNet block with dilated convolution, gated activation, and residual connection.\n",
        "\n",
        "    Args:\n",
        "        inputs: Input tensor\n",
        "        filters: Number of filters\n",
        "        kernel_size: Convolution kernel size\n",
        "        dilation_rate: Dilation rate for convolution\n",
        "        name: Block name prefix\n",
        "\n",
        "    Returns:\n",
        "        Output tensor and skip connection\n",
        "    \"\"\"\n",
        "    # Dilated causal convolution for filter\n",
        "    conv_filter = keras.layers.Conv1D(\n",
        "        filters=filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding='causal',\n",
        "        name=f'{name}_conv_filter'\n",
        "    )(inputs)\n",
        "\n",
        "    # Dilated causal convolution for gate\n",
        "    conv_gate = keras.layers.Conv1D(\n",
        "        filters=filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding='causal',\n",
        "        name=f'{name}_conv_gate'\n",
        "    )(inputs)\n",
        "\n",
        "    # Gated activation unit\n",
        "    tanh_out = keras.layers.Activation('tanh', name=f'{name}_tanh')(conv_filter)\n",
        "    sigm_out = keras.layers.Activation('sigmoid', name=f'{name}_sigmoid')(conv_gate)\n",
        "    gated = keras.layers.Multiply(name=f'{name}_gated')([tanh_out, sigm_out])\n",
        "\n",
        "    # 1x1 convolution for residual connection\n",
        "    residual = keras.layers.Conv1D(\n",
        "        filters=filters,\n",
        "        kernel_size=1,\n",
        "        name=f'{name}_residual_conv'\n",
        "    )(gated)\n",
        "\n",
        "    # Skip connection (for final output)\n",
        "    skip = keras.layers.Conv1D(\n",
        "        filters=filters,\n",
        "        kernel_size=1,\n",
        "        name=f'{name}_skip_conv'\n",
        "    )(gated)\n",
        "\n",
        "    # Residual connection\n",
        "    # Note: Need to match dimensions for residual connection\n",
        "    if inputs.shape[-1] != filters:\n",
        "        inputs_proj = keras.layers.Conv1D(\n",
        "            filters=filters,\n",
        "            kernel_size=1,\n",
        "            name=f'{name}_input_proj'\n",
        "        )(inputs)\n",
        "    else:\n",
        "        inputs_proj = inputs\n",
        "\n",
        "    output = keras.layers.Add(name=f'{name}_residual_add')([inputs_proj, residual])\n",
        "\n",
        "    return output, skip\n",
        "\n",
        "def build_wavenet_model(input_shape, num_blocks=8, num_stacks=3, filters=32, kernel_size=2):\n",
        "    \"\"\"\n",
        "    Builds a complete WaveNet model.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Shape of input sequences\n",
        "        num_blocks: Number of blocks per stack\n",
        "        num_stacks: Number of stacks\n",
        "        filters: Number of filters per layer\n",
        "        kernel_size: Convolution kernel size\n",
        "\n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    inputs = keras.layers.Input(shape=input_shape, name='wavenet_input')\n",
        "\n",
        "    # Initial causal convolution\n",
        "    x = keras.layers.Conv1D(\n",
        "        filters=filters,\n",
        "        kernel_size=kernel_size,\n",
        "        padding='causal',\n",
        "        name='initial_conv'\n",
        "    )(inputs)\n",
        "\n",
        "    # Collect skip connections\n",
        "    skip_connections = []\n",
        "\n",
        "    # Build stacks of dilated convolution blocks\n",
        "    for stack in range(num_stacks):\n",
        "        for block in range(num_blocks):\n",
        "            dilation_rate = 2 ** block\n",
        "            block_name = f'stack_{stack}_block_{block}_dil_{dilation_rate}'\n",
        "\n",
        "            x, skip = build_wavenet_block(\n",
        "                x, filters, kernel_size, dilation_rate, block_name\n",
        "            )\n",
        "            skip_connections.append(skip)\n",
        "\n",
        "    # Sum all skip connections\n",
        "    skip_sum = keras.layers.Add(name='skip_sum')(skip_connections)\n",
        "\n",
        "    # Final processing\n",
        "    x = keras.layers.Activation('relu', name='final_relu1')(skip_sum)\n",
        "    x = keras.layers.Conv1D(filters=filters, kernel_size=1, name='final_conv1')(x)\n",
        "    x = keras.layers.Activation('relu', name='final_relu2')(x)\n",
        "    x = keras.layers.Conv1D(filters=1, kernel_size=1, name='final_conv2')(x)\n",
        "\n",
        "    # Global pooling for sequence-to-vector output\n",
        "    output = keras.layers.GlobalAveragePooling1D(name='global_pool')(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=output, name='WaveNet')\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_simplified_wavenet():\n",
        "    \"\"\"\n",
        "    Builds the simplified WaveNet as described in the book.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential(name='Simplified_WaveNet')\n",
        "    model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
        "\n",
        "    # Add dilated convolution layers as in the book\n",
        "    for rate in (1, 2, 4, 8) * 2:  # Repeat the pattern twice\n",
        "        model.add(keras.layers.Conv1D(\n",
        "            filters=20,\n",
        "            kernel_size=2,\n",
        "            padding=\"causal\",\n",
        "            activation=\"relu\",\n",
        "            dilation_rate=rate\n",
        "        ))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(keras.layers.Conv1D(filters=1, kernel_size=1))\n",
        "\n",
        "    # Global pooling to get single output\n",
        "    model.add(keras.layers.GlobalAveragePooling1D())\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "def analyze_wavenet_receptive_field():\n",
        "    \"\"\"\n",
        "    Analyzes and visualizes WaveNet's receptive field growth.\n",
        "    \"\"\"\n",
        "    print(\"\\nWAVENET RECEPTIVE FIELD ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Calculate receptive field for different WaveNet configurations\n",
        "    configs = {\n",
        "        'Book Example (1,2,4,8)×2': (4, 2, 2),  # (blocks, kernel_size, stacks)\n",
        "        'Small WaveNet (1,2,4)×3': (3, 2, 3),\n",
        "        'Standard WaveNet (1-512)×3': (10, 2, 3),\n",
        "        'Large WaveNet (1-1024)×4': (11, 2, 4)\n",
        "    }\n",
        "\n",
        "    def calculate_wavenet_rf(num_blocks, kernel_size, num_stacks):\n",
        "        \"\"\"Calculate WaveNet receptive field.\"\"\"\n",
        "        # Each stack has dilation rates: 1, 2, 4, ..., 2^(num_blocks-1)\n",
        "        rf = 1  # Initial point\n",
        "        for stack in range(num_stacks):\n",
        "            for block in range(num_blocks):\n",
        "                dilation = 2 ** block\n",
        "                rf += (kernel_size - 1) * dilation\n",
        "        return rf\n",
        "\n",
        "    rf_results = {}\n",
        "    for name, (blocks, kernel_size, stacks) in configs.items():\n",
        "        rf = calculate_wavenet_rf(blocks, kernel_size, stacks)\n",
        "        rf_results[name] = rf\n",
        "        print(f\"{name:<25}: RF = {rf:>6} (blocks={blocks}, stacks={stacks})\")\n",
        "\n",
        "    # Visualize receptive field growth\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # 1. Receptive field by layer for book example\n",
        "    ax = axes[0, 0]\n",
        "    layers = []\n",
        "    rfs = [1]  # Start with RF of 1\n",
        "\n",
        "    # Book example: (1,2,4,8) × 2\n",
        "    current_rf = 1\n",
        "    layer_count = 0\n",
        "    for stack in range(2):\n",
        "        for dilation in [1, 2, 4, 8]:\n",
        "            current_rf += (2 - 1) * dilation  # kernel_size = 2\n",
        "            layer_count += 1\n",
        "            layers.append(layer_count)\n",
        "            rfs.append(current_rf)\n",
        "\n",
        "    ax.plot(layers, rfs[1:], 'o-', linewidth=2, markersize=8)\n",
        "    ax.set_title('Receptive Field Growth (Book Example)')\n",
        "    ax.set_xlabel('Layer Number')\n",
        "    ax.set_ylabel('Receptive Field Size')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add dilation rate annotations\n",
        "    dilations = [1, 2, 4, 8] * 2\n",
        "    for i, (layer, rf, dil) in enumerate(zip(layers, rfs[1:], dilations)):\n",
        "        ax.annotate(f'd={dil}', (layer, rf), xytext=(0, 10),\n",
        "                   textcoords='offset points', ha='center', fontsize=9)\n",
        "\n",
        "    # 2. Comparison of different configurations\n",
        "    ax = axes[0, 1]\n",
        "    names = list(rf_results.keys())\n",
        "    rfs_values = list(rf_results.values())\n",
        "\n",
        "    bars = ax.bar(range(len(names)), rfs_values, alpha=0.7, color='skyblue')\n",
        "    ax.set_title('Receptive Field Comparison')\n",
        "    ax.set_xlabel('Configuration')\n",
        "    ax.set_ylabel('Receptive Field Size')\n",
        "    ax.set_xticks(range(len(names)))\n",
        "    ax.set_xticklabels(names, rotation=45, ha='right')\n",
        "    ax.set_yscale('log')\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, rfs_values):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
        "                f'{value}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # 3. Exponential growth visualization\n",
        "    ax = axes[1, 0]\n",
        "\n",
        "    # Show how dilation creates exponential growth\n",
        "    dilations = [2**i for i in range(8)]\n",
        "    cumulative_rf = np.cumsum([1] + dilations)\n",
        "\n",
        "    ax.semilogy(range(len(cumulative_rf)), cumulative_rf, 'ro-',\n",
        "               linewidth=2, markersize=8, label='WaveNet (exponential)')\n",
        "\n",
        "    # Compare with linear growth\n",
        "    linear_rf = np.arange(1, len(cumulative_rf) + 1) * 3  # kernel_size = 3\n",
        "    ax.semilogy(range(len(linear_rf)), linear_rf, 'bs-',\n",
        "               linewidth=2, markersize=6, label='Standard CNN (linear)')\n",
        "\n",
        "    ax.set_title('Receptive Field Growth: Exponential vs Linear')\n",
        "    ax.set_xlabel('Layer Number')\n",
        "    ax.set_ylabel('Receptive Field Size (log scale)')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Memory efficiency comparison\n",
        "    ax = axes[1, 1]\n",
        "\n",
        "    # Calculate parameters for different approaches to achieve RF=1024\n",
        "    target_rf = 1024\n",
        "\n",
        "    approaches = {\n",
        "        'WaveNet\\n(dilation)': 10 * 20 * 2 * 2,  # 10 layers, 20 filters, kernel=2, 2 feature maps\n",
        "        'Standard CNN\\n(large kernels)': 3 * 20 * 341 * 1,  # 3 layers, 20 filters, kernel=341\n",
        "        'Deep CNN\\n(small kernels)': 341 * 20 * 3 * 1,  # 341 layers, 20 filters, kernel=3\n",
        "        'LSTM\\n(sequential)': 1024 * 4 * 20 * 20  # LSTM parameters for 20 units\n",
        "    }\n",
        "\n",
        "    approach_names = list(approaches.keys())\n",
        "    param_counts = list(approaches.values())\n",
        "\n",
        "    bars = ax.bar(range(len(approach_names)), param_counts, alpha=0.7,\n",
        "                 color=['green', 'red', 'orange', 'blue'])\n",
        "    ax.set_title('Parameter Efficiency for RF=1024')\n",
        "    ax.set_xlabel('Approach')\n",
        "    ax.set_ylabel('Number of Parameters')\n",
        "    ax.set_xticks(range(len(approach_names)))\n",
        "    ax.set_xticklabels(approach_names)\n",
        "    ax.set_yscale('log')\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, param_counts):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
        "                f'{value:,}', ha='center', va='bottom', fontsize=9, rotation=90)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return rf_results\n",
        "\n",
        "def demonstrate_wavenet_performance():\n",
        "    \"\"\"\n",
        "    Demonstrates WaveNet performance on time series forecasting.\n",
        "    \"\"\"\n",
        "    print(\"\\nWAVENET PERFORMANCE DEMONSTRATION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Build different WaveNet variants\n",
        "    models = {\n",
        "        'Simplified WaveNet (Book)': build_simplified_wavenet(),\n",
        "        'Small WaveNet': build_wavenet_model(\n",
        "            input_shape=[None, 1],\n",
        "            num_blocks=4,\n",
        "            num_stacks=2,\n",
        "            filters=16\n",
        "        ),\n",
        "        'Medium WaveNet': build_wavenet_model(\n",
        "            input_shape=[None, 1],\n",
        "            num_blocks=6,\n",
        "            num_stacks=2,\n",
        "            filters=32\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Add baseline models for comparison\n",
        "    models['LSTM Baseline'] = keras.Sequential([\n",
        "        keras.layers.LSTM(32, return_sequences=True, input_shape=[None, 1]),\n",
        "        keras.layers.LSTM(32),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    models['CNN Baseline'] = keras.Sequential([\n",
        "        keras.layers.Conv1D(32, 5, activation='relu', input_shape=[None, 1]),\n",
        "        keras.layers.Conv1D(32, 5, activation='relu'),\n",
        "        keras.layers.GlobalMaxPooling1D(),\n",
        "        keras.layers.Dense(50, activation='relu'),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Compile baseline models\n",
        "    models['LSTM Baseline'].compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    models['CNN Baseline'].compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "    # Train and evaluate models\n",
        "    results = {}\n",
        "    histories = {}\n",
        "\n",
        "    epochs = 15\n",
        "    batch_size = 32\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        print(f\"Parameters: {model.count_params():,}\")\n",
        "\n",
        "        # Display model architecture for WaveNet models\n",
        "        if 'WaveNet' in name:\n",
        "            print(f\"Model summary for {name}:\")\n",
        "            model.summary()\n",
        "\n",
        "        import time\n",
        "        start_time = time.time()\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(X_valid, y_valid),\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Evaluate\n",
        "        test_loss = model.evaluate(X_test, y_test, verbose=0)[0]\n",
        "\n",
        "        results[name] = {\n",
        "            'test_mse': test_loss,\n",
        "            'params': model.count_params(),\n",
        "            'training_time': training_time,\n",
        "            'best_val_loss': min(history.history['val_loss'])\n",
        "        }\n",
        "\n",
        "        histories[name] = history\n",
        "\n",
        "        print(f\"Test MSE: {test_loss:.6f}, Training time: {training_time:.1f}s\")\n",
        "\n",
        "    # Comprehensive visualization\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
        "\n",
        "    # 1. Performance comparison\n",
        "    ax = axes[0, 0]\n",
        "    names = list(results.keys())\n",
        "    test_mses = [results[name]['test_mse'] for name in names]\n",
        "    colors = ['green' if 'WaveNet' in name else 'blue' if 'LSTM' in name else 'red' for name in names]\n",
        "\n",
        "    bars = ax.bar(range(len(names)), test_mses, alpha=0.7, color=colors)\n",
        "    ax.set_title('Test MSE Comparison')\n",
        "    ax.set_xlabel('Model')\n",
        "    ax.set_ylabel('Test MSE')\n",
        "    ax.set_xticks(range(len(names)))\n",
        "    ax.set_xticklabels(names, rotation=45, ha='right')\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, test_mses):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0001,\n",
        "                f'{value:.4f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # 2. Training curves\n",
        "    ax = axes[0, 1]\n",
        "    for name, history in histories.items():\n",
        "        color = 'green' if 'WaveNet' in name else 'blue' if 'LSTM' in name else 'red'\n",
        "        ax.plot(history.history['val_loss'], label=name, linewidth=2, color=color, alpha=0.8)\n",
        "\n",
        "    ax.set_title('Validation Loss Curves')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Validation Loss')\n",
        "    ax.legend()\n",
        "    ax.set_yscale('log')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Parameter efficiency\n",
        "    ax = axes[1, 0]\n",
        "    param_counts = [results[name]['params'] for name in names]\n",
        "\n",
        "    scatter = ax.scatter(param_counts, test_mses, c=colors, s=100, alpha=0.7)\n",
        "    for i, name in enumerate(names):\n",
        "        ax.annotate(name, (param_counts[i], test_mses[i]),\n",
        "                   xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "\n",
        "    ax.set_xlabel('Number of Parameters')\n",
        "    ax.set_ylabel('Test MSE')\n",
        "    ax.set_title('Parameter Efficiency')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Training time efficiency\n",
        "    ax = axes[1, 1]\n",
        "    training_times = [results[name]['training_time'] for name in names]\n",
        "\n",
        "    scatter = ax.scatter(training_times, test_mses, c=colors, s=100, alpha=0.7)\n",
        "    for i, name in enumerate(names):\n",
        "        ax.annotate(name, (training_times[i], test_mses[i]),\n",
        "                   xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "\n",
        "    ax.set_xlabel('Training Time (seconds)')\n",
        "    ax.set_ylabel('Test MSE')\n",
        "    ax.set_title('Training Time Efficiency')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # 5. Prediction examples\n",
        "    ax = axes[2, 0]\n",
        "    best_model_name = min(names, key=lambda x: results[x]['test_mse'])\n",
        "    best_model = models[best_model_name]\n",
        "\n",
        "    # Show predictions for a few test examples\n",
        "    n_examples = 3\n",
        "    for i in range(n_examples):\n",
        "        input_seq = X_test[i, :, 0]\n",
        "        true_val = y_test[i]\n",
        "        pred_val = best_model.predict(X_test[i:i+1], verbose=0)[0]\n",
        "\n",
        "        # Plot input sequence\n",
        "        ax.plot(range(len(input_seq)), input_seq, 'b-', alpha=0.6, linewidth=1)\n",
        "\n",
        "        # Plot true and predicted next values\n",
        "        ax.scatter(len(input_seq), true_val, color='red', s=50, alpha=0.8,\n",
        "                  label='True' if i == 0 else '')\n",
        "        ax.scatter(len(input_seq), pred_val, color='green', s=50, marker='x', alpha=0.8,\n",
        "                  label='Predicted' if i == 0 else '')\n",
        "\n",
        "    ax.set_title(f'Predictions: {best_model_name}')\n",
        "    ax.set_xlabel('Time Step')\n",
        "    ax.set_ylabel('Value')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # 6. Architecture comparison summary\n",
        "    ax = axes[2, 1]\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Create a summary table\n",
        "    table_data = []\n",
        "    for name in names:\n",
        "        table_data.append([\n",
        "            name,\n",
        "            f\"{results[name]['test_mse']:.6f}\",\n",
        "            f\"{results[name]['params']:,}\",\n",
        "            f\"{results[name]['training_time']:.1f}s\"\n",
        "        ])\n",
        "\n",
        "    table = ax.table(\n",
        "        cellText=table_data,\n",
        "        colLabels=['Model', 'Test MSE', 'Parameters', 'Training Time'],\n",
        "        cellLoc='center',\n",
        "        loc='center'\n",
        "    )\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(9)\n",
        "    table.scale(1.2, 1.5)\n",
        "\n",
        "    ax.set_title('Performance Summary', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results, models\n",
        "\n",
        "# Run WaveNet demonstrations\n",
        "print(\"WAVENET RECEPTIVE FIELD ANALYSIS:\")\n",
        "wavenet_rf_results = analyze_wavenet_receptive_field()\n",
        "\n",
        "print(\"\\nWAVENET PERFORMANCE DEMONSTRATION:\")\n",
        "wavenet_results, wavenet_models = demonstrate_wavenet_performance()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercises-header"
      },
      "source": [
        "## 9. Comprehensive Exercises and Solutions\n",
        "\n",
        "This section provides detailed solutions to all exercises from Chapter 15, with theoretical explanations and practical implementations.\n",
        "\n",
        "### Exercise Framework\n",
        "\n",
        "Each exercise solution includes:\n",
        "1. **Theoretical Analysis**: Mathematical foundations and concepts\n",
        "2. **Implementation**: Working code with detailed explanations\n",
        "3. **Experimental Results**: Performance analysis and visualizations\n",
        "4. **Extensions**: Additional considerations and improvements\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise-1"
      },
      "source": [
        "### Exercise 1: Applications for Different RNN Architectures\n",
        "\n",
        "**Question**: Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN, and a vector-to-sequence RNN?\n",
        "\n",
        "#### Theoretical Analysis\n",
        "\n",
        "Different RNN architectures are suited for different types of sequential problems based on their input-output mapping:\n",
        "\n",
        "**Sequence-to-Sequence RNNs:**\n",
        "- **Mathematical Framework**: $f: \\mathbb{R}^{T_{in} \\times d_{in}} \\rightarrow \\mathbb{R}^{T_{out} \\times d_{out}}$\n",
        "- **Characteristics**: Both input and output are sequences, potentially of different lengths\n",
        "\n",
        "**Sequence-to-Vector RNNs:**\n",
        "- **Mathematical Framework**: $f: \\mathbb{R}^{T \\times d_{in}} \\rightarrow \\mathbb{R}^{d_{out}}$\n",
        "- **Characteristics**: Input is a sequence, output is a fixed-size vector\n",
        "\n",
        "**Vector-to-Sequence RNNs:**\n",
        "- **Mathematical Framework**: $f: \\mathbb{R}^{d_{in}} \\rightarrow \\mathbb{R}^{T \\times d_{out}}$\n",
        "- **Characteristics**: Input is a fixed-size vector, output is a sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise-1-implementation"
      },
      "source": [
        "# Exercise 1: RNN Architecture Applications\n",
        "\n",
        "def demonstrate_rnn_applications():\n",
        "    \"\"\"\n",
        "    Demonstrates different RNN architectures with practical examples.\n",
        "    \"\"\"\n",
        "    print(\"EXERCISE 1: RNN ARCHITECTURE APPLICATIONS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    applications = {\n",
        "        'Sequence-to-Sequence': {\n",
        "            'applications': [\n",
        "                'Machine Translation (English → French)',\n",
        "                'Speech Recognition (Audio → Text)',\n",
        "                'Time Series Forecasting (Past → Future)',\n",
        "                'Video Captioning (Video Frames → Description)',\n",
        "                'Code Generation (Comments → Code)',\n",
        "                'Chatbot Responses (Question → Answer)',\n",
        "                'Music Generation (Melody → Harmony)',\n",
        "                'DNA Sequence Analysis (Input Sequence → Output Sequence)'\n",
        "            ],\n",
        "            'mathematical_form': 'f: R^(T_in × d_in) → R^(T_out × d_out)',\n",
        "            'key_features': [\n",
        "                'Both input and output are sequences',\n",
        "                'Can handle variable-length inputs and outputs',\n",
        "                'Often uses encoder-decoder architecture',\n",
        "                'Attention mechanisms commonly used'\n",
        "            ]\n",
        "        },\n",
        "\n",
        "        'Sequence-to-Vector': {\n",
        "            'applications': [\n",
        "                'Sentiment Analysis (Text → Sentiment Score)',\n",
        "                'Document Classification (Document → Category)',\n",
        "                'Intent Recognition (Speech → Intent)',\n",
        "                'Anomaly Detection (Time Series → Anomaly Score)',\n",
        "                'Feature Extraction (Sequence → Embedding)',\n",
        "                'Health Monitoring (Sensor Data → Health Status)',\n",
        "                'Stock Market Prediction (Price History → Buy/Sell)',\n",
        "                'Protein Function Prediction (Sequence → Function)'\n",
        "            ],\n",
        "            'mathematical_form': 'f: R^(T × d_in) → R^d_out',\n",
        "            'key_features': [\n",
        "                'Processes entire sequence to single output',\n",
        "                'Global pooling or final hidden state used',\n",
        "                'Good for classification/regression tasks',\n",
        "                'Summarizes sequential information'\n",
        "            ]\n",
        "        },\n",
        "\n",
        "        'Vector-to-Sequence': {\n",
        "            'applications': [\n",
        "                'Image Captioning (Image → Caption)',\n",
        "                'Music Generation (Style Vector → Melody)',\n",
        "                'Text Generation (Topic → Article)',\n",
        "                'Data Augmentation (Seed → Synthetic Sequence)',\n",
        "                'Story Generation (Theme → Story)',\n",
        "                'Code Generation (Specification → Implementation)',\n",
        "                'Weather Forecasting (Current State → Forecast)',\n",
        "                'Drug Discovery (Molecule → Properties)'\n",
        "            ],\n",
        "            'mathematical_form': 'f: R^d_in → R^(T × d_out)',\n",
        "            'key_features': [\n",
        "                'Single input generates sequence output',\n",
        "                'Input often repeated at each time step',\n",
        "                'Useful for generation tasks',\n",
        "                'Often combined with attention mechanisms'\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create comprehensive visualization\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
        "\n",
        "    for idx, (arch_type, info) in enumerate(applications.items()):\n",
        "        # Architecture diagram\n",
        "        ax1 = axes[idx, 0]\n",
        "        ax1.set_title(f'{arch_type} Architecture', fontweight='bold', fontsize=12)\n",
        "\n",
        "        if arch_type == 'Sequence-to-Sequence':\n",
        "            # Draw seq2seq diagram\n",
        "            # Input sequence\n",
        "            for i in range(4):\n",
        "                ax1.add_patch(plt.Rectangle((i*0.2, 0.3), 0.15, 0.2,\n",
        "                                          facecolor='lightblue', edgecolor='black'))\n",
        "                ax1.text(i*0.2 + 0.075, 0.4, f'x{i+1}', ha='center', va='center', fontsize=8)\n",
        "                # Arrow to RNN\n",
        "                ax1.arrow(i*0.2 + 0.075, 0.5, 0, 0.1, head_width=0.02, head_length=0.02, fc='black')\n",
        "                # RNN cell\n",
        "                ax1.add_patch(plt.Rectangle((i*0.2, 0.6), 0.15, 0.15,\n",
        "                                          facecolor='yellow', edgecolor='black'))\n",
        "                ax1.text(i*0.2 + 0.075, 0.675, 'RNN', ha='center', va='center', fontsize=7)\n",
        "                # Arrow to output\n",
        "                ax1.arrow(i*0.2 + 0.075, 0.75, 0, 0.1, head_width=0.02, head_length=0.02, fc='black')\n",
        "                # Output\n",
        "                ax1.add_patch(plt.Rectangle((i*0.2, 0.85), 0.15, 0.1,\n",
        "                                          facecolor='lightgreen', edgecolor='black'))\n",
        "                ax1.text(i*0.2 + 0.075, 0.9, f'y{i+1}', ha='center', va='center', fontsize=8)\n",
        "\n",
        "            # Horizontal connections\n",
        "            for i in range(3):\n",
        "                ax1.arrow((i+1)*0.2 - 0.025, 0.675, 0.05, 0, head_width=0.02, head_length=0.02, fc='red')\n",
        "\n",
        "        elif arch_type == 'Sequence-to-Vector':\n",
        "            # Draw seq2vec diagram\n",
        "            for i in range(4):\n",
        "                ax1.add_patch(plt.Rectangle((i*0.2, 0.3), 0.15, 0.2,\n",
        "                                          facecolor='lightblue', edgecolor='black'))\n",
        "                ax1.text(i*0.2 + 0.075, 0.4, f'x{i+1}', ha='center', va='center', fontsize=8)\n",
        "                ax1.arrow(i*0.2 + 0.075, 0.5, 0, 0.1, head_width=0.02, head_length=0.02, fc='black')\n",
        "                ax1.add_patch(plt.Rectangle((i*0.2, 0.6), 0.15, 0.15,\n",
        "                                          facecolor='yellow', edgecolor='black'))\n",
        "                ax1.text(i*0.2 + 0.075, 0.675, 'RNN', ha='center', va='center', fontsize=7)\n",
        "                if i < 3:\n",
        "                    ax1.arrow((i+1)*0.2 - 0.025, 0.675, 0.05, 0, head_width=0.02, head_length=0.02, fc='red')\n",
        "\n",
        "            # Final output\n",
        "            ax1.arrow(0.6, 0.75, 0, 0.1, head_width=0.02, head_length=0.02, fc='black')\n",
        "            ax1.add_patch(plt.Rectangle((0.5, 0.85), 0.2, 0.1,\n",
        "                                      facecolor='lightgreen', edgecolor='black'))\n",
        "            ax1.text(0.6, 0.9, 'y', ha='center', va='center', fontsize=10)\n",
        "\n",
        "        else:  # Vector-to-Sequence\n",
        "            # Draw vec2seq diagram\n",
        "            # Single input\n",
        "            ax1.add_patch(plt.Rectangle((0.4, 0.2), 0.2, 0.15,\n",
        "                                      facecolor='lightblue', edgecolor='black'))\n",
        "            ax1.text(0.5, 0.275, 'x', ha='center', va='center', fontsize=10)\n",
        "\n",
        "            # Multiple outputs\n",
        "            for i in range(4):\n",
        "                ax1.arrow(0.5, 0.35, (i-1.5)*0.15, 0.25, head_width=0.02, head_length=0.02, fc='black')\n",
        "                ax1.add_patch(plt.Rectangle((i*0.2, 0.6), 0.15, 0.15,\n",
        "                                          facecolor='yellow', edgecolor='black'))\n",
        "                ax1.text(i*0.2 + 0.075, 0.675, 'RNN', ha='center', va='center', fontsize=7)\n",
        "                ax1.arrow(i*0.2 + 0.075, 0.75, 0, 0.1, head_width=0.02, head_length=0.02, fc='black')\n",
        "                ax1.add_patch(plt.Rectangle((i*0.2, 0.85), 0.15, 0.1,\n",
        "                                          facecolor='lightgreen', edgecolor='black'))\n",
        "                ax1.text(i*0.2 + 0.075, 0.9, f'y{i+1}', ha='center', va='center', fontsize=8)\n",
        "                if i < 3:\n",
        "                    ax1.arrow((i+1)*0.2 - 0.025, 0.675, 0.05, 0, head_width=0.02, head_length=0.02, fc='red')\n",
        "\n",
        "        ax1.set_xlim(-0.1, 1.0)\n",
        "        ax1.set_ylim(0, 1)\n",
        "        ax1.axis('off')\n",
        "\n",
        "        # Applications list\n",
        "        ax2 = axes[idx, 1]\n",
        "        ax2.set_title(f'Applications', fontweight='bold', fontsize=12)\n",
        "\n",
        "        # Create text summary\n",
        "        text_content = f\"Mathematical Form:\\n{info['mathematical_form']}\\n\\n\"\n",
        "        text_content += \"Key Features:\\n\"\n",
        "        for feature in info['key_features']:\n",
        "            text_content += f\"• {feature}\\n\"\n",
        "\n",
        "        text_content += \"\\nExample Applications:\\n\"\n",
        "        for i, app in enumerate(info['applications'][:6]):  # Show first 6\n",
        "            text_content += f\"{i+1}. {app}\\n\"\n",
        "\n",
        "        ax2.text(0.05, 0.95, text_content, transform=ax2.transAxes,\n",
        "                fontsize=9, verticalalignment='top', fontfamily='monospace')\n",
        "        ax2.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed analysis\n",
        "    print(\"\\nDETAILED ANALYSIS:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for arch_type, info in applications.items():\n",
        "        print(f\"\\n{arch_type.upper()}:\")\n",
        "        print(f\"Mathematical Form: {info['mathematical_form']}\")\n",
        "        print(\"\\nKey Characteristics:\")\n",
        "        for feature in info['key_features']:\n",
        "            print(f\"  • {feature}\")\n",
        "        print(\"\\nPractical Applications:\")\n",
        "        for i, app in enumerate(info['applications'], 1):\n",
        "            print(f\"  {i:2d}. {app}\")\n",
        "\n",
        "    return applications\n",
        "\n",
        "# Run Exercise 1\n",
        "exercise_1_results = demonstrate_rnn_applications()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise-2"
      },
      "source": [
        "### Exercise 2: RNN Input and Output Dimensions\n",
        "\n",
        "**Question**: How many dimensions must the inputs of an RNN layer have? What does each dimension represent? What about its outputs?\n",
        "\n",
        "#### Theoretical Analysis\n",
        "\n",
        "RNN layers in deep learning frameworks follow specific tensor dimension conventions:\n",
        "\n",
        "**Input Tensor Dimensions**: `[batch_size, time_steps, features]`\n",
        "- **batch_size**: Number of sequences processed simultaneously\n",
        "- **time_steps**: Length of each sequence (can be variable with padding)\n",
        "- **features**: Number of features at each time step\n",
        "\n",
        "**Mathematical Representation**:\n",
        "$$\\mathbf{X} \\in \\mathbb{R}^{B \\times T \\times D}$$\n",
        "\n",
        "Where:\n",
        "- $B$ = batch size\n",
        "- $T$ = time steps\n",
        "- $D$ = input feature dimension\n",
        "\n",
        "**Output Tensor Dimensions**:\n",
        "- **Without `return_sequences=True`**: `[batch_size, units]`\n",
        "- **With `return_sequences=True`**: `[batch_size, time_steps, units]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise-2-implementation"
      },
      "source": [
        "# Exercise 2: RNN Input/Output Dimensions\n",
        "\n",
        "def analyze_rnn_dimensions():\n",
        "    \"\"\"\n",
        "    Comprehensive analysis of RNN input and output dimensions.\n",
        "    \"\"\"\n",
        "    print(\"EXERCISE 2: RNN INPUT/OUTPUT DIMENSIONS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create sample data with different configurations\n",
        "    configs = {\n",
        "        'Univariate Time Series': {\n",
        "            'batch_size': 32,\n",
        "            'time_steps': 50,\n",
        "            'features': 1,\n",
        "            'description': 'Single feature per time step (e.g., stock price)'\n",
        "        },\n",
        "        'Multivariate Time Series': {\n",
        "            'batch_size': 16,\n",
        "            'time_steps': 100,\n",
        "            'features': 5,\n",
        "            'description': 'Multiple features per time step (e.g., weather data)'\n",
        "        },\n",
        "        'Text Sequences (Word Embeddings)': {\n",
        "            'batch_size': 64,\n",
        "            'time_steps': 20,\n",
        "            'features': 300,\n",
        "            'description': 'Word vectors in sentences'\n",
        "        },\n",
        "        'Audio Features': {\n",
        "            'batch_size': 8,\n",
        "            'time_steps': 1000,\n",
        "            'features': 13,\n",
        "            'description': 'MFCC features for speech recognition'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Demonstrate different RNN configurations\n",
        "    print(\"\\nRNN LAYER CONFIGURATIONS:\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for config_name, config in configs.items():\n",
        "        print(f\"\\n{config_name}:\")\n",
        "        print(f\"Description: {config['description']}\")\n",
        "\n",
        "        batch_size = config['batch_size']\n",
        "        time_steps = config['time_steps']\n",
        "        features = config['features']\n",
        "\n",
        "        # Create sample input data\n",
        "        X = np.random.randn(batch_size, time_steps, features)\n",
        "        print(f\"Input shape: {X.shape}\")\n",
        "        print(f\"  • Batch size: {batch_size} (number of sequences)\")\n",
        "        print(f\"  • Time steps: {time_steps} (sequence length)\")\n",
        "        print(f\"  • Features: {features} (features per time step)\")\n",
        "\n",
        "        # Test different RNN configurations\n",
        "        rnn_configs = {\n",
        "            'Basic RNN (last output only)': {\n",
        "                'return_sequences': False,\n",
        "                'units': 64\n",
        "            },\n",
        "            'RNN with all outputs': {\n",
        "                'return_sequences': True,\n",
        "                'units': 64\n",
        "            },\n",
        "            'Bidirectional RNN': {\n",
        "                'return_sequences': True,\n",
        "                'units': 32,\n",
        "                'bidirectional': True\n",
        "            }\n",
        "        }\n",
        "\n",
        "        config_results = {}\n",
        "\n",
        "        for rnn_name, rnn_config in rnn_configs.items():\n",
        "            print(f\"\\n  {rnn_name}:\")\n",
        "\n",
        "            # Build model\n",
        "            if rnn_config.get('bidirectional', False):\n",
        "                layer = keras.layers.Bidirectional(\n",
        "                    keras.layers.LSTM(rnn_config['units'],\n",
        "                                     return_sequences=rnn_config['return_sequences']),\n",
        "                    input_shape=(time_steps, features)\n",
        "                )\n",
        "            else:\n",
        "                layer = keras.layers.LSTM(\n",
        "                    rnn_config['units'],\n",
        "                    return_sequences=rnn_config['return_sequences'],\n",
        "                    input_shape=(time_steps, features)\n",
        "                )\n",
        "\n",
        "            model = keras.Sequential([layer])\n",
        "\n",
        "            # Get output shape\n",
        "            output = model(X)\n",
        "            print(f\"    Output shape: {output.shape}\")\n",
        "\n",
        "            # Explain dimensions\n",
        "            if len(output.shape) == 2:\n",
        "                print(f\"    • Batch size: {output.shape[0]}\")\n",
        "                print(f\"    • Units: {output.shape[1]}\")\n",
        "                print(f\"    • Returns: Final hidden state only\")\n",
        "            else:\n",
        "                print(f\"    • Batch size: {output.shape[0]}\")\n",
        "                print(f\"    • Time steps: {output.shape[1]}\")\n",
        "                print(f\"    • Units: {output.shape[2]}\")\n",
        "                print(f\"    • Returns: Hidden state at each time step\")\n",
        "\n",
        "            config_results[rnn_name] = {\n",
        "                'input_shape': X.shape,\n",
        "                'output_shape': output.shape,\n",
        "                'parameters': model.count_params()\n",
        "            }\n",
        "\n",
        "        results[config_name] = {\n",
        "            'config': config,\n",
        "            'rnn_results': config_results\n",
        "        }\n",
        "\n",
        "    # Create comprehensive visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # 1. Input dimension analysis\n",
        "    ax = axes[0, 0]\n",
        "    config_names = list(configs.keys())\n",
        "    batch_sizes = [configs[name]['batch_size'] for name in config_names]\n",
        "    time_steps = [configs[name]['time_steps'] for name in config_names]\n",
        "    features = [configs[name]['features'] for name in config_names]\n",
        "\n",
        "    x_pos = np.arange(len(config_names))\n",
        "    width = 0.25\n",
        "\n",
        "    ax.bar(x_pos - width, batch_sizes, width, label='Batch Size', alpha=0.7)\n",
        "    ax.bar(x_pos, time_steps, width, label='Time Steps', alpha=0.7)\n",
        "    ax.bar(x_pos + width, features, width, label='Features', alpha=0.7)\n",
        "\n",
        "    ax.set_title('Input Dimension Analysis')\n",
        "    ax.set_xlabel('Configuration')\n",
        "    ax.set_ylabel('Dimension Size')\n",
        "    ax.set_xticks(x_pos)\n",
        "    ax.set_xticklabels(config_names, rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    ax.set_yscale('log')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Memory usage analysis\n",
        "    ax = axes[0, 1]\n",
        "    memory_usage = []\n",
        "    for config in configs.values():\n",
        "        # Calculate memory usage (in MB, assuming float32)\n",
        "        memory = config['batch_size'] * config['time_steps'] * config['features'] * 4 / (1024**2)\n",
        "        memory_usage.append(memory)\n",
        "\n",
        "    bars = ax.bar(config_names, memory_usage, alpha=0.7, color='orange')\n",
        "    ax.set_title('Input Tensor Memory Usage')\n",
        "    ax.set_xlabel('Configuration')\n",
        "    ax.set_ylabel('Memory (MB)')\n",
        "    ax.set_xticklabels(config_names, rotation=45, ha='right')\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, memory_usage):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "                f'{value:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    # 3. Output shape comparison\n",
        "    ax = axes[1, 0]\n",
        "\n",
        "    # Create a visual representation of different output shapes\n",
        "    output_types = ['Last Output Only', 'All Outputs', 'Bidirectional']\n",
        "    y_positions = [2, 1, 0]\n",
        "\n",
        "    for i, output_type in enumerate(output_types):\n",
        "        y = y_positions[i]\n",
        "\n",
        "        if output_type == 'Last Output Only':\n",
        "            # Draw 2D tensor\n",
        "            rect = plt.Rectangle((1, y), 2, 0.5, facecolor='lightblue',\n",
        "                               edgecolor='black', alpha=0.7)\n",
        "            ax.add_patch(rect)\n",
        "            ax.text(2, y+0.25, '[batch, units]', ha='center', va='center', fontsize=10)\n",
        "\n",
        "        elif output_type == 'All Outputs':\n",
        "            # Draw 3D tensor representation\n",
        "            for j in range(3):\n",
        "                rect = plt.Rectangle((1+j*0.1, y+j*0.1), 2, 0.5,\n",
        "                                   facecolor='lightgreen', edgecolor='black', alpha=0.7)\n",
        "                ax.add_patch(rect)\n",
        "            ax.text(2.1, y+0.35, '[batch, time, units]', ha='center', va='center', fontsize=10)\n",
        "\n",
        "        else:  # Bidirectional\n",
        "            # Draw wider 3D tensor\n",
        "            for j in range(3):\n",
        "                rect = plt.Rectangle((1+j*0.1, y+j*0.1), 2.5, 0.5,\n",
        "                                   facecolor='lightcoral', edgecolor='black', alpha=0.7)\n",
        "                ax.add_patch(rect)\n",
        "            ax.text(2.35, y+0.35, '[batch, time, 2×units]', ha='center', va='center', fontsize=10)\n",
        "\n",
        "        ax.text(0.5, y+0.25, output_type, ha='right', va='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "    ax.set_xlim(0, 4.5)\n",
        "    ax.set_ylim(-0.5, 3)\n",
        "    ax.set_title('RNN Output Shape Types')\n",
        "    ax.axis('off')\n",
        "\n",
        "    # 4. Dimension explanation table\n",
        "    ax = axes[1, 1]\n",
        "    ax.axis('off')\n",
        "\n",
        "    table_data = [\n",
        "        ['Dimension', 'Symbol', 'Description', 'Example'],\n",
        "        ['Batch Size', 'B', 'Number of sequences processed together', '32'],\n",
        "        ['Time Steps', 'T', 'Length of each sequence', '50'],\n",
        "        ['Input Features', 'D_in', 'Features per time step', '1-300'],\n",
        "        ['RNN Units', 'D_out', 'Size of hidden state', '64-512'],\n",
        "        ['Output (no seq)', '[B, D_out]', 'Final hidden state only', '[32, 64]'],\n",
        "        ['Output (with seq)', '[B, T, D_out]', 'All hidden states', '[32, 50, 64]']\n",
        "    ]\n",
        "\n",
        "    table = ax.table(\n",
        "        cellText=table_data[1:],\n",
        "        colLabels=table_data[0],\n",
        "        cellLoc='center',\n",
        "        loc='center'\n",
        "    )\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(9)\n",
        "    table.scale(1.2, 2)\n",
        "\n",
        "    # Style the header\n",
        "    for i in range(len(table_data[0])):\n",
        "        table[(0, i)].set_facecolor('#40466e')\n",
        "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "    ax.set_title('RNN Dimension Reference', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Mathematical formulation\n",
        "    print(\"\\nMATHEMATICAL FORMULATION:\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"Input Tensor: X ∈ ℝ^(B×T×D_in)\")\n",
        "    print(\"Where:\")\n",
        "    print(\"  • B = batch_size (number of sequences)\")\n",
        "    print(\"  • T = time_steps (sequence length)\")\n",
        "    print(\"  • D_in = input_features (features per time step)\")\n",
        "    print(\"\\nOutput Tensor:\")\n",
        "    print(\"  • return_sequences=False: Y ∈ ℝ^(B×D_out)\")\n",
        "    print(\"  • return_sequences=True:  Y ∈ ℝ^(B×T×D_out)\")\n",
        "    print(\"Where:\")\n",
        "    print(\"  • D_out = units (RNN hidden size)\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run Exercise 2\n",
        "exercise_2_results = analyze_rnn_dimensions()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise-3"
      },
      "source": [
        "### Exercise 3: Deep Sequence-to-Sequence RNN Configuration\n",
        "\n",
        "**Question**: If you want to build a deep sequence-to-sequence RNN, which RNN layers should have `return_sequences=True`? What about a sequence-to-vector RNN?\n",
        "\n",
        "#### Theoretical Analysis\n",
        "\n",
        "The `return_sequences` parameter controls whether an RNN layer outputs:\n",
        "- **`False`**: Only the final hidden state (2D tensor)\n",
        "- **`True`**: Hidden states for all time steps (3D tensor)\n",
        "\n",
        "**Deep Sequence-to-Sequence RNN:**\n",
        "- All layers except the last should have `return_sequences=True`\n",
        "- The last layer's `return_sequences` depends on desired output format\n",
        "\n",
        "**Sequence-to-Vector RNN:**\n",
        "- All layers should have `return_sequences=False` except intermediate layers\n",
        "- Only the final layer needs the aggregated representation\n",
        "\n",
        "**Mathematical Justification:**\n",
        "For layer $l$ to receive input from layer $l-1$:\n",
        "$$\\mathbf{h}^{(l)}_{t} = f^{(l)}(\\mathbf{h}^{(l-1)}_{t}, \\mathbf{h}^{(l)}_{t-1})$$\n",
        "\n",
        "This requires $\\mathbf{h}^{(l-1)}_{t}$ for all $t$, hence `return_sequences=True` for layer $l-1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise-3-implementation"
      },
      "source": [
        "# Exercise 3: Deep RNN Configuration\n",
        "\n",
        "def demonstrate_deep_rnn_configurations():\n",
        "    \"\"\"\n",
        "    Demonstrates proper configuration of deep RNNs for different tasks.\n",
        "    \"\"\"\n",
        "    print(\"EXERCISE 3: DEEP RNN CONFIGURATIONS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create sample data\n",
        "    batch_size = 32\n",
        "    input_time_steps = 20\n",
        "    output_time_steps = 15\n",
        "    input_features = 10\n",
        "\n",
        "    X_seq2seq = np.random.randn(batch_size, input_time_steps, input_features)\n",
        "    X_seq2vec = np.random.randn(batch_size, input_time_steps, input_features)\n",
        "\n",
        "    # Target data\n",
        "    y_seq2seq = np.random.randn(batch_size, output_time_steps, 1)  # Sequence output\n",
        "    y_seq2vec = np.random.randn(batch_size, 5)  # Vector output\n",
        "\n",
        "    print(f\"Sample data shapes:\")\n",
        "    print(f\"  Sequence-to-Sequence: X={X_seq2seq.shape}, y={y_seq2seq.shape}\")\n",
        "    print(f\"  Sequence-to-Vector: X={X_seq2vec.shape}, y={y_seq2vec.shape}\")\n",
        "\n",
        "    # Define different architectures\n",
        "    architectures = {\n",
        "        'Deep Seq2Seq (Correct)': {\n",
        "            'type': 'seq2seq',\n",
        "            'layers': [\n",
        "                {'units': 64, 'return_sequences': True, 'layer_num': 1},\n",
        "                {'units': 64, 'return_sequences': True, 'layer_num': 2},\n",
        "                {'units': 32, 'return_sequences': True, 'layer_num': 3},\n",
        "                {'type': 'TimeDistributed', 'units': 1}\n",
        "            ],\n",
        "            'description': 'All RNN layers return sequences, TimeDistributed for output'\n",
        "        },\n",
        "\n",
        "        'Deep Seq2Seq (Incorrect)': {\n",
        "            'type': 'seq2seq_wrong',\n",
        "            'layers': [\n",
        "                {'units': 64, 'return_sequences': False, 'layer_num': 1},  # Wrong!\n",
        "                {'units': 64, 'return_sequences': True, 'layer_num': 2},\n",
        "                {'units': 32, 'return_sequences': True, 'layer_num': 3}\n",
        "            ],\n",
        "            'description': 'WRONG: First layer does not return sequences'\n",
        "        },\n",
        "\n",
        "        'Deep Seq2Vec (Correct)': {\n",
        "            'type': 'seq2vec',\n",
        "            'layers': [\n",
        "                {'units': 64, 'return_sequences': True, 'layer_num': 1},\n",
        "                {'units': 64, 'return_sequences': True, 'layer_num': 2},\n",
        "                {'units': 32, 'return_sequences': False, 'layer_num': 3},  # Last layer only\n",
        "                {'type': 'Dense', 'units': 5}\n",
        "            ],\n",
        "            'description': 'Only last RNN layer returns final state, Dense for output'\n",
        "        },\n",
        "\n",
        "        'Deep Seq2Vec (Alternative)': {\n",
        "            'type': 'seq2vec_alt',\n",
        "            'layers': [\n",
        "                {'units': 64, 'return_sequences': True, 'layer_num': 1},\n",
        "                {'units': 64, 'return_sequences': True, 'layer_num': 2},\n",
        "                {'units': 32, 'return_sequences': True, 'layer_num': 3},\n",
        "                {'type': 'GlobalMaxPooling1D'},\n",
        "                {'type': 'Dense', 'units': 5}\n",
        "            ],\n",
        "            'description': 'All layers return sequences, global pooling for aggregation'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Build and test models\n",
        "    model_results = {}\n",
        "\n",
        "    for arch_name, arch_config in architectures.items():\n",
        "        print(f\"\\n{arch_name}:\")\n",
        "        print(f\"Description: {arch_config['description']}\")\n",
        "\n",
        "        try:\n",
        "            # Build model\n",
        "            model = keras.Sequential(name=arch_name.replace(' ', '_'))\n",
        "\n",
        "            for i, layer_config in enumerate(arch_config['layers']):\n",
        "                if layer_config.get('type') == 'TimeDistributed':\n",
        "                    model.add(keras.layers.TimeDistributed(\n",
        "                        keras.layers.Dense(layer_config['units'])\n",
        "                    ))\n",
        "                elif layer_config.get('type') == 'Dense':\n",
        "                    model.add(keras.layers.Dense(layer_config['units']))\n",
        "                elif layer_config.get('type') == 'GlobalMaxPooling1D':\n",
        "                    model.add(keras.layers.GlobalMaxPooling1D())\n",
        "                else:\n",
        "                    # RNN layer\n",
        "                    if i == 0:  # First layer needs input_shape\n",
        "                        model.add(keras.layers.LSTM(\n",
        "                            layer_config['units'],\n",
        "                            return_sequences=layer_config['return_sequences'],\n",
        "                            input_shape=(None, input_features)\n",
        "                        ))\n",
        "                    else:\n",
        "                        model.add(keras.layers.LSTM(\n",
        "                            layer_config['units'],\n",
        "                            return_sequences=layer_config['return_sequences']\n",
        "                        ))\n",
        "\n",
        "            # Test with appropriate data\n",
        "            if 'seq2seq' in arch_config['type']:\n",
        "                if arch_config['type'] != 'seq2seq_wrong':\n",
        "                    test_output = model(X_seq2seq)\n",
        "                    print(f\"  ✓ Model built successfully\")\n",
        "                    print(f\"  Input shape: {X_seq2seq.shape}\")\n",
        "                    print(f\"  Output shape: {test_output.shape}\")\n",
        "                    print(f\"  Expected output shape: {y_seq2seq.shape}\")\n",
        "\n",
        "                    model_results[arch_name] = {\n",
        "                        'success': True,\n",
        "                        'input_shape': X_seq2seq.shape,\n",
        "                        'output_shape': test_output.shape,\n",
        "                        'parameters': model.count_params()\n",
        "                    }\n",
        "                else:\n",
        "                    # This should fail\n",
        "                    print(f\"  ✗ This configuration will cause errors!\")\n",
        "                    print(f\"  Reason: Second layer expects 3D input but gets 2D\")\n",
        "                    model_results[arch_name] = {\n",
        "                        'success': False,\n",
        "                        'error': 'Dimension mismatch'\n",
        "                    }\n",
        "\n",
        "            else:  # seq2vec\n",
        "                test_output = model(X_seq2vec)\n",
        "                print(f\"  ✓ Model built successfully\")\n",
        "                print(f\"  Input shape: {X_seq2vec.shape}\")\n",
        "                print(f\"  Output shape: {test_output.shape}\")\n",
        "                print(f\"  Expected output shape: {y_seq2vec.shape}\")\n",
        "\n",
        "                model_results[arch_name] = {\n",
        "                    'success': True,\n",
        "                    'input_shape': X_seq2vec.shape,\n",
        "                    'output_shape': test_output.shape,\n",
        "                    'parameters': model.count_params()\n",
        "                }\n",
        "\n",
        "            # Print layer-by-layer analysis\n",
        "            print(f\"  Layer-by-layer analysis:\")\n",
        "            for j, layer in enumerate(model.layers):\n",
        "                if hasattr(layer, 'return_sequences'):\n",
        "                    print(f\"    Layer {j+1} ({layer.__class__.__name__}): \"\n",
        "                          f\"units={layer.units}, return_sequences={layer.return_sequences}\")\n",
        "                else:\n",
        "                    print(f\"    Layer {j+1} ({layer.__class__.__name__})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error: {str(e)}\")\n",
        "            model_results[arch_name] = {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # 1. Correct Seq2Seq Architecture\n",
        "    ax = axes[0, 0]\n",
        "    ax.set_title('Correct Deep Seq2Seq Architecture', fontweight='bold')\n",
        "\n",
        "    # Draw the architecture\n",
        "    layer_positions = [0.2, 0.4, 0.6, 0.8]\n",
        "    layer_names = ['LSTM\\n(return_seq=True)', 'LSTM\\n(return_seq=True)',\n",
        "                   'LSTM\\n(return_seq=True)', 'TimeDistributed\\nDense']\n",
        "    colors = ['lightblue', 'lightblue', 'lightblue', 'lightgreen']\n",
        "\n",
        "    for i, (pos, name, color) in enumerate(zip(layer_positions, layer_names, colors)):\n",
        "        # Layer box\n",
        "        rect = plt.Rectangle((0.1, pos-0.05), 0.8, 0.1,\n",
        "                           facecolor=color, edgecolor='black', alpha=0.7)\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(0.5, pos, name, ha='center', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "        # Arrow to next layer\n",
        "        if i < len(layer_positions) - 1:\n",
        "            ax.arrow(0.5, pos + 0.05, 0, 0.08, head_width=0.02, head_length=0.01,\n",
        "                    fc='black', ec='black')\n",
        "            # Show tensor shape\n",
        "            ax.text(0.92, pos + 0.09, '[B,T,D]', ha='left', va='center', fontsize=8,\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='yellow', alpha=0.7))\n",
        "\n",
        "    # Input and output labels\n",
        "    ax.text(0.5, 0.05, 'Input: [B, T_in, D_in]', ha='center', va='center',\n",
        "           fontsize=10, fontweight='bold')\n",
        "    ax.text(0.5, 0.95, 'Output: [B, T_out, D_out]', ha='center', va='center',\n",
        "           fontsize=10, fontweight='bold')\n",
        "\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # 2. Incorrect Seq2Seq Architecture\n",
        "    ax = axes[0, 1]\n",
        "    ax.set_title('INCORRECT Deep Seq2Seq Architecture', fontweight='bold', color='red')\n",
        "\n",
        "    layer_names_wrong = ['LSTM\\n(return_seq=FALSE)', 'LSTM\\n(return_seq=True)',\n",
        "                         'LSTM\\n(return_seq=True)', 'Dense']\n",
        "    colors_wrong = ['lightcoral', 'lightblue', 'lightblue', 'lightgreen']\n",
        "\n",
        "    for i, (pos, name, color) in enumerate(zip(layer_positions, layer_names_wrong, colors_wrong)):\n",
        "        rect = plt.Rectangle((0.1, pos-0.05), 0.8, 0.1,\n",
        "                           facecolor=color, edgecolor='black', alpha=0.7)\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(0.5, pos, name, ha='center', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "        if i < len(layer_positions) - 1:\n",
        "            if i == 0:  # First arrow (problematic)\n",
        "                ax.arrow(0.5, pos + 0.05, 0, 0.08, head_width=0.02, head_length=0.01,\n",
        "                        fc='red', ec='red', linewidth=3)\n",
        "                ax.text(0.92, pos + 0.09, '[B,D]', ha='left', va='center', fontsize=8,\n",
        "                       bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='red', alpha=0.7))\n",
        "                ax.text(0.5, pos + 0.09, '✗ ERROR!', ha='center', va='center',\n",
        "                       fontsize=8, color='red', fontweight='bold')\n",
        "            else:\n",
        "                ax.arrow(0.5, pos + 0.05, 0, 0.08, head_width=0.02, head_length=0.01,\n",
        "                        fc='black', ec='black')\n",
        "\n",
        "    ax.text(0.5, 0.05, 'Input: [B, T_in, D_in]', ha='center', va='center',\n",
        "           fontsize=10, fontweight='bold')\n",
        "    ax.text(0.5, 0.95, 'Cannot produce sequence output!', ha='center', va='center',\n",
        "           fontsize=10, fontweight='bold', color='red')\n",
        "\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # 3. Correct Seq2Vec Architecture\n",
        "    ax = axes[1, 0]\n",
        "    ax.set_title('Correct Deep Seq2Vec Architecture', fontweight='bold')\n",
        "\n",
        "    layer_names_vec = ['LSTM\\n(return_seq=True)', 'LSTM\\n(return_seq=True)',\n",
        "                       'LSTM\\n(return_seq=FALSE)', 'Dense']\n",
        "    colors_vec = ['lightblue', 'lightblue', 'orange', 'lightgreen']\n",
        "\n",
        "    for i, (pos, name, color) in enumerate(zip(layer_positions, layer_names_vec, colors_vec)):\n",
        "        rect = plt.Rectangle((0.1, pos-0.05), 0.8, 0.1,\n",
        "                           facecolor=color, edgecolor='black', alpha=0.7)\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(0.5, pos, name, ha='center', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "        if i < len(layer_positions) - 1:\n",
        "            ax.arrow(0.5, pos + 0.05, 0, 0.08, head_width=0.02, head_length=0.01,\n",
        "                    fc='black', ec='black')\n",
        "            if i < 2:\n",
        "                ax.text(0.92, pos + 0.09, '[B,T,D]', ha='left', va='center', fontsize=8,\n",
        "                       bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='yellow', alpha=0.7))\n",
        "            else:\n",
        "                ax.text(0.92, pos + 0.09, '[B,D]', ha='left', va='center', fontsize=8,\n",
        "                       bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='orange', alpha=0.7))\n",
        "\n",
        "    ax.text(0.5, 0.05, 'Input: [B, T, D_in]', ha='center', va='center',\n",
        "           fontsize=10, fontweight='bold')\n",
        "    ax.text(0.5, 0.95, 'Output: [B, D_out]', ha='center', va='center',\n",
        "           fontsize=10, fontweight='bold')\n",
        "\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # 4. Summary table\n",
        "    ax = axes[1, 1]\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Create summary table\n",
        "    summary_data = [\n",
        "        ['Architecture', 'Hidden Layers', 'Output Layer', 'Use Case'],\n",
        "        ['Deep Seq2Seq', 'return_sequences=True', 'TimeDistributed', 'Translation, Forecasting'],\n",
        "        ['Deep Seq2Vec', 'return_sequences=True\\n(except last RNN)', 'Dense', 'Classification, Regression'],\n",
        "        ['Encoder-Decoder', 'Encoder: True\\nDecoder: varies', 'Decoder dependent', 'Complex transformations']\n",
        "    ]\n",
        "\n",
        "    table = ax.table(\n",
        "        cellText=summary_data[1:],\n",
        "        colLabels=summary_data[0],\n",
        "        cellLoc='center',\n",
        "        loc='center'\n",
        "    )\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(9)\n",
        "    table.scale(1.2, 2.5)\n",
        "\n",
        "    # Style the header\n",
        "    for i in range(len(summary_data[0])):\n",
        "        table[(0, i)].set_facecolor('#40466e')\n",
        "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "    ax.set_title('Configuration Summary', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print rules\n",
        "    print(\"\\nCONFIGURATION RULES:\")\n",
        "    print(\"=\" * 30)\n",
        "    print(\"\\nDEEP SEQUENCE-TO-SEQUENCE RNN:\")\n",
        "    print(\"• ALL hidden RNN layers: return_sequences=True\")\n",
        "    print(\"• Output layer: TimeDistributed(Dense) or return_sequences=True\")\n",
        "    print(\"• Reason: Each layer needs full sequence from previous layer\")\n",
        "\n",
        "    print(\"\\nDEEP SEQUENCE-TO-VECTOR RNN:\")\n",
        "    print(\"• Hidden RNN layers: return_sequences=True\")\n",
        "    print(\"• LAST RNN layer: return_sequences=False\")\n",
        "    print(\"• Output layer: Dense\")\n",
        "    print(\"• Alternative: All layers True + GlobalPooling\")\n",
        "\n",
        "    print(\"\\nCOMMON MISTAKES:\")\n",
        "    print(\"• Setting return_sequences=False in hidden layers\")\n",
        "    print(\"• Forgetting TimeDistributed for sequence outputs\")\n",
        "    print(\"• Mismatching input/output dimensions\")\n",
        "\n",
        "    return model_results\n",
        "\n",
        "# Run Exercise 3\n",
        "exercise_3_results = demonstrate_deep_rnn_configurations()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise-4"
      },
      "source": [
        "### Exercise 4: Time Series Forecasting Architecture\n",
        "\n",
        "**Question**: Suppose you have a daily univariate time series, and you want to forecast the next seven days. Which RNN architecture should you use?\n",
        "\n",
        "#### Theoretical Analysis\n",
        "\n",
        "For forecasting the next 7 days from daily univariate time series, we have several architectural choices:\n",
        "\n",
        "**Option 1: Sequence-to-Vector + Dense Output**\n",
        "- Input: Historical sequence `[batch, time_steps, 1]`\n",
        "- RNN: Process sequence, output final state\n",
        "- Dense: Map to 7-dimensional output\n",
        "- Mathematical form: $f: \\mathbb{R}^{T \\times 1} \\rightarrow \\mathbb{R}^7$\n",
        "\n",
        "**Option 2: Sequence-to-Sequence**\n",
        "- Input: Historical sequence\n",
        "- Output: Sequence of 7 future values\n",
        "- Each time step predicts multiple future steps\n",
        "\n",
        "**Option 3: Autoregressive (Iterative)**\n",
        "- Predict one step ahead, add to input, repeat\n",
        "- More prone to error accumulation\n",
        "\n",
        "**Option 4: Encoder-Decoder**\n",
        "- Encoder: Compress historical data\n",
        "- Decoder: Generate 7-day forecast\n",
        "\n",
        "**Recommended**: Sequence-to-Vector with Dense output for its simplicity and effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise-4-implementation"
      },
      "source": [
        "# Exercise 4: Time Series Forecasting Architecture\n",
        "\n",
        "def design_forecasting_architectures():\n",
        "    \"\"\"\n",
        "    Designs and compares different architectures for 7-day forecasting.\n",
        "    \"\"\"\n",
        "    print(\"EXERCISE 4: TIME SERIES FORECASTING ARCHITECTURES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Generate realistic daily time series data\n",
        "    np.random.seed(42)\n",
        "\n",
        "    def generate_realistic_daily_series(n_series=1000, n_days=100):\n",
        "        \"\"\"\n",
        "        Generates realistic daily time series with trend, seasonality, and noise.\n",
        "        \"\"\"\n",
        "        series_list = []\n",
        "\n",
        "        for i in range(n_series):\n",
        "            # Time axis\n",
        "            t = np.arange(n_days)\n",
        "\n",
        "            # Trend component\n",
        "            trend = np.random.uniform(-0.01, 0.01) * t\n",
        "\n",
        "            # Weekly seasonality (7-day cycle)\n",
        "            weekly_pattern = np.random.uniform(0.5, 2.0) * np.sin(2 * np.pi * t / 7 + np.random.uniform(0, 2*np.pi))\n",
        "\n",
        "            # Monthly seasonality (30-day cycle)\n",
        "            monthly_pattern = np.random.uniform(0.2, 1.0) * np.sin(2 * np.pi * t / 30 + np.random.uniform(0, 2*np.pi))\n",
        "\n",
        "            # Random walk component\n",
        "            random_walk = np.cumsum(np.random.normal(0, 0.1, n_days))\n",
        "\n",
        "            # Noise\n",
        "            noise = np.random.normal(0, 0.2, n_days)\n",
        "\n",
        "            # Combine components\n",
        "            series = trend + weekly_pattern + monthly_pattern + random_walk + noise\n",
        "\n",
        "            # Add a base level\n",
        "            series += np.random.uniform(10, 50)\n",
        "\n",
        "            series_list.append(series)\n",
        "\n",
        "        return np.array(series_list)\n",
        "\n",
        "    # Generate data\n",
        "    all_series = generate_realistic_daily_series(1000, 100)\n",
        "\n",
        "    # Prepare data for 7-day forecasting\n",
        "    lookback_days = 30  # Use 30 days to predict next 7\n",
        "    forecast_days = 7\n",
        "\n",
        "    X, y = [], []\n",
        "    for series in all_series:\n",
        "        for i in range(len(series) - lookback_days - forecast_days + 1):\n",
        "            X.append(series[i:i + lookback_days])\n",
        "            y.append(series[i + lookback_days:i + lookback_days + forecast_days])\n",
        "\n",
        "    X = np.array(X).reshape(-1, lookback_days, 1)\n",
        "    y = np.array(y)\n",
        "\n",
        "    print(f\"Dataset prepared:\")\n",
        "    print(f\"  Input shape: {X.shape} (samples, lookback_days, features)\")\n",
        "    print(f\"  Output shape: {y.shape} (samples, forecast_days)\")\n",
        "\n",
        "    # Split data\n",
        "    train_size = int(0.7 * len(X))\n",
        "    val_size = int(0.2 * len(X))\n",
        "\n",
        "    X_train, y_train = X[:train_size], y[:train_size]\n",
        "    X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
        "    X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
        "\n",
        "    # Define different architectures\n",
        "    architectures = {\n",
        "        'Sequence-to-Vector + Dense (Recommended)': {\n",
        "            'model': keras.Sequential([\n",
        "                keras.layers.LSTM(64, return_sequences=True, input_shape=(lookback_days, 1)),\n",
        "                keras.layers.LSTM(32, return_sequences=False),\n",
        "                keras.layers.Dense(64, activation='relu'),\n",
        "                keras.layers.Dropout(0.2),\n",
        "                keras.layers.Dense(forecast_days)  # Direct 7-day output\n",
        "            ]),\n",
        "            'description': 'Process sequence, output vector of 7 forecasts',\n",
        "            'pros': ['Simple', 'Direct optimization', 'No error accumulation'],\n",
        "            'cons': ['Assumes independence of forecast days']\n",
        "        },\n",
        "\n",
        "        'Sequence-to-Sequence': {\n",
        "            'model': keras.Sequential([\n",
        "                keras.layers.LSTM(64, return_sequences=True, input_shape=(lookback_days, 1)),\n",
        "                keras.layers.LSTM(32, return_sequences=True),\n",
        "                keras.layers.TimeDistributed(keras.layers.Dense(32, activation='relu')),\n",
        "                keras.layers.TimeDistributed(keras.layers.Dense(1))\n",
        "            ]),\n",
        "            'description': 'Each time step predicts corresponding future day',\n",
        "            'pros': ['Models sequential dependencies', 'Rich training signal'],\n",
        "            'cons': ['More complex', 'Requires reshaping']\n",
        "        },\n",
        "\n",
        "        'Encoder-Decoder': {\n",
        "            'model': None,  # Will build custom model\n",
        "            'description': 'Encode past, decode future',\n",
        "            'pros': ['Handles variable lengths', 'Attention can be added'],\n",
        "            'cons': ['Most complex', 'More parameters']\n",
        "        },\n",
        "\n",
        "        'CNN-LSTM Hybrid': {\n",
        "            'model': keras.Sequential([\n",
        "                keras.layers.Conv1D(32, 3, activation='relu', input_shape=(lookback_days, 1)),\n",
        "                keras.layers.Conv1D(32, 3, activation='relu'),\n",
        "                keras.layers.LSTM(64, return_sequences=False),\n",
        "                keras.layers.Dense(32, activation='relu'),\n",
        "                keras.layers.Dense(forecast_days)\n",
        "            ]),\n",
        "            'description': 'CNN for local patterns, LSTM for long-term dependencies',\n",
        "            'pros': ['Captures both local and long-term patterns', 'Efficient'],\n",
        "            'cons': ['More hyperparameters to tune']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Build Encoder-Decoder model\n",
        "    # Encoder\n",
        "    encoder_inputs = keras.layers.Input(shape=(lookback_days, 1))\n",
        "    encoder_lstm = keras.layers.LSTM(64, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = keras.layers.Input(shape=(forecast_days, 1))\n",
        "    decoder_lstm = keras.layers.LSTM(64, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_dense = keras.layers.TimeDistributed(keras.layers.Dense(1))\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    encoder_decoder_model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    architectures['Encoder-Decoder']['model'] = encoder_decoder_model\n",
        "\n",
        "    # Train and evaluate models\n",
        "    results = {}\n",
        "\n",
        "    for arch_name, arch_info in architectures.items():\n",
        "        print(f\"\\nTraining {arch_name}...\")\n",
        "        model = arch_info['model']\n",
        "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "        print(f\"Parameters: {model.count_params():,}\")\n",
        "\n",
        "        # Prepare training data based on architecture\n",
        "        if arch_name == 'Sequence-to-Sequence':\n",
        "            # Reshape y for sequence output\n",
        "            y_train_arch = y_train.reshape(-1, forecast_days, 1)\n",
        "            y_val_arch = y_val.reshape(-1, forecast_days, 1)\n",
        "            y_test_arch = y_test.reshape(-1, forecast_days, 1)\n",
        "        elif arch_name == 'Encoder-Decoder':\n",
        "            # Create decoder inputs (shifted target)\n",
        "            decoder_input_train = np.zeros((len(y_train), forecast_days, 1))\n",
        "            decoder_input_val = np.zeros((len(y_val), forecast_days, 1))\n",
        "            decoder_input_test = np.zeros((len(y_test), forecast_days, 1))\n",
        "\n",
        "            y_train_arch = y_train.reshape(-1, forecast_days, 1)\n",
        "            y_val_arch = y_val.reshape(-1, forecast_days, 1)\n",
        "            y_test_arch = y_test.reshape(-1, forecast_days, 1)\n",
        "\n",
        "            X_train_arch = [X_train, decoder_input_train]\n",
        "            X_val_arch = [X_val, decoder_input_val]\n",
        "            X_test_arch = [X_test, decoder_input_test]\n",
        "        else:\n",
        "            y_train_arch = y_train\n",
        "            y_val_arch = y_val\n",
        "            y_test_arch = y_test\n",
        "            X_train_arch = X_train\n",
        "            X_val_arch = X_val\n",
        "            X_test_arch = X_test\n",
        "\n",
        "        # Train model\n",
        "        import time\n",
        "        start_time = time.time()\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train_arch, y_train_arch,\n",
        "            epochs=15,\n",
        "            batch_size=32,\n",
        "            validation_data=(X_val_arch, y_val_arch),\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Evaluate\n",
        "        test_loss = model.evaluate(X_test_arch, y_test_arch, verbose=0)[0]\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = model.predict(X_test_arch, verbose=0)\n",
        "        if len(predictions.shape) == 3:  # Sequence output\n",
        "            predictions = predictions.reshape(-1, forecast_days)\n",
        "\n",
        "        # Calculate metrics for each forecast day\n",
        "        day_mses = []\n",
        "        for day in range(forecast_days):\n",
        "            day_mse = np.mean((y_test[:, day] - predictions[:, day]) ** 2)\n",
        "            day_mses.append(day_mse)\n",
        "\n",
        "        results[arch_name] = {\n",
        "            'test_mse': test_loss,\n",
        "            'day_mses': day_mses,\n",
        "            'parameters': model.count_params(),\n",
        "            'training_time': training_time,\n",
        "            'predictions': predictions[:100],  # Save first 100 predictions\n",
        "            'history': history,\n",
        "            'pros': arch_info['pros'],\n",
        "            'cons': arch_info['cons']\n",
        "        }\n",
        "\n",
        "        print(f\"Test MSE: {test_loss:.6f}, Training time: {training_time:.1f}s\")\n",
        "        print(f\"MSE by day: {[f'{mse:.4f}' for mse in day_mses]}\")\n",
        "\n",
        "    # Comprehensive visualization\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(16, 15))\n",
        "\n",
        "    # 1. Sample time series\n",
        "    ax = axes[0, 0]\n",
        "    sample_series = all_series[:3]\n",
        "        # Sample time series visualization\n",
        "        for i, series in enumerate(sample_series):\n",
        "            ax.plot(series, alpha=0.7, linewidth=2, label=f'Series {i+1}')\n",
        "\n",
        "        ax.set_title('Sample Daily Time Series')\n",
        "        ax.set_xlabel('Day')\n",
        "        ax.set_ylabel('Value')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add annotations for patterns\n",
        "        ax.annotate('Weekly Pattern', xy=(7, sample_series[0][7]), xytext=(15, sample_series[0][7]+5),\n",
        "                   arrowprops=dict(arrowstyle='->', color='red', alpha=0.7),\n",
        "                   fontsize=10, color='red')\n",
        "\n",
        "        # 2. Architecture comparison\n",
        "        ax = axes[0, 1]\n",
        "        arch_names = list(results.keys())\n",
        "        test_mses = [results[name]['test_mse'] for name in arch_names]\n",
        "\n",
        "        bars = ax.bar(range(len(arch_names)), test_mses, alpha=0.7,\n",
        "                     color=['skyblue', 'lightgreen', 'orange', 'lightcoral'])\n",
        "        ax.set_title('7-Day Forecasting Performance')\n",
        "        ax.set_xlabel('Architecture')\n",
        "        ax.set_ylabel('Test MSE')\n",
        "        ax.set_xticks(range(len(arch_names)))\n",
        "        ax.set_xticklabels(arch_names, rotation=45, ha='right')\n",
        "\n",
        "        # Add value labels\n",
        "        for bar, value in zip(bars, test_mses):\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0001,\n",
        "                    f'{value:.4f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "        # 3. Forecast examples\n",
        "        ax = axes[1, 0]\n",
        "        best_arch = min(arch_names, key=lambda x: results[x]['test_mse'])\n",
        "        best_model = architectures[best_arch]['model']\n",
        "\n",
        "        # Show forecast examples\n",
        "        for i in range(3):\n",
        "            if best_arch == 'Encoder-Decoder':\n",
        "                sample_pred = best_model.predict(X_test_arch[i:i+1], verbose=0)\n",
        "                if len(sample_pred.shape) == 3:\n",
        "                    sample_pred = sample_pred.reshape(-1, forecast_days)[0]\n",
        "                else:\n",
        "                    sample_pred = sample_pred[0]\n",
        "            else:\n",
        "                sample_pred = best_model.predict(X_test[i:i+1], verbose=0)[0]\n",
        "\n",
        "            input_seq = X_test[i, :, 0]\n",
        "            true_forecast = y_test[i]\n",
        "\n",
        "            # Plot input sequence\n",
        "            days = range(len(input_seq))\n",
        "            ax.plot(days, input_seq, 'b-', linewidth=2, alpha=0.7)\n",
        "\n",
        "            # Plot forecasts\n",
        "            forecast_days_range = range(len(input_seq), len(input_seq) + forecast_days)\n",
        "            ax.plot(forecast_days_range, true_forecast, 'r-', linewidth=3,\n",
        "                   label='True Future' if i == 0 else '', alpha=0.8)\n",
        "            ax.plot(forecast_days_range, sample_pred, 'g--', linewidth=2,\n",
        "                   label='Forecast' if i == 0 else '', alpha=0.8)\n",
        "\n",
        "            # Add vertical line separating past and future\n",
        "            ax.axvline(x=len(input_seq)-0.5, color='gray', linestyle=':', alpha=0.7)\n",
        "\n",
        "        ax.set_title(f'7-Day Forecasts: {best_arch}')\n",
        "        ax.set_xlabel('Day')\n",
        "        ax.set_ylabel('Value')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # 4. Error by forecast day\n",
        "        ax = axes[1, 1]\n",
        "\n",
        "        # Calculate MSE for each forecast day\n",
        "        day_errors = {name: [] for name in arch_names}\n",
        "\n",
        "        for name in arch_names:\n",
        "            model = architectures[name]['model']\n",
        "            if name == 'Encoder-Decoder':\n",
        "                preds = model.predict(X_test_arch, verbose=0)\n",
        "                if len(preds.shape) == 3:\n",
        "                    preds = preds.reshape(-1, forecast_days)\n",
        "            else:\n",
        "                preds = model.predict(X_test, verbose=0)\n",
        "                if len(preds.shape) == 3:\n",
        "                    preds = preds.reshape(-1, forecast_days)\n",
        "\n",
        "            for day in range(forecast_days):\n",
        "                day_mse = np.mean((y_test[:, day] - preds[:, day]) ** 2)\n",
        "                day_errors[name].append(day_mse)\n",
        "\n",
        "        # Plot error by day\n",
        "        for name, errors in day_errors.items():\n",
        "            ax.plot(range(1, forecast_days + 1), errors, 'o-',\n",
        "                   linewidth=2, label=name, alpha=0.8)\n",
        "\n",
        "        ax.set_title('Forecast Error by Day')\n",
        "        ax.set_xlabel('Forecast Day')\n",
        "        ax.set_ylabel('MSE')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print detailed recommendations\n",
        "        print(\"\\nRECOMMENDATION ANALYSIS:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Rank architectures\n",
        "        ranked_archs = sorted(arch_names, key=lambda x: results[x]['test_mse'])\n",
        "\n",
        "        print(f\"\\nRanking by Performance:\")\n",
        "        for i, name in enumerate(ranked_archs, 1):\n",
        "            print(f\"{i}. {name}: MSE = {results[name]['test_mse']:.6f}\")\n",
        "            print(f\"   Pros: {', '.join(results[name]['pros'])}\")\n",
        "            print(f\"   Cons: {', '.join(results[name]['cons'])}\")\n",
        "            print(f\"   Parameters: {results[name]['parameters']:,}\")\n",
        "            print(f\"   Training time: {results[name]['training_time']:.1f}s\\n\")\n",
        "\n",
        "        print(\"FINAL RECOMMENDATION:\")\n",
        "        print(f\"For 7-day univariate time series forecasting, use: {ranked_archs[0]}\")\n",
        "        print(f\"Reasoning:\")\n",
        "        print(f\"• Best performance: {results[ranked_archs[0]]['test_mse']:.6f} MSE\")\n",
        "        print(f\"• {', '.join(results[ranked_archs[0]]['pros'])}\")\n",
        "        print(f\"• Manageable complexity: {results[ranked_archs[0]]['parameters']:,} parameters\")\n",
        "\n",
        "        return results\n",
        "\n",
        "# Run Exercise 4\n",
        "exercise_4_results = design_forecasting_architectures()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise-5"
      },
      "source": [
        "### Exercise 5: Main Difficulties Training RNNs\n",
        "\n",
        "**Question**: What are the main difficulties when training RNNs? How can you handle them?\n",
        "\n",
        "#### Theoretical Analysis\n",
        "\n",
        "Training RNNs presents several fundamental challenges:\n",
        "\n",
        "**1. Vanishing Gradient Problem**\n",
        "- **Mathematical Cause**: Gradients are computed via chain rule across time steps\n",
        "- **Formula**: $\\frac{\\partial L}{\\partial W} = \\sum_{t=1}^T \\sum_{k=1}^t \\frac{\\partial L^{(t)}}{\\partial h^{(t)}} \\prod_{i=k+1}^t \\frac{\\partial h^{(i)}}{\\partial h^{(i-1)}} \\frac{\\partial h^{(k)}}{\\partial W}$\n",
        "- **Problem**: Product term $\\prod_{i=k+1}^t \\frac{\\partial h^{(i)}}{\\partial h^{(i-1)}}$ vanishes exponentially\n",
        "\n",
        "**2. Exploding Gradient Problem**\n",
        "- **Cause**: Same chain rule, but gradients grow exponentially\n",
        "- **Effect**: Unstable training, parameter updates too large\n",
        "\n",
        "**3. Long-term Dependencies**\n",
        "- **Issue**: Information from early time steps gets lost\n",
        "- **Mathematical**: $h^{(t)} = f(W h^{(t-1)} + U x^{(t)})$ - early information diluted\n",
        "\n",
        "**4. Computational Complexity**\n",
        "- **Memory**: $O(T)$ for sequence length $T$\n",
        "- **Time**: Sequential processing prevents parallelization\n",
        "\n",
        "#### Solutions and Techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise-5-implementation"
      },
      "source": [
        "# Exercise 5: RNN Training Difficulties and Solutions\n",
        "\n",
        "def demonstrate_rnn_training_challenges():\n",
        "    \"\"\"\n",
        "    Comprehensive demonstration of RNN training challenges and solutions.\n",
        "    \"\"\"\n",
        "    print(\"EXERCISE 5: RNN TRAINING DIFFICULTIES AND SOLUTIONS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Define the main challenges and their solutions\n",
        "    challenges = {\n",
        "        '1. Vanishing Gradients': {\n",
        "            'description': 'Gradients become exponentially smaller through time',\n",
        "            'mathematical_cause': 'Product of Jacobians < 1 across time steps',\n",
        "            'formula': '∏(∂h^(i)/∂h^(i-1)) → 0 as sequence length increases',\n",
        "            'effects': [\n",
        "                'Early layers learn very slowly',\n",
        "                'Long-term dependencies not captured',\n",
        "                'Training stagnation'\n",
        "            ],\n",
        "            'solutions': [\n",
        "                'LSTM/GRU cells with gating mechanisms',\n",
        "                'Better weight initialization (Xavier/He)',\n",
        "                'Layer normalization',\n",
        "                'Residual connections',\n",
        "                'Gradient clipping (helps with exploding too)'\n",
        "            ]\n",
        "        },\n",
        "\n",
        "        '2. Exploding Gradients': {\n",
        "            'description': 'Gradients become exponentially larger through time',\n",
        "            'mathematical_cause': 'Product of Jacobians > 1 across time steps',\n",
        "            'formula': '∏(∂h^(i)/∂h^(i-1)) → ∞ causing numerical instability',\n",
        "            'effects': [\n",
        "                'Training becomes unstable',\n",
        "                'Parameters change too rapidly',\n",
        "                'Loss oscillates or diverges',\n",
        "                'NaN values in computations'\n",
        "            ],\n",
        "            'solutions': [\n",
        "                'Gradient clipping (norm or value clipping)',\n",
        "                'Lower learning rates',\n",
        "                'Better weight initialization',\n",
        "                'Layer normalization',\n",
        "                'LSTM/GRU architectures'\n",
        "            ]\n",
        "        },\n",
        "\n",
        "        '3. Long-term Dependencies': {\n",
        "            'description': 'Difficulty learning patterns across long sequences',\n",
        "            'mathematical_cause': 'Information decay through recurrent transformations',\n",
        "            'formula': 'h^(t) = f(Wh^(t-1) + Ux^(t)) - early info gets diluted',\n",
        "            'effects': [\n",
        "                'Cannot learn long-range patterns',\n",
        "                'Poor performance on tasks requiring long memory',\n",
        "                'Information bottleneck at hidden state'\n",
        "            ],\n",
        "            'solutions': [\n",
        "                'LSTM/GRU with memory cells',\n",
        "                'Attention mechanisms',\n",
        "                'Transformer architectures',\n",
        "                'Residual connections',\n",
        "                'Memory networks'\n",
        "            ]\n",
        "        },\n",
        "\n",
        "        '4. Computational Complexity': {\n",
        "            'description': 'High memory and time complexity for long sequences',\n",
        "            'mathematical_cause': 'Sequential nature prevents parallelization',\n",
        "            'formula': 'Time: O(T), Memory: O(T×H) for T steps, H hidden units',\n",
        "            'effects': [\n",
        "                'Slow training and inference',\n",
        "                'Memory bottlenecks for long sequences',\n",
        "                'Difficulty scaling to large datasets'\n",
        "            ],\n",
        "            'solutions': [\n",
        "                'Truncated backpropagation through time',\n",
        "                '1D CNNs for parallelizable processing',\n",
        "                'Attention mechanisms (Transformers)',\n",
        "                'Model parallelism',\n",
        "                'Mixed precision training'\n",
        "            ]\n",
        "        },\n",
        "\n",
        "        '5. Overfitting': {\n",
        "            'description': 'RNNs can easily overfit to training sequences',\n",
        "            'mathematical_cause': 'High model capacity and sequential dependencies',\n",
        "            'formula': 'Model learns specific patterns rather than generalizable features',\n",
        "            'effects': [\n",
        "                'Poor generalization to new sequences',\n",
        "                'High variance in predictions',\n",
        "                'Memorization instead of learning'\n",
        "            ],\n",
        "            'solutions': [\n",
        "                'Dropout (regular and recurrent)',\n",
        "                'Weight regularization (L1/L2)',\n",
        "                'Early stopping',\n",
        "                'Data augmentation',\n",
        "                'Batch normalization/Layer normalization'\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create practical demonstrations\n",
        "\n",
        "    # 1. Gradient flow analysis\n",
        "    def analyze_gradient_flow_detailed():\n",
        "        \"\"\"\n",
        "        Detailed analysis of how gradients flow through RNN layers.\n",
        "        \"\"\"\n",
        "        print(\"\\nGRADIENT FLOW ANALYSIS:\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Simulate different scenarios\n",
        "        sequence_lengths = [10, 50, 100, 200]\n",
        "        weight_scales = [0.5, 1.0, 1.5, 2.0]\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # Different weight scales\n",
        "        ax = axes[0, 0]\n",
        "        for weight_scale in weight_scales:\n",
        "            gradients = [1.0]  # Initial gradient\n",
        "            for step in range(50):\n",
        "                # Simplified gradient computation\n",
        "                activation_grad = 0.25  # tanh derivative\n",
        "                new_grad = gradients[-1] * weight_scale * activation_grad\n",
        "                gradients.append(new_grad)\n",
        "\n",
        "            ax.semilogy(gradients, label=f'Weight scale = {weight_scale}', linewidth=2)\n",
        "\n",
        "        ax.set_title('Gradient Magnitude vs Weight Scale')\n",
        "        ax.set_xlabel('Steps Back in Time')\n",
        "        ax.set_ylabel('Gradient Magnitude (log scale)')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Different activation functions\n",
        "        ax = axes[0, 1]\n",
        "        activations = {\n",
        "            'tanh': 0.25,\n",
        "            'sigmoid': 0.25,\n",
        "            'ReLU': 1.0,\n",
        "            'LeakyReLU': 0.95\n",
        "        }\n",
        "\n",
        "        weight_scale = 1.0\n",
        "        for act_name, act_grad in activations.items():\n",
        "            gradients = [1.0]\n",
        "            for step in range(50):\n",
        "                new_grad = gradients[-1] * weight_scale * act_grad\n",
        "                gradients.append(new_grad)\n",
        "\n",
        "            ax.semilogy(gradients, label=act_name, linewidth=2)\n",
        "\n",
        "        ax.set_title('Gradient Flow by Activation Function')\n",
        "        ax.set_xlabel('Steps Back in Time')\n",
        "        ax.set_ylabel('Gradient Magnitude (log scale)')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Sequence length effect\n",
        "        ax = axes[1, 0]\n",
        "        for seq_len in sequence_lengths:\n",
        "            gradients = [1.0]\n",
        "            weight_scale = 0.9  # Slightly vanishing\n",
        "            for step in range(seq_len):\n",
        "                new_grad = gradients[-1] * weight_scale * 0.25\n",
        "                gradients.append(new_grad)\n",
        "\n",
        "            ax.semilogy(range(len(gradients)), gradients,\n",
        "                       label=f'Seq length = {seq_len}', linewidth=2)\n",
        "\n",
        "        ax.set_title('Vanishing Gradients vs Sequence Length')\n",
        "        ax.set_xlabel('Steps Back in Time')\n",
        "        ax.set_ylabel('Gradient Magnitude (log scale)')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Solutions effectiveness\n",
        "        ax = axes[1, 1]\n",
        "        solutions = {\n",
        "            'Standard RNN': {'weight_scale': 0.9, 'clip': False, 'norm': False},\n",
        "            'Gradient Clipping': {'weight_scale': 1.5, 'clip': True, 'norm': False},\n",
        "            'Layer Norm': {'weight_scale': 1.2, 'clip': False, 'norm': True},\n",
        "            'Both Solutions': {'weight_scale': 1.2, 'clip': True, 'norm': True}\n",
        "        }\n",
        "\n",
        "        for sol_name, params in solutions.items():\n",
        "            gradients = [1.0]\n",
        "            for step in range(30):\n",
        "                new_grad = gradients[-1] * params['weight_scale'] * 0.25\n",
        "\n",
        "                # Apply gradient clipping\n",
        "                if params['clip'] and abs(new_grad) > 5.0:\n",
        "                    new_grad = 5.0 * np.sign(new_grad)\n",
        "\n",
        "                # Apply layer normalization (simplified)\n",
        "                if params['norm']:\n",
        "                    new_grad = new_grad * 0.8  # Stabilizing effect\n",
        "\n",
        "                gradients.append(new_grad)\n",
        "\n",
        "            ax.plot(gradients, label=sol_name, linewidth=2)\n",
        "\n",
        "        ax.set_title('Solution Effectiveness')\n",
        "        ax.set_xlabel('Steps Back in Time')\n",
        "        ax.set_ylabel('Gradient Magnitude')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # 2. Practical solution implementations\n",
        "    def implement_solutions():\n",
        "        \"\"\"\n",
        "        Implements and compares different solutions to RNN training problems.\n",
        "        \"\"\"\n",
        "        print(\"\\nPRACTICAL SOLUTION IMPLEMENTATIONS:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Generate challenging sequence data (long sequences with long-term dependencies)\n",
        "        def generate_long_term_dependency_data(n_samples=1000, seq_len=100):\n",
        "            \"\"\"\n",
        "            Generates data where the target depends on information from early in the sequence.\n",
        "            \"\"\"\n",
        "            X = np.random.randn(n_samples, seq_len, 1)\n",
        "            # Target depends on sum of first 10 values and pattern in middle\n",
        "            y = np.sum(X[:, :10, 0], axis=1) + np.mean(X[:, 40:50, 0], axis=1)\n",
        "            y = y.reshape(-1, 1)\n",
        "            return X.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "        X_long, y_long = generate_long_term_dependency_data(1000, 100)\n",
        "\n",
        "        # Split data\n",
        "        train_size = int(0.7 * len(X_long))\n",
        "        val_size = int(0.2 * len(X_long))\n",
        "\n",
        "        X_train_long = X_long[:train_size]\n",
        "        y_train_long = y_long[:train_size]\n",
        "        X_val_long = X_long[train_size:train_size + val_size]\n",
        "        y_val_long = y_long[train_size:train_size + val_size]\n",
        "        X_test_long = X_long[train_size + val_size:]\n",
        "        y_test_long = y_long[train_size + val_size:]\n",
        "\n",
        "        print(f\"Long-term dependency data: {X_long.shape}, {y_long.shape}\")\n",
        "\n",
        "        # Define models with different solutions\n",
        "        models = {}\n",
        "\n",
        "        # 1. Problematic RNN (baseline)\n",
        "        models['Problematic RNN'] = keras.Sequential([\n",
        "            keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 1]),\n",
        "            keras.layers.SimpleRNN(32),\n",
        "            keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "        # 2. With gradient clipping\n",
        "        models['RNN + Grad Clipping'] = keras.Sequential([\n",
        "            keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 1]),\n",
        "            keras.layers.SimpleRNN(32),\n",
        "            keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "        # 3. With dropout\n",
        "        models['RNN + Dropout'] = keras.Sequential([\n",
        "            keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 1],\n",
        "                                   dropout=0.2, recurrent_dropout=0.2),\n",
        "            keras.layers.SimpleRNN(32, dropout=0.2, recurrent_dropout=0.2),\n",
        "            keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "        # 4. LSTM solution\n",
        "        models['LSTM Solution'] = keras.Sequential([\n",
        "            keras.layers.LSTM(32, return_sequences=True, input_shape=[None, 1]),\n",
        "            keras.layers.LSTM(32),\n",
        "            keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "        # 5. GRU solution\n",
        "        models['GRU Solution'] = keras.Sequential([\n",
        "            keras.layers.GRU(32, return_sequences=True, input_shape=[None, 1]),\n",
        "            keras.layers.GRU(32),\n",
        "            keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "        # Compile with different optimizers\n",
        "        optimizers = {\n",
        "            'Problematic RNN': keras.optimizers.Adam(learning_rate=0.001),\n",
        "            'RNN + Grad Clipping': keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
        "            'RNN + Dropout': keras.optimizers.Adam(learning_rate=0.001),\n",
        "            'LSTM Solution': keras.optimizers.Adam(learning_rate=0.001),\n",
        "            'GRU Solution': keras.optimizers.Adam(learning_rate=0.001)\n",
        "        }\n",
        "\n",
        "        for name, model in models.items():\n",
        "            model.compile(optimizer=optimizers[name], loss='mse', metrics=['mae'])\n",
        "\n",
        "        # Train and evaluate\n",
        "        results = {}\n",
        "        training_histories = {}\n",
        "\n",
        "        epochs = 20\n",
        "\n",
        "        for name, model in models.items():\n",
        "            print(f\"\\nTraining {name}...\")\n",
        "\n",
        "            try:\n",
        "                history = model.fit(\n",
        "                    X_train_long, y_train_long,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_val_long, y_val_long),\n",
        "                    verbose=0\n",
        "                )\n",
        "\n",
        "                # Evaluate\n",
        "                test_loss = model.evaluate(X_test_long, y_test_long, verbose=0)[0]\n",
        "                final_val_loss = min(history.history['val_loss'])\n",
        "                convergence_epoch = np.argmin(history.history['val_loss'])\n",
        "\n",
        "                results[name] = {\n",
        "                    'test_mse': test_loss,\n",
        "                    'best_val_mse': final_val_loss,\n",
        "                    'convergence_epoch': convergence_epoch,\n",
        "                    'training_stable': True,\n",
        "                    'parameters': model.count_params()\n",
        "                }\n",
        "\n",
        "                training_histories[name] = history\n",
        "\n",
        "                print(f\"  Test MSE: {test_loss:.6f}\")\n",
        "                print(f\"  Best Val MSE: {final_val_loss:.6f}\")\n",
        "                print(f\"  Converged at epoch: {convergence_epoch}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Training failed: {str(e)}\")\n",
        "                results[name] = {\n",
        "                    'test_mse': float('inf'),\n",
        "                    'best_val_mse': float('inf'),\n",
        "                    'convergence_epoch': -1,\n",
        "                    'training_stable': False,\n",
        "                    'error': str(e)\n",
        "                }\n",
        "\n",
        "        return results, training_histories\n",
        "\n",
        "    # Run analyses\n",
        "    analyze_gradient_flow_detailed()\n",
        "    solution_results, solution_histories = implement_solutions()\n",
        "\n",
        "    # Comprehensive summary\n",
        "    print(\"\\nCOMPREHENSIVE SUMMARY:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create summary visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # 1. Challenge overview\n",
        "    ax = axes[0, 0]\n",
        "    challenge_names = list(challenges.keys())\n",
        "    severity_scores = [5, 4, 5, 3, 3]  # Relative severity (subjective)\n",
        "\n",
        "    bars = ax.barh(range(len(challenge_names)), severity_scores,\n",
        "                   color=['red', 'orange', 'darkred', 'blue', 'green'], alpha=0.7)\n",
        "    ax.set_yticks(range(len(challenge_names)))\n",
        "    ax.set_yticklabels([name.split('.')[1].strip() for name in challenge_names])\n",
        "    ax.set_xlabel('Relative Difficulty/Impact')\n",
        "    ax.set_title('RNN Training Challenges by Severity')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Solution effectiveness\n",
        "    ax = axes[0, 1]\n",
        "    if solution_results:\n",
        "        stable_models = {name: res for name, res in solution_results.items()\n",
        "                        if res['training_stable']}\n",
        "\n",
        "        if stable_models:\n",
        "            model_names = list(stable_models.keys())\n",
        "            test_mses = [stable_models[name]['test_mse'] for name in model_names]\n",
        "\n",
        "            bars = ax.bar(range(len(model_names)), test_mses, alpha=0.7,\n",
        "                         color=['red', 'orange', 'yellow', 'lightgreen', 'green'])\n",
        "            ax.set_xticks(range(len(model_names)))\n",
        "            ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
        "            ax.set_ylabel('Test MSE')\n",
        "            ax.set_title('Solution Effectiveness')\n",
        "\n",
        "            # Add value labels\n",
        "            for bar, value in zip(bars, test_mses):\n",
        "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "                        f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # 3. Training curves comparison\n",
        "    ax = axes[1, 0]\n",
        "    if solution_histories:\n",
        "        for name, history in solution_histories.items():\n",
        "            if 'val_loss' in history.history:\n",
        "                ax.plot(history.history['val_loss'], label=name, linewidth=2)\n",
        "\n",
        "        ax.set_title('Training Curves Comparison')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Validation Loss')\n",
        "        ax.set_yscale('log')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Solution summary table\n",
        "    ax = axes[1, 1]\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Create summary of solutions\n",
        "    solution_summary = [\n",
        "        ['Problem', 'Primary Solution', 'Secondary Solutions'],\n",
        "        ['Vanishing Gradients', 'LSTM/GRU', 'Layer Norm, Residual Conn.'],\n",
        "        ['Exploding Gradients', 'Gradient Clipping', 'Lower LR, Better Init'],\n",
        "        ['Long-term Deps', 'LSTM/GRU', 'Attention, Memory Networks'],\n",
        "        ['Complexity', '1D CNNs', 'Truncated BPTT, Parallelism'],\n",
        "        ['Overfitting', 'Dropout', 'Regularization, Early Stop']\n",
        "    ]\n",
        "\n",
        "    table = ax.table(\n",
        "        cellText=solution_summary[1:],\n",
        "        colLabels=solution_summary[0],\n",
        "        cellLoc='center',\n",
        "        loc='center'\n",
        "    )\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(9)\n",
        "    table.scale(1.2, 2)\n",
        "\n",
        "    # Style the header\n",
        "    for i in range(len(solution_summary[0])):\n",
        "        table[(0, i)].set_facecolor('#40466e')\n",
        "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "    ax.set_title('Solution Summary', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed explanation for each challenge\n",
        "    for challenge_name, challenge_info in challenges.items():\n",
        "        print(f\"\\n{challenge_name.upper()}\")\n",
        "        print(\"=\" * len(challenge_name))\n",
        "        print(f\"Description: {challenge_info['description']}\")\n",
        "        print(f\"Mathematical Cause: {challenge_info['mathematical_cause']}\")\n",
        "        print(f\"Formula: {challenge_info['formula']}\")\n",
        "        print(\"\\nEffects:\")\n",
        "        for effect in challenge_info['effects']:\n",
        "            print(f\"  • {effect}\")\n",
        "        print(\"\\nSolutions:\")\n",
        "        for solution in challenge_info['solutions']:\n",
        "            print(f\"  • {solution}\")\n",
        "\n",
        "    return challenges, solution_results\n",
        "\n",
        "# Run Exercise 5\n",
        "exercise_5_results = demonstrate_rnn_training_challenges()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise-6"
      },
      "source": [
        "### Exercise 6: LSTM Cell Architecture\n",
        "\n",
        "**Question**: Can you sketch the LSTM cell's architecture?\n",
        "\n",
        "#### Theoretical Foundation\n",
        "\n",
        "The LSTM (Long Short-Term Memory) cell is designed to solve the vanishing gradient problem through gating mechanisms.\n",
        "\n",
        "#### Mathematical Components\n",
        "\n",
        "**Gate Equations:**\n",
        "$$\\mathbf{f}^{(t)} = \\sigma(\\mathbf{W}_f \\cdot [\\mathbf{h}^{(t-1)}, \\mathbf{x}^{(t)}] + \\mathbf{b}_f)$$ (Forget Gate)\n",
        "$$\\mathbf{i}^{(t)} = \\sigma(\\mathbf{W}_i \\cdot [\\mathbf{h}^{(t-1)}, \\mathbf{x}^{(t)}] + \\mathbf{b}_i)$$ (Input Gate)\n",
        "$$\\mathbf{o}^{(t)} = \\sigma(\\mathbf{W}_o \\cdot [\\mathbf{h}^{(t-1)}, \\mathbf{x}^{(t)}] + \\mathbf{b}_o)$$ (Output Gate)\n",
        "\n",
        "**Candidate Values:**\n",
        "$$\\tilde{\\mathbf{C}}^{(t)} = \\tanh(\\mathbf{W}_C \\cdot [\\mathbf{h}^{(t-1)}, \\mathbf{x}^{(t)}] + \\mathbf{b}_C)$$\n",
        "\n",
        "**State Updates:**\n",
        "$$\\mathbf{C}^{(t)} = \\mathbf{f}^{(t)} * \\mathbf{C}^{(t-1)} + \\mathbf{i}^{(t)} * \\tilde{\\mathbf{C}}^{(t)}$$\n",
        "$$\\mathbf{h}^{(t)} = \\mathbf{o}^{(t)} * \\tanh(\\mathbf{C}^{(t)})$$\n",
        "\n",
        "Where $*$ denotes element-wise multiplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise-6-implementation"
      },
      "source": [
        "# Exercise 6: LSTM Cell Architecture Sketch and Implementation\n",
        "\n",
        "def sketch_lstm_architecture():\n",
        "    \"\"\"\n",
        "    Creates detailed visual sketch and explanation of LSTM architecture.\n",
        "    \"\"\"\n",
        "    print(\"EXERCISE 6: LSTM CELL ARCHITECTURE\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create comprehensive LSTM visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "    # 1. Complete LSTM Cell Architecture\n",
        "    ax = axes[0, 0]\n",
        "    ax.set_title('LSTM Cell Architecture', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Cell state line (top)\n",
        "    ax.arrow(0.05, 0.8, 0.9, 0, head_width=0.02, head_length=0.02,\n",
        "             fc='blue', ec='blue', linewidth=4)\n",
        "    ax.text(0.5, 0.85, 'Cell State C(t-1) → C(t)', ha='center', fontsize=12,\n",
        "           color='blue', fontweight='bold')\n",
        "\n",
        "    # Gates and operations\n",
        "    gates = [\n",
        "        (0.15, 0.5, 'σ', 'Forget\\nGate', 'lightcoral'),\n",
        "        (0.35, 0.5, 'σ', 'Input\\nGate', 'lightgreen'),\n",
        "        (0.55, 0.5, 'tanh', 'Candidate\\nValues', 'lightyellow'),\n",
        "        (0.85, 0.5, 'σ', 'Output\\nGate', 'lightblue')\n",
        "    ]\n",
        "\n",
        "    for x, y, symbol, label, color in gates:\n",
        "        # Gate box\n",
        "        rect = plt.Rectangle((x-0.04, y-0.08), 0.08, 0.16,\n",
        "                           facecolor=color, edgecolor='black', linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x, y+0.03, symbol, ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "        ax.text(x, y-0.05, label, ha='center', va='center', fontsize=8, fontweight='bold')\n",
        "\n",
        "    # Multiplication and addition operations\n",
        "    ax.text(0.25, 0.65, '×', ha='center', fontsize=24, fontweight='bold')  # Forget operation\n",
        "    ax.text(0.45, 0.65, '×', ha='center', fontsize=24, fontweight='bold')  # Input operation\n",
        "    ax.text(0.55, 0.8, '+', ha='center', fontsize=24, fontweight='bold', color='blue')  # Add to cell state\n",
        "    ax.text(0.85, 0.65, '×', ha='center', fontsize=24, fontweight='bold')  # Output operation\n",
        "\n",
        "    # Input and previous hidden state\n",
        "    ax.arrow(0.15, 0.2, 0, 0.22, head_width=0.02, head_length=0.02, fc='black')\n",
        "    ax.arrow(0.35, 0.2, 0, 0.22, head_width=0.02, head_length=0.02, fc='black')\n",
        "    ax.arrow(0.55, 0.2, 0, 0.22, head_width=0.02, head_length=0.02, fc='black')\n",
        "    ax.arrow(0.85, 0.2, 0, 0.22, head_width=0.02, head_length=0.02, fc='black')\n",
        "\n",
        "    ax.text(0.45, 0.1, 'x(t), h(t-1)', ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Output\n",
        "    ax.arrow(0.85, 0.73, 0, 0.1, head_width=0.02, head_length=0.02, fc='red', linewidth=3)\n",
        "    ax.text(0.9, 0.9, 'h(t)', ha='center', fontsize=12, color='red', fontweight='bold')\n",
        "\n",
        "    # tanh for output\n",
        "    ax.text(0.75, 0.65, 'tanh', ha='center', fontsize=10,\n",
        "           bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='yellow', alpha=0.7))\n",
        "\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # 2. Step-by-step computation flow\n",
        "    ax = axes[0, 1]\n",
        "    ax.set_title('LSTM Computation Steps', fontsize=14, fontweight='bold')\n",
        "\n",
        "    steps = [\n",
        "        '1. Forget Gate: f(t) = σ(Wf·[h(t-1), x(t)] + bf)',\n",
        "        '2. Input Gate: i(t) = σ(Wi·[h(t-1), x(t)] + bi)',\n",
        "        '3. Candidate: C̃(t) = tanh(WC·[h(t-1), x(t)] + bC)',\n",
        "        '4. Update Cell: C(t) = f(t)⊙C(t-1) + i(t)⊙C̃(t)',\n",
        "        '5. Output Gate: o(t) = σ(Wo·[h(t-1), x(t)] + bo)',\n",
        "        '6. Hidden State: h(t) = o(t)⊙tanh(C(t))'\n",
        "    ]\n",
        "\n",
        "    colors = ['lightcoral', 'lightgreen', 'lightyellow', 'lightblue', 'lightcyan', 'lightpink']\n",
        "\n",
        "    for i, (step, color) in enumerate(zip(steps, colors)):\n",
        "        y_pos = 0.9 - i * 0.13\n",
        "        rect = plt.Rectangle((0.05, y_pos-0.05), 0.9, 0.1,\n",
        "                           facecolor=color, edgecolor='black', alpha=0.7)\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(0.5, y_pos, step, ha='center', va='center', fontsize=10,\n",
        "               fontweight='bold', fontfamily='monospace')\n",
        "\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # 3. Information flow diagram\n",
        "    ax = axes[1, 0]\n",
        "    ax.set_title('Information Flow in LSTM', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Draw information pathways\n",
        "    # Long-term memory (cell state)\n",
        "    ax.arrow(0.1, 0.8, 0.8, 0, head_width=0.03, head_length=0.03,\n",
        "             fc='blue', ec='blue', linewidth=6, alpha=0.7)\n",
        "    ax.text(0.5, 0.85, 'Long-term Memory (Cell State)', ha='center',\n",
        "           fontsize=12, color='blue', fontweight='bold')\n",
        "\n",
        "    # Short-term memory (hidden state)\n",
        "    ax.arrow(0.1, 0.5, 0.8, 0, head_width=0.03, head_length=0.03,\n",
        "             fc='red', ec='red', linewidth=4, alpha=0.7)\n",
        "    ax.text(0.5, 0.55, 'Short-term Memory (Hidden State)', ha='center',\n",
        "           fontsize=12, color='red', fontweight='bold')\n",
        "\n",
        "    # Input flow\n",
        "    ax.arrow(0.5, 0.1, 0, 0.3, head_width=0.03, head_length=0.03,\n",
        "             fc='green', ec='green', linewidth=3, alpha=0.7)\n",
        "    ax.text(0.55, 0.25, 'Input x(t)', ha='left', fontsize=12,\n",
        "           color='green', fontweight='bold')\n",
        "\n",
        "    # Gate controls\n",
        "    gate_positions = [0.2, 0.4, 0.6, 0.8]\n",
        "    gate_names = ['Forget', 'Input', 'Candidate', 'Output']\n",
        "    gate_colors = ['coral', 'lightgreen', 'yellow', 'lightblue']\n",
        "\n",
        "    for pos, name, color in zip(gate_positions, gate_names, gate_colors):\n",
        "        circle = plt.Circle((pos, 0.65), 0.03, color=color, alpha=0.8)\n",
        "        ax.add_patch(circle)\n",
        "        ax.text(pos, 0.7, name, ha='center', fontsize=8, fontweight='bold')\n",
        "\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # 4. Comparison with Simple RNN\n",
        "    ax = axes[1, 1]\n",
        "    ax.set_title('LSTM vs Simple RNN', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Create comparison table\n",
        "    comparison_data = [\n",
        "        ['Aspect', 'Simple RNN', 'LSTM'],\n",
        "        ['Memory', 'Only hidden state', 'Cell state + hidden state'],\n",
        "        ['Gates', 'None', 'Forget, Input, Output'],\n",
        "        ['Gradient Flow', 'Vanishing problem', 'Controlled by gates'],\n",
        "        ['Long-term Deps', 'Poor', 'Excellent'],\n",
        "        ['Parameters', 'Fewer', 'More (4x weight matrices)'],\n",
        "        ['Computation', 'Faster', 'Slower but more capable']\n",
        "    ]\n",
        "\n",
        "    table = ax.table(\n",
        "        cellText=comparison_data[1:],\n",
        "        colLabels=comparison_data[0],\n",
        "        cellLoc='center',\n",
        "        loc='center'\n",
        "    )\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1.2, 2.5)\n",
        "\n",
        "    # Style the table\n",
        "    for i in range(len(comparison_data[0])):\n",
        "        table[(0, i)].set_facecolor('#40466e')\n",
        "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "    # Color code the comparison\n",
        "    for i in range(1, len(comparison_data)):\n",
        "        table[(i, 1)].set_facecolor('#ffcccc')  # Simple RNN - light red\n",
        "        table[(i, 2)].set_facecolor('#ccffcc')  # LSTM - light green\n",
        "\n",
        "    ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Detailed mathematical explanation\n",
        "    print(\"\\nDETAILED MATHEMATICAL EXPLANATION:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    print(\"\\n1. GATE COMPUTATIONS:\")\n",
        "    print(\"   Forget Gate:    f(t) = σ(Wf·[h(t-1), x(t)] + bf)\")\n",
        "    print(\"   Input Gate:     i(t) = σ(Wi·[h(t-1), x(t)] + bi)\")\n",
        "    print(\"   Output Gate:    o(t) = σ(Wo·[h(t-1), x(t)] + bo)\")\n",
        "    print(\"   Candidate:      C̃(t) = tanh(WC·[h(t-1), x(t)] + bC)\")\n",
        "\n",
        "    print(\"\\n2. STATE UPDATES:\")\n",
        "    print(\"   Cell State:     C(t) = f(t) ⊙ C(t-1) + i(t) ⊙ C̃(t)\")\n",
        "    print(\"   Hidden State:   h(t) = o(t) ⊙ tanh(C(t))\")\n",
        "\n",
        "    print(\"\\n3. KEY INNOVATIONS:\")\n",
        "    print(\"   • Separate cell state preserves long-term information\")\n",
        "    print(\"   • Forget gate selectively removes irrelevant information\")\n",
        "    print(\"   • Input gate controls what new information to store\")\n",
        "    print(\"   • Output gate controls what parts of cell state to reveal\")\n",
        "    print(\"   • Gradient flows through cell state without modification\")\n",
        "\n",
        "    print(\"\\n4. INTUITIVE UNDERSTANDING:\")\n",
        "    print(\"   • Cell state = long-term memory (conveyor belt)\")\n",
        "    print(\"   • Hidden state = working memory (what you're thinking about)\")\n",
        "    print(\"   • Gates = smart controllers deciding information flow\")\n",
        "    print(\"   • Solves vanishing gradients through controlled gradient flow\")\n",
        "\n",
        "def implement_custom_lstm():\n",
        "    \"\"\"\n",
        "    Implements a custom LSTM cell to demonstrate the architecture.\n",
        "    \"\"\"\n",
        "    print(\"\\nCUSTOM LSTM IMPLEMENTATION:\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    class CustomLSTMCell(keras.layers.Layer):\n",
        "        \"\"\"\n",
        "        Custom LSTM cell implementation for educational purposes.\n",
        "        \"\"\"\n",
        "        def __init__(self, units, **kwargs):\n",
        "            super().__init__(**kwargs)\n",
        "            self.units = units\n",
        "            self.state_size = [units, units]  # [hidden_state, cell_state]\n",
        "            self.output_size = units\n",
        "\n",
        "        def build(self, input_shape):\n",
        "            input_dim = input_shape[-1]\n",
        "\n",
        "            # Weight matrices for gates\n",
        "            self.W_f = self.add_weight(shape=(input_dim + self.units, self.units),\n",
        "                                      initializer='glorot_uniform', name='W_f')\n",
        "            self.W_i = self.add_weight(shape=(input_dim + self.units, self.units),\n",
        "                                      initializer='glorot_uniform', name='W_i')\n",
        "            self.W_o = self.add_weight(shape=(input_dim + self.units, self.units),\n",
        "                                      initializer='glorot_uniform', name='W_o')\n",
        "            self.W_c = self.add_weight(shape=(input_dim + self.units, self.units),\n",
        "                                      initializer='glorot_uniform', name='W_c')\n",
        "\n",
        "            # Bias vectors\n",
        "            self.b_f = self.add_weight(shape=(self.units,), initializer='zeros', name='b_f')\n",
        "            self.b_i = self.add_weight(shape=(self.units,), initializer='zeros', name='b_i')\n",
        "            self.b_o = self.add_weight(shape=(self.units,), initializer='zeros', name='b_o')\n",
        "            self.b_c = self.add_weight(shape=(self.units,), initializer='zeros', name='b_c')\n",
        "\n",
        "            super().build(input_shape)\n",
        "\n",
        "        def call(self, inputs, states):\n",
        "            h_prev, c_prev = states\n",
        "\n",
        "            # Concatenate input and previous hidden state\n",
        "            combined = tf.concat([inputs, h_prev], axis=1)\n",
        "\n",
        "            # Compute gates\n",
        "            f_gate = tf.sigmoid(tf.matmul(combined, self.W_f) + self.b_f)\n",
        "            i_gate = tf.sigmoid(tf.matmul(combined, self.W_i) + self.b_i)\n",
        "            o_gate = tf.sigmoid(tf.matmul(combined, self.W_o) + self.b_o)\n",
        "            c_candidate = tf.tanh(tf.matmul(combined, self.W_c) + self.b_c)\n",
        "\n",
        "            # Update cell state\n",
        "            c_new = f_gate * c_prev + i_gate * c_candidate\n",
        "\n",
        "            # Compute new hidden state\n",
        "            h_new = o_gate * tf.tanh(c_new)\n",
        "\n",
        "            return h_new, [h_new, c_new]\n",
        "\n",
        "    # Test the custom LSTM\n",
        "    print(\"Testing custom LSTM implementation...\")\n",
        "\n",
        "    # Create test data\n",
        "    batch_size = 4\n",
        "    seq_length = 10\n",
        "    input_size = 5\n",
        "    hidden_size = 8\n",
        "\n",
        "    test_input = tf.random.normal((batch_size, seq_length, input_size))\n",
        "\n",
        "    # Build model with custom LSTM\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.RNN(CustomLSTMCell(hidden_size), return_sequences=True),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Test forward pass\n",
        "    output = model(test_input)\n",
        "    print(f\"Custom LSTM output shape: {output.shape}\")\n",
        "    print(f\"Expected shape: ({batch_size}, {seq_length}, 1)\")\n",
        "\n",
        "    # Compare with built-in LSTM\n",
        "    builtin_model = keras.Sequential([\n",
        "        keras.layers.LSTM(hidden_size, return_sequences=True),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    builtin_output = builtin_model(test_input)\n",
        "    print(f\"Built-in LSTM output shape: {builtin_output.shape}\")\n",
        "\n",
        "    print(\"\\nCustom LSTM implementation successful!\")\n",
        "    print(f\"Custom LSTM parameters: {model.count_params():,}\")\n",
        "    print(f\"Built-in LSTM parameters: {builtin_model.count_params():,}\")\n",
        "\n",
        "    return model, builtin_model\n",
        "\n",
        "# Run Exercise 6\n",
        "sketch_lstm_architecture()\n",
        "custom_lstm, builtin_lstm = implement_custom_lstm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise-7"
      },
      "source": [
        "### Exercise 7: 1D Convolutional Layers in RNNs\n",
        "\n",
        "**Question**: Why would you want to use 1D convolutional layers in an RNN?\n",
        "\n",
        "#### Theoretical Justification\n",
        "\n",
        "Combining 1D CNNs with RNNs provides several advantages:\n",
        "\n",
        "**1. Computational Efficiency**\n",
        "- **Parallelization**: CNNs process all time steps simultaneously\n",
        "- **Speed**: Reduces sequence length before RNN processing\n",
        "- **Memory**: Lower memory requirements for long sequences\n",
        "\n",
        "**2. Feature Extraction**\n",
        "- **Local Patterns**: CNNs excel at detecting local temporal patterns\n",
        "- **Translation Invariance**: Same pattern detected regardless of position\n",
        "- **Hierarchical Features**: Multiple layers build complex representations\n",
        "\n",
        "**3. Preprocessing Benefits**\n",
        "- **Noise Reduction**: Convolutional filters can smooth noisy sequences\n",
        "- **Downsampling**: Reduces sequence length while preserving information\n",
        "- **Feature Engineering**: Automatically learns relevant features\n",
        "\n",
        "**Mathematical Framework:**\n",
        "For a 1D convolution followed by RNN:\n",
        "$$\\mathbf{z}^{(t)} = \\text{Conv1D}(\\mathbf{x}^{(t-k+1:t)})$$\n",
        "$$\\mathbf{h}^{(t)} = \\text{RNN}(\\mathbf{z}^{(t)}, \\mathbf{h}^{(t-1)})$$\n",
        "\n",
        "Where the CNN extracts local features $\\mathbf{z}^{(t)}$ that the RNN then processes."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}