{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Chapter 16: Natural Language Processing with RNNs and Attention\n",
        "\n",
        "## Comprehensive Implementation and Theory Guide\n",
        "\n",
        "This notebook provides a complete implementation and theoretical explanation of Chapter 16 from \"Hands-On Machine Learning\" by Aurélien Géron, covering Natural Language Processing with RNNs and Attention mechanisms.\n",
        "\n",
        "### Learning Objectives:\n",
        "1. **Character-level RNNs** for text generation\n",
        "2. **Sentiment Analysis** using word-level RNNs\n",
        "3. **Encoder-Decoder Networks** for Neural Machine Translation\n",
        "4. **Attention Mechanisms** (Bahdanau, Luong, Self-Attention)\n",
        "5. **Transformer Architecture** - the revolutionary \"Attention is All You Need\" model\n",
        "6. **Recent Language Models** (ELMo, ULMFiT, GPT, BERT)\n",
        "\n",
        "### Mathematical Foundations:\n",
        "We'll cover the mathematical theory behind each concept, including:\n",
        "- RNN forward propagation equations\n",
        "- Attention mechanism formulations\n",
        "- Scaled Dot-Product Attention\n",
        "- Multi-Head Attention mathematics\n",
        "- Positional encoding formulas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## Setup and Imports\n",
        "\n",
        "First, let's set up our environment with all necessary dependencies for this comprehensive NLP tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_code"
      },
      "source": [
        "# Install required packages for advanced NLP\n",
        "!pip install tensorflow-addons tensorflow-datasets tensorflow-hub\n",
        "!pip install transformers datasets tokenizers\n",
        "!pip install matplotlib seaborn plotly\n",
        "!pip install numpy pandas scipy scikit-learn\n",
        "\n",
        "# Clone the hands-on-ml2 repository for additional resources\n",
        "!git clone https://github.com/ageron/handson-ml2.git\n",
        "%cd handson-ml2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, losses, metrics\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Configure TensorFlow\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {tf.config.experimental.list_physical_devices('GPU')}\")\n",
        "print(f\"Eager execution: {tf.executing_eagerly()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_theory"
      },
      "source": [
        "## Introduction: The Turing Test and Language Understanding\n",
        "\n",
        "### Historical Context\n",
        "\n",
        "Alan Turing's famous 1950 paper \"Computing Machinery and Intelligence\" proposed what became known as the **Turing Test**. Interestingly, Turing chose a **linguistic task** to evaluate machine intelligence, highlighting that mastering language is arguably Homo sapiens's greatest cognitive ability.\n",
        "\n",
        "### The Turing Test (Imitation Game)\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "Let $M$ be a machine, $H$ be a human, and $I$ be an interrogator. The test can be formalized as:\n",
        "\n",
        "$$P(\\text{Machine passes}) = P(I \\text{ mistakes } M \\text{ for } H | \\text{text-only conversation})$$\n",
        "\n",
        "The machine passes if it can fool the interrogator into thinking it's human.\n",
        "\n",
        "### Why RNNs for NLP?\n",
        "\n",
        "Natural language has **sequential structure** where:\n",
        "1. **Order matters**: \"The cat sat on the mat\" ≠ \"Mat the on sat cat the\"\n",
        "2. **Context dependency**: Meaning depends on previous words\n",
        "3. **Variable length**: Sentences have different lengths\n",
        "4. **Long-term dependencies**: Words can reference concepts mentioned much earlier\n",
        "\n",
        "**RNN Mathematical Foundation:**\n",
        "For a sequence $x_1, x_2, ..., x_T$, an RNN computes:\n",
        "\n",
        "$$h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)$$\n",
        "$$y_t = W_{hy} \\cdot h_t + b_y$$\n",
        "\n",
        "Where:\n",
        "- $h_t$: hidden state at time $t$\n",
        "- $x_t$: input at time $t$ \n",
        "- $y_t$: output at time $t$\n",
        "- $W_{xh}, W_{hh}, W_{hy}$: weight matrices\n",
        "- $b_h, b_y$: bias vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "char_rnn_theory"
      },
      "source": [
        "## Part 1: Character-Level RNNs for Text Generation\n",
        "\n",
        "### Theory: Character-Level Language Modeling\n",
        "\n",
        "A **Character RNN** predicts the next character in a sequence, learning to model the probability distribution:\n",
        "\n",
        "$$P(c_{t+1} | c_1, c_2, ..., c_t)$$\n",
        "\n",
        "Where $c_i$ represents the $i$-th character in the sequence.\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "For character-level modeling:\n",
        "1. **Input encoding**: Each character $c$ is mapped to a one-hot vector $x \\in \\mathbb{R}^V$ where $V$ is vocabulary size\n",
        "2. **Hidden state evolution**: $h_t = \\text{GRU}(x_t, h_{t-1})$ or $h_t = \\text{LSTM}(x_t, h_{t-1})$\n",
        "3. **Output prediction**: $y_t = \\text{softmax}(W_o h_t + b_o)$ gives probability distribution over characters\n",
        "4. **Loss function**: Cross-entropy loss $L = -\\sum_{t=1}^T \\log P(c_{t+1} | c_1, ..., c_t)$\n",
        "\n",
        "### Why Character-Level?\n",
        "\n",
        "**Advantages:**\n",
        "- **Smaller vocabulary**: Only need ~100 characters vs 50K+ words\n",
        "- **No out-of-vocabulary issues**: Can handle any text\n",
        "- **Learns morphology**: Understands word structure\n",
        "- **Language agnostic**: Works across languages\n",
        "\n",
        "**Disadvantages:**\n",
        "- **Longer sequences**: More time steps to process\n",
        "- **Harder to capture long-term dependencies**: Character-level patterns vs word-level semantics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_prep_theory"
      },
      "source": [
        "### Dataset Preparation Theory\n",
        "\n",
        "#### Sequential Dataset Splitting\n",
        "\n",
        "Unlike image classification, we **cannot randomly shuffle** sequential data. Proper splitting for time series/sequential data:\n",
        "\n",
        "**Temporal Split:**\n",
        "- Training: First 90% chronologically\n",
        "- Validation: Next 5% \n",
        "- Test: Final 5%\n",
        "\n",
        "**Mathematical Consideration - Stationarity:**\n",
        "We assume the time series is **stationary**, meaning statistical properties don't change over time:\n",
        "$$E[X_t] = \\mu \\text{ (constant mean)}$$\n",
        "$$\\text{Var}(X_t) = \\sigma^2 \\text{ (constant variance)}$$\n",
        "$$\\text{Cov}(X_t, X_{t+k}) = \\gamma(k) \\text{ (covariance depends only on lag)}$$\n",
        "\n",
        "#### Windowing Strategy\n",
        "\n",
        "**Problem:** Cannot train on entire sequence (would be like a network with millions of layers)\n",
        "\n",
        "**Solution:** **Truncated Backpropagation Through Time (TBPTT)**\n",
        "\n",
        "Split long sequence into overlapping windows:\n",
        "- Window 1: characters 0-100\n",
        "- Window 2: characters 1-101  \n",
        "- Window 3: characters 2-102\n",
        "- ...\n",
        "\n",
        "**Mathematical Impact:**\n",
        "- **Gradient flow**: Limited to window size\n",
        "- **Memory requirements**: $O(\\text{window_size})$ instead of $O(\\text{sequence_length})$\n",
        "- **Trade-off**: Shorter windows → faster training but limited long-term learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shakespeare_data"
      },
      "source": [
        "# Download Shakespeare's complete works\n",
        "# This demonstrates the data acquisition step from the book\n",
        "shakespeare_url = \"https://homl.info/shakespeare\"\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "\n",
        "# Read the text data\n",
        "with open(filepath, 'r', encoding='utf-8') as f:\n",
        "    shakespeare_text = f.read()\n",
        "\n",
        "print(f\"Shakespeare corpus statistics:\")\n",
        "print(f\"Total characters: {len(shakespeare_text):,}\")\n",
        "print(f\"First 200 characters:\")\n",
        "print(repr(shakespeare_text[:200]))\n",
        "\n",
        "# Character frequency analysis\n",
        "char_freq = Counter(shakespeare_text)\n",
        "print(f\"\\nUnique characters: {len(char_freq)}\")\n",
        "print(f\"Most common characters: {char_freq.most_common(10)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tokenization_theory"
      },
      "source": [
        "### Tokenization Theory\n",
        "\n",
        "**Tokenization** converts text into numerical sequences for neural network processing.\n",
        "\n",
        "#### Character-Level Tokenization\n",
        "\n",
        "**Process:**\n",
        "1. **Vocabulary creation**: $V = \\{c_1, c_2, ..., c_{|V|}\\}$ where each $c_i$ is a unique character\n",
        "2. **Character mapping**: $f: c \\rightarrow \\mathbb{N}$ where $f(c_i) = i$\n",
        "3. **Sequence encoding**: Text \"hello\" → [8, 5, 12, 12, 15] (example indices)\n",
        "\n",
        "**One-Hot Encoding:**\n",
        "Each character index $i$ becomes a vector $e_i \\in \\{0,1\\}^{|V|}$ where:\n",
        "$$e_i[j] = \\begin{cases} 1 & \\text{if } j = i \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
        "\n",
        "**Memory Complexity:**\n",
        "- **Dense representation**: $O(T)$ for sequence length $T$\n",
        "- **One-hot representation**: $O(T \\times |V|)$\n",
        "- **Embedding representation**: $O(T \\times d)$ where $d$ is embedding dimension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tokenizer_implementation"
      },
      "source": [
        "# Character-level tokenization implementation\n",
        "# This follows the exact approach from the book\n",
        "\n",
        "# Create character-level tokenizer\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts([shakespeare_text])\n",
        "\n",
        "# Analyze tokenizer properties\n",
        "max_id = len(tokenizer.word_index)  # Number of distinct characters\n",
        "dataset_size = len(shakespeare_text)  # Total number of characters\n",
        "\n",
        "print(f\"Tokenizer Analysis:\")\n",
        "print(f\"Vocabulary size (unique characters): {max_id}\")\n",
        "print(f\"Total dataset size: {dataset_size:,} characters\")\n",
        "\n",
        "# Demonstrate encoding and decoding\n",
        "test_text = \"First\"\n",
        "encoded = tokenizer.texts_to_sequences([test_text])\n",
        "decoded = tokenizer.sequences_to_texts(encoded)\n",
        "\n",
        "print(f\"\\nTokenization demonstration:\")\n",
        "print(f\"Original: '{test_text}'\")\n",
        "print(f\"Encoded: {encoded[0]}\")\n",
        "print(f\"Decoded: '{decoded[0]}'\")\n",
        "\n",
        "# Show character to ID mapping for first 20 characters\n",
        "print(f\"\\nCharacter to ID mapping (first 20):\")\n",
        "for char, idx in list(tokenizer.word_index.items())[:20]:\n",
        "    print(f\"'{char}' -> {idx}\")\n",
        "\n",
        "# Encode the entire text (subtract 1 to get IDs from 0 to max_id-1)\n",
        "[encoded_text] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
        "print(f\"\\nEncoded text shape: {encoded_text.shape}\")\n",
        "print(f\"First 50 encoded characters: {encoded_text[:50]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "windowing_theory"
      },
      "source": [
        "### Windowing and Dataset Creation Theory\n",
        "\n",
        "#### The Windowing Problem\n",
        "\n",
        "**Challenge:** Transform a single long sequence into multiple training examples\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "Given sequence $S = [s_1, s_2, ..., s_N]$ of length $N$, create windows of size $w$:\n",
        "\n",
        "$$W_i = [s_i, s_{i+1}, ..., s_{i+w-1}] \\text{ for } i = 1, 2, ..., N-w+1$$\n",
        "\n",
        "**Input-Target Pairs:**\n",
        "For next-character prediction:\n",
        "- **Input**: $X_i = [s_i, s_{i+1}, ..., s_{i+w-2}]$ (first $w-1$ characters)\n",
        "- **Target**: $Y_i = [s_{i+1}, s_{i+2}, ..., s_{i+w-1}]$ (shifted by 1)\n",
        "\n",
        "#### TensorFlow Dataset Operations\n",
        "\n",
        "**Key Operations:**\n",
        "1. **`tf.data.Dataset.from_tensor_slices()`**: Creates dataset from tensor\n",
        "2. **`dataset.window()`**: Creates sliding windows\n",
        "3. **`dataset.flat_map()`**: Flattens nested datasets\n",
        "4. **`dataset.batch()`**: Groups examples into batches\n",
        "5. **`dataset.map()`**: Applies transformations\n",
        "6. **`dataset.shuffle()`**: Randomizes order\n",
        "7. **`dataset.prefetch()`**: Optimizes pipeline performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dataset_creation"
      },
      "source": [
        "# Dataset creation following the book's methodology\n",
        "# This demonstrates the complete pipeline from raw text to training data\n",
        "\n",
        "# Split data: 90% train, 10% validation/test\n",
        "train_size = dataset_size * 90 // 100\n",
        "train_encoded = encoded_text[:train_size]\n",
        "val_encoded = encoded_text[train_size:]\n",
        "\n",
        "print(f\"Data splitting:\")\n",
        "print(f\"Training size: {len(train_encoded):,} characters\")\n",
        "print(f\"Validation size: {len(val_encoded):,} characters\")\n",
        "\n",
        "# Create TensorFlow dataset from training data\n",
        "dataset = tf.data.Dataset.from_tensor_slices(train_encoded)\n",
        "\n",
        "# Define window parameters\n",
        "n_steps = 100  # Sequence length for training\n",
        "window_length = n_steps + 1  # +1 because target = input shifted by 1\n",
        "\n",
        "print(f\"\\nWindow configuration:\")\n",
        "print(f\"Input sequence length: {n_steps}\")\n",
        "print(f\"Window length (input + target): {window_length}\")\n",
        "\n",
        "# Create sliding windows with overlap\n",
        "# shift=1 means each window starts 1 character after the previous\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
        "\n",
        "# Convert nested dataset to flat dataset of tensors\n",
        "# Each window becomes a single tensor of shape [window_length]\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "# Batch windows together for efficient training\n",
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "\n",
        "# Split each window into input (first n_steps) and target (last n_steps)\n",
        "# Input: characters 0 to n_steps-1\n",
        "# Target: characters 1 to n_steps (shifted by 1)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "\n",
        "# Convert character indices to one-hot vectors\n",
        "# This is necessary for the embedding layer or direct one-hot input\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "\n",
        "# Optimize dataset performance\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Examine a sample batch\n",
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(f\"\\nBatch shapes:\")\n",
        "    print(f\"Input shape: {X_batch.shape}\")\n",
        "    print(f\"Target shape: {Y_batch.shape}\")\n",
        "    print(f\"\\nFirst example in batch:\")\n",
        "    print(f\"Input characters (first 10): {tf.argmax(X_batch[0][:10], axis=-1).numpy()}\")\n",
        "    print(f\"Target characters (first 10): {Y_batch[0][:10].numpy()}\")\n",
        "    \n",
        "    # Verify the shift relationship\n",
        "    input_chars = tf.argmax(X_batch[0], axis=-1).numpy()\n",
        "    target_chars = Y_batch[0].numpy()\n",
        "    print(f\"\\nVerifying shift relationship (first 5 pairs):\")\n",
        "    for i in range(5):\n",
        "        print(f\"Input[{i}]: {input_chars[i]} -> Target[{i}]: {target_chars[i]} (should be Input[{i+1}]: {input_chars[i+1] if i+1 < len(input_chars) else 'N/A'})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "char_rnn_model_theory"
      },
      "source": [
        "### Character RNN Model Architecture Theory\n",
        "\n",
        "#### GRU vs LSTM for Text Generation\n",
        "\n",
        "**GRU (Gated Recurrent Unit) Mathematical Formulation:**\n",
        "\n",
        "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$$\n",
        "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$$\n",
        "$$\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t])$$\n",
        "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
        "\n",
        "Where:\n",
        "- $r_t$: reset gate (controls how much past information to forget)\n",
        "- $z_t$: update gate (controls how much new information to accept)\n",
        "- $\\tilde{h}_t$: candidate hidden state\n",
        "- $\\odot$: element-wise multiplication\n",
        "- $\\sigma$: sigmoid function\n",
        "\n",
        "**Why GRU for Text Generation?**\n",
        "1. **Simpler than LSTM**: Fewer parameters (2 gates vs 3)\n",
        "2. **Good performance**: Often matches LSTM performance\n",
        "3. **Faster training**: Fewer computations per time step\n",
        "4. **Better gradient flow**: Less vanishing gradient issues than vanilla RNN\n",
        "\n",
        "#### Dropout in RNNs\n",
        "\n",
        "**Two types of dropout:**\n",
        "1. **Input dropout**: Applied to inputs at each time step\n",
        "2. **Recurrent dropout**: Applied to hidden state connections\n",
        "\n",
        "**Mathematical formulation:**\n",
        "$$h_t = \\text{GRU}(\\text{dropout}(x_t), \\text{recurrent_dropout}(h_{t-1}))$$\n",
        "\n",
        "**Benefits:**\n",
        "- **Regularization**: Prevents overfitting\n",
        "- **Improved generalization**: Forces model to not rely on specific neurons\n",
        "- **Better text diversity**: Reduces repetitive patterns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "char_rnn_model"
      },
      "source": [
        "# Character RNN Model Implementation\n",
        "# This follows the exact architecture described in the book\n",
        "\n",
        "def create_char_rnn_model(vocab_size, embedding_dim=None, rnn_units=128, \n",
        "                         dropout_rate=0.2, num_layers=2):\n",
        "    \"\"\"\n",
        "    Create a character-level RNN model for text generation.\n",
        "    \n",
        "    Args:\n",
        "        vocab_size: Size of character vocabulary\n",
        "        embedding_dim: Dimension of character embeddings (None for one-hot)\n",
        "        rnn_units: Number of units in each GRU layer\n",
        "        dropout_rate: Dropout rate for regularization\n",
        "        num_layers: Number of GRU layers\n",
        "    \n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    model = keras.models.Sequential([\n",
        "        # First GRU layer\n",
        "        # return_sequences=True: return full sequence, not just last output\n",
        "        # input_shape=[None, vocab_size]: variable length sequences, one-hot encoded\n",
        "        keras.layers.GRU(\n",
        "            rnn_units, \n",
        "            return_sequences=True, \n",
        "            input_shape=[None, vocab_size],\n",
        "            dropout=dropout_rate,           # Input dropout\n",
        "            recurrent_dropout=dropout_rate  # Recurrent dropout\n",
        "        ),\n",
        "        \n",
        "        # Second GRU layer\n",
        "        keras.layers.GRU(\n",
        "            rnn_units, \n",
        "            return_sequences=True,\n",
        "            dropout=dropout_rate,\n",
        "            recurrent_dropout=dropout_rate\n",
        "        ),\n",
        "        \n",
        "        # Output layer: Dense layer applied at each time step\n",
        "        # TimeDistributed applies the Dense layer to each time step independently\n",
        "        keras.layers.TimeDistributed(\n",
        "            keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "        )\n",
        "    ])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "char_rnn_model = create_char_rnn_model(\n",
        "    vocab_size=max_id,\n",
        "    rnn_units=128,\n",
        "    dropout_rate=0.2,\n",
        "    num_layers=2\n",
        ")\n",
        "\n",
        "# Display model architecture\n",
        "print(\"Character RNN Model Architecture:\")\n",
        "char_rnn_model.summary()\n",
        "\n",
        "# Compile the model\n",
        "# sparse_categorical_crossentropy: targets are integers, not one-hot\n",
        "char_rnn_model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Calculate total parameters\n",
        "total_params = char_rnn_model.count_params()\n",
        "print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
        "\n",
        "# Analyze parameter distribution\n",
        "print(\"\\nParameter distribution by layer:\")\n",
        "for i, layer in enumerate(char_rnn_model.layers):\n",
        "    layer_params = layer.count_params()\n",
        "    print(f\"Layer {i+1} ({layer.__class__.__name__}): {layer_params:,} parameters\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_theory"
      },
      "source": [
        "### Training Theory and Optimization\n",
        "\n",
        "#### Loss Function: Sparse Categorical Crossentropy\n",
        "\n",
        "For character prediction, we use **sparse categorical crossentropy**:\n",
        "\n",
        "$$L = -\\frac{1}{T} \\sum_{t=1}^{T} \\log P(c_{t+1}^{\\text{true}} | c_1, ..., c_t)$$\n",
        "\n",
        "Where:\n",
        "- $T$: sequence length\n",
        "- $c_{t+1}^{\\text{true}}$: true next character (integer index)\n",
        "- $P(c_{t+1}^{\\text{true}} | c_1, ..., c_t)$: predicted probability of true character\n",
        "\n",
        "#### Gradient Computation in RNNs\n",
        "\n",
        "**Backpropagation Through Time (BPTT):**\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial W} = \\sum_{t=1}^{T} \\frac{\\partial L_t}{\\partial W}$$\n",
        "\n",
        "$$\\frac{\\partial L_t}{\\partial W} = \\frac{\\partial L_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial W} + \\sum_{k=1}^{t-1} \\frac{\\partial L_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial h_k} \\frac{\\partial h_k}{\\partial W}$$\n",
        "\n",
        "**Vanishing Gradient Problem:**\n",
        "$$\\frac{\\partial h_t}{\\partial h_k} = \\prod_{i=k+1}^{t} \\frac{\\partial h_i}{\\partial h_{i-1}}$$\n",
        "\n",
        "If $\\left|\\frac{\\partial h_i}{\\partial h_{i-1}}\\right| < 1$, then gradients vanish exponentially with sequence length.\n",
        "\n",
        "#### Adam Optimizer\n",
        "\n",
        "**Adaptive Moment Estimation:**\n",
        "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n",
        "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n",
        "$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n",
        "$$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
        "$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$\n",
        "\n",
        "Where:\n",
        "- $g_t$: gradient at step $t$\n",
        "- $m_t$: first moment estimate (momentum)\n",
        "- $v_t$: second moment estimate (variance)\n",
        "- $\\beta_1, \\beta_2$: decay rates (typically 0.9, 0.999)\n",
        "- $\\alpha$: learning rate\n",
        "- $\\epsilon$: numerical stability term"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "training_char_rnn"
      },
      "source": [
        "# Training the Character RNN\n",
        "# This demonstrates the training process with monitoring and callbacks\n",
        "\n",
        "# Create callbacks for monitoring training\n",
        "callbacks = [\n",
        "    # Reduce learning rate when validation loss plateaus\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    ),\n",
        "    \n",
        "    # Early stopping to prevent overfitting\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    \n",
        "    # Save model checkpoints\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        'char_rnn_best.h5',\n",
        "        monitor='loss',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting Character RNN training...\")\n",
        "print(f\"Dataset size: {len(list(dataset))} batches\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Sequence length: {n_steps}\")\n",
        "\n",
        "# Note: Training on full dataset takes hours. For demonstration, we'll train on subset\n",
        "# For full training, increase epochs to 20-50\n",
        "epochs = 3  # Reduced for demonstration\n",
        "\n",
        "# Get dataset size for progress tracking\n",
        "steps_per_epoch = len(list(dataset.take(-1)))\n",
        "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "\n",
        "# Train the model\n",
        "history = char_rnn_model.fit(\n",
        "    dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.title('Model Loss During Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.title('Model Accuracy During Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final metrics\n",
        "final_loss = history.history['loss'][-1]\n",
        "final_accuracy = history.history['accuracy'][-1]\n",
        "print(f\"\\nFinal Training Metrics:\")\n",
        "print(f\"Loss: {final_loss:.4f}\")\n",
        "print(f\"Accuracy: {final_accuracy:.4f}\")\n",
        "print(f\"Perplexity: {np.exp(final_loss):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "text_generation_theory"
      },
      "source": [
        "### Text Generation Theory\n",
        "\n",
        "#### Sampling Strategies\n",
        "\n",
        "**1. Greedy Sampling:**\n",
        "Always pick the most likely next character:\n",
        "$$c_{t+1} = \\arg\\max_c P(c | c_1, ..., c_t)$$\n",
        "\n",
        "**Problem:** Often leads to repetitive, boring text\n",
        "\n",
        "**2. Random Sampling with Temperature:**\n",
        "Sample from probability distribution with temperature scaling:\n",
        "\n",
        "$$P_\\tau(c_i | \\text{context}) = \\frac{\\exp(z_i / \\tau)}{\\sum_j \\exp(z_j / \\tau)}$$\n",
        "\n",
        "Where:\n",
        "- $z_i$: logit for character $i$\n",
        "- $\\tau$: temperature parameter\n",
        "\n",
        "**Temperature Effects:**\n",
        "- $\\tau \\to 0$: **Deterministic** (greedy sampling)\n",
        "- $\\tau = 1$: **Original distribution**\n",
        "- $\\tau > 1$: **More random** (flatter distribution)\n",
        "- $\\tau \\to \\infty$: **Uniform random**\n",
        "\n",
        "#### TensorFlow's tf.random.categorical()\n",
        "\n",
        "Samples from categorical distribution given log probabilities:\n",
        "$$\\text{categorical}(\\text{logits}) \\sim \\text{Multinomial}(1, \\text{softmax}(\\text{logits}))$$\n",
        "\n",
        "#### Generation Process\n",
        "\n",
        "**Iterative Generation:**\n",
        "1. Start with seed text\n",
        "2. Predict next character probabilities\n",
        "3. Sample next character using temperature\n",
        "4. Append to sequence\n",
        "5. Repeat until desired length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "text_generation"
      },
      "source": [
        "# Text Generation Implementation\n",
        "# This implements the text generation strategies described in the book\n",
        "\n",
        "def preprocess_text(texts, tokenizer, max_id):\n",
        "    \"\"\"\n",
        "    Preprocess text for model input.\n",
        "    \n",
        "    Args:\n",
        "        texts: List of text strings\n",
        "        tokenizer: Fitted tokenizer\n",
        "        max_id: Vocabulary size\n",
        "    \n",
        "    Returns:\n",
        "        One-hot encoded tensor\n",
        "    \"\"\"\n",
        "    # Convert text to sequences of character IDs\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "    # Convert to one-hot encoding\n",
        "    return tf.one_hot(X, max_id)\n",
        "\n",
        "def next_char(text, model, tokenizer, max_id, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Predict the next character given current text.\n",
        "    \n",
        "    Args:\n",
        "        text: Current text string\n",
        "        model: Trained character RNN model\n",
        "        tokenizer: Fitted tokenizer\n",
        "        max_id: Vocabulary size\n",
        "        temperature: Sampling temperature\n",
        "    \n",
        "    Returns:\n",
        "        Next character string\n",
        "    \"\"\"\n",
        "    # Preprocess input text\n",
        "    X_new = preprocess_text([text], tokenizer, max_id)\n",
        "    \n",
        "    # Get model predictions for the last time step\n",
        "    y_proba = model.predict(X_new, verbose=0)[0, -1:, :]\n",
        "    \n",
        "    # Apply temperature scaling to logits\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    \n",
        "    # Sample from the rescaled distribution\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "    \n",
        "    # Convert back to character\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
        "\n",
        "def complete_text(text, model, tokenizer, max_id, n_chars=50, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate text by repeatedly predicting next characters.\n",
        "    \n",
        "    Args:\n",
        "        text: Seed text\n",
        "        model: Trained model\n",
        "        tokenizer: Fitted tokenizer  \n",
        "        max_id: Vocabulary size\n",
        "        n_chars: Number of characters to generate\n",
        "        temperature: Sampling temperature\n",
        "    \n",
        "    Returns:\n",
        "        Generated text string\n",
        "    \"\"\"\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, model, tokenizer, max_id, temperature)\n",
        "    return text\n",
        "\n",
        "# Test text generation with different temperatures\n",
        "seed_texts = [\"To be or not to be\", \"Romeo\", \"The king\"]\n",
        "temperatures = [0.2, 1.0, 2.0]\n",
        "\n",
        "print(\"Text Generation Examples:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for seed in seed_texts:\n",
        "    print(f\"\\nSeed text: '{seed}'\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    for temp in temperatures:\n",
        "        generated = complete_text(\n",
        "            seed, char_rnn_model, tokenizer, max_id, \n",
        "            n_chars=100, temperature=temp\n",
        "        )\n",
        "        print(f\"Temperature {temp}: {generated}\")\n",
        "\n",
        "# Analyze temperature effects\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Temperature Analysis:\")\n",
        "print(\"• Low temperature (0.2): Conservative, repetitive, more realistic\")\n",
        "print(\"• Medium temperature (1.0): Balanced creativity and coherence\")\n",
        "print(\"• High temperature (2.0): Creative but potentially nonsensical\")\n",
        "\n",
        "# Character frequency analysis in generated text\n",
        "test_generation = complete_text(\n",
        "    \"Shakespeare \", char_rnn_model, tokenizer, max_id, \n",
        "    n_chars=500, temperature=1.0\n",
        ")\n",
        "\n",
        "print(f\"\\nGenerated text sample (500 chars):\")\n",
        "print(test_generation)\n",
        "\n",
        "# Analyze character frequencies\n",
        "generated_freq = Counter(test_generation)\n",
        "original_freq = Counter(shakespeare_text[:1000])  # Compare with original\n",
        "\n",
        "print(f\"\\nCharacter frequency comparison (top 10):\")\n",
        "print(\"Generated | Original\")\n",
        "for (char_g, freq_g), (char_o, freq_o) in zip(generated_freq.most_common(10), original_freq.most_common(10)):\n",
        "    print(f\"'{char_g}': {freq_g:3d}   | '{char_o}': {freq_o:3d}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stateful_rnn_theory"
      },
      "source": [
        "## Stateful RNNs: Theory and Implementation\n",
        "\n",
        "### Problem with Stateless RNNs\n",
        "\n",
        "**Stateless RNN Limitation:**\n",
        "- Hidden state resets to zero at each batch\n",
        "- Cannot learn patterns longer than sequence length\n",
        "- Information from previous batches is lost\n",
        "\n",
        "### Stateful RNN Theory\n",
        "\n",
        "**Key Concept:** Preserve hidden state between batches\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "For batch $b$ at time $t$:\n",
        "$$h_t^{(b)} = f(x_t^{(b)}, h_{T}^{(b-1)})$$\n",
        "\n",
        "Where:\n",
        "- $h_T^{(b-1)}$: final hidden state from previous batch\n",
        "- $h_0^{(0)} = \\mathbf{0}$: initial state is zero\n",
        "\n",
        "### Requirements for Stateful RNNs\n",
        "\n",
        "**1. Sequential Data Ordering:**\n",
        "Each sequence in batch $b$ must continue where corresponding sequence in batch $b-1$ ended.\n",
        "\n",
        "**2. No Shuffling:**\n",
        "Data order must be preserved chronologically.\n",
        "\n",
        "**3. Fixed Batch Size:**\n",
        "Model needs to know batch size for state management.\n",
        "\n",
        "**4. Manual State Reset:**\n",
        "Must reset states at logical boundaries (e.g., end of epoch).\n",
        "\n",
        "### Batching Strategy for Stateful RNNs\n",
        "\n",
        "**Problem:** Standard batching doesn't preserve continuity\n",
        "\n",
        "**Solution 1: Single Sequence per Batch**\n",
        "- Batch size = 1\n",
        "- Simple but inefficient\n",
        "\n",
        "**Solution 2: Parallel Sequences**\n",
        "- Split text into $N$ parallel streams\n",
        "- Each batch contains one segment from each stream\n",
        "- Requires careful data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stateful_rnn_implementation"
      },
      "source": [
        "# Stateful RNN Implementation\n",
        "# This demonstrates the stateful RNN approach from the book\n",
        "\n",
        "def create_stateful_dataset(encoded_text, n_steps, batch_size=1):\n",
        "    \"\"\"\n",
        "    Create dataset for stateful RNN training.\n",
        "    \n",
        "    Args:\n",
        "        encoded_text: Encoded text sequence\n",
        "        n_steps: Sequence length\n",
        "        batch_size: Batch size (must be fixed for stateful RNN)\n",
        "    \n",
        "    Returns:\n",
        "        TensorFlow dataset\n",
        "    \"\"\"\n",
        "    # Create dataset from encoded text\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "    \n",
        "    # Create non-overlapping windows (shift = n_steps, not 1)\n",
        "    window_length = n_steps + 1\n",
        "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "    \n",
        "    # Flatten nested dataset\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "    \n",
        "    # Batch the windows (batch_size sequences per batch)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    \n",
        "    # Split into input and target\n",
        "    dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "    \n",
        "    # Convert to one-hot encoding\n",
        "    dataset = dataset.map(\n",
        "        lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
        "    )\n",
        "    \n",
        "    # Prefetch for performance\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def create_stateful_char_rnn(vocab_size, rnn_units=128, batch_size=1):\n",
        "    \"\"\"\n",
        "    Create a stateful character RNN model.\n",
        "    \n",
        "    Args:\n",
        "        vocab_size: Size of character vocabulary\n",
        "        rnn_units: Number of RNN units\n",
        "        batch_size: Fixed batch size for stateful RNN\n",
        "    \n",
        "    Returns:\n",
        "        Compiled stateful model\n",
        "    \"\"\"\n",
        "    model = keras.models.Sequential([\n",
        "        # First GRU layer with stateful=True\n",
        "        keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            stateful=True,  # KEY: Enable stateful mode\n",
        "            dropout=0.2,\n",
        "            recurrent_dropout=0.2,\n",
        "            # CRITICAL: Must specify batch_input_shape for stateful RNN\n",
        "            batch_input_shape=[batch_size, None, vocab_size]\n",
        "        ),\n",
        "        \n",
        "        # Second GRU layer\n",
        "        keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            stateful=True,  # Also stateful\n",
        "            dropout=0.2,\n",
        "            recurrent_dropout=0.2\n",
        "        ),\n",
        "        \n",
        "        # Output layer\n",
        "        keras.layers.TimeDistributed(\n",
        "            keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "        )\n",
        "    ])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Custom callback to reset states at epoch boundaries\n",
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Callback to reset RNN states at the beginning of each epoch.\n",
        "    This is crucial for stateful RNNs to prevent information leakage\n",
        "    across epoch boundaries.\n",
        "    \"\"\"\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        print(f\"\\nEpoch {epoch + 1}: Resetting RNN states\")\n",
        "        self.model.reset_states()\n",
        "\n",
        "# Create stateful dataset and model\n",
        "print(\"Creating Stateful RNN Dataset and Model...\")\n",
        "\n",
        "# Use smaller batch size for stateful RNN demonstration\n",
        "stateful_batch_size = 1\n",
        "stateful_n_steps = 100\n",
        "\n",
        "# Create stateful dataset\n",
        "stateful_dataset = create_stateful_dataset(\n",
        "    train_encoded, \n",
        "    stateful_n_steps, \n",
        "    stateful_batch_size\n",
        ")\n",
        "\n",
        "# Create stateful model\n",
        "stateful_model = create_stateful_char_rnn(\n",
        "    vocab_size=max_id,\n",
        "    rnn_units=128,\n",
        "    batch_size=stateful_batch_size\n",
        ")\n",
        "\n",
        "print(\"\\nStateful RNN Model Architecture:\")\n",
        "stateful_model.summary()\n",
        "\n",
        "# Compile the stateful model\n",
        "stateful_model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Analyze dataset structure\n",
        "print(f\"\\nStateful Dataset Analysis:\")\n",
        "dataset_size = len(list(stateful_dataset.take(-1)))\n",
        "print(f\"Total batches: {dataset_size}\")\n",
        "print(f\"Batch size: {stateful_batch_size}\")\n",
        "print(f\"Sequence length: {stateful_n_steps}\")\n",
        "\n",
        "# Examine a sample batch\n",
        "for X_batch, Y_batch in stateful_dataset.take(1):\n",
        "    print(f\"\\nBatch shapes:\")\n",
        "    print(f\"Input: {X_batch.shape}\")\n",
        "    print(f\"Target: {Y_batch.shape}\")\n",
        "\n",
        "# Key differences from stateless RNN\n",
        "print(\"\\nKey Differences from Stateless RNN:\")\n",
        "print(\"1. stateful=True in GRU layers\")\n",
        "print(\"2. batch_input_shape specified (required for stateful)\")\n",
        "print(\"3. Non-overlapping windows (shift=n_steps)\")\n",
        "print(\"4. No shuffling of data\")\n",
        "print(\"5. Manual state reset via callback\")\n",
        "print(\"6. Fixed batch size during training and inference\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stateful_training"
      },
      "source": [
        "# Train the Stateful RNN\n",
        "# Note: This demonstrates the training process, but is computationally intensive\n",
        "\n",
        "print(\"Training Stateful RNN (limited epochs for demonstration)...\")\n",
        "\n",
        "# Create the reset states callback\n",
        "reset_callback = ResetStatesCallback()\n",
        "\n",
        "# Train for fewer epochs due to computational constraints\n",
        "stateful_epochs = 2\n",
        "\n",
        "# Train the stateful model\n",
        "stateful_history = stateful_model.fit(\n",
        "    stateful_dataset,\n",
        "    epochs=stateful_epochs,\n",
        "    callbacks=[reset_callback],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Compare performance metrics\n",
        "print(f\"\\nStateful RNN Training Results:\")\n",
        "print(f\"Final Loss: {stateful_history.history['loss'][-1]:.4f}\")\n",
        "print(f\"Final Accuracy: {stateful_history.history['accuracy'][-1]:.4f}\")\n",
        "\n",
        "# Create a stateless version for inference\n",
        "# This is necessary because stateful models can only predict with the same batch size\n",
        "print(\"\\nCreating stateless version for text generation...\")\n",
        "\n",
        "stateless_model = create_char_rnn_model(\n",
        "    vocab_size=max_id,\n",
        "    rnn_units=128,\n",
        "    dropout_rate=0.0  # No dropout for inference\n",
        ")\n",
        "\n",
        "# Copy weights from stateful to stateless model\n",
        "stateless_model.set_weights(stateful_model.get_weights())\n",
        "\n",
        "print(\"Weights copied from stateful to stateless model.\")\n",
        "\n",
        "# Test text generation with stateless model\n",
        "print(\"\\nGenerating text with stateless model (copied weights):\")\n",
        "test_generation = complete_text(\n",
        "    \"The stateful \", stateless_model, tokenizer, max_id, \n",
        "    n_chars=200, temperature=1.0\n",
        ")\n",
        "print(test_generation)\n",
        "\n",
        "# Advantages and disadvantages summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STATEFUL vs STATELESS RNN COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nStateful RNN Advantages:\")\n",
        "print(\"• Can learn longer-term dependencies (beyond window size)\")\n",
        "print(\"• More coherent text generation\")\n",
        "print(\"• Better context preservation across sequences\")\n",
        "\n",
        "print(\"\\nStateful RNN Disadvantages:\")\n",
        "print(\"• More complex data preparation\")\n",
        "print(\"• Fixed batch size requirement\")\n",
        "print(\"• Manual state management needed\")\n",
        "print(\"• Cannot parallelize as effectively\")\n",
        "print(\"• Need separate model for inference\")\n",
        "\n",
        "print(\"\\nWhen to use Stateful RNNs:\")\n",
        "print(\"• When learning very long sequences (>1000 time steps)\")\n",
        "print(\"• When context preservation is crucial\")\n",
        "print(\"• For online learning scenarios\")\n",
        "print(\"• When computational resources allow for complexity\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sentiment_analysis_theory"
      },
      "source": [
        "## Part 2: Sentiment Analysis with Word-Level RNNs\n",
        "\n",
        "### Theory: From Characters to Words\n",
        "\n",
        "**Word-Level vs Character-Level:**\n",
        "\n",
        "| Aspect | Character-Level | Word-Level |\n",
        "|--------|----------------|------------|\n",
        "| **Vocabulary Size** | ~100 | ~10,000-50,000 |\n",
        "| **Sequence Length** | Long (1000+ chars) | Short (10-100 words) |\n",
        "| **Semantic Understanding** | Limited | Rich |\n",
        "| **OOV Handling** | Natural | Problematic |\n",
        "| **Training Speed** | Slower | Faster |\n",
        "\n",
        "### IMDb Dataset Theory\n",
        "\n",
        "**Dataset Structure:**\n",
        "- **50,000 movie reviews** (25K train, 25K test)\n",
        "- **Binary sentiment**: 0 (negative), 1 (positive)\n",
        "- **Preprocessing**: Lowercase, punctuation removed, frequency-based indexing\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "Given review $r = [w_1, w_2, ..., w_T]$ with words $w_i$, predict:\n",
        "$$P(\\text{sentiment} = 1 | r) = \\sigma(f(r))$$\n",
        "\n",
        "Where $f(r)$ is the RNN output and $\\sigma$ is the sigmoid function.\n",
        "\n",
        "### Subword Tokenization Theory\n",
        "\n",
        "**Problems with Word-Level Tokenization:**\n",
        "1. **Large vocabularies**: Memory and computational overhead\n",
        "2. **OOV words**: Cannot handle unseen words\n",
        "3. **Morphological variants**: \"run\", \"running\", \"ran\" treated as different\n",
        "4. **Language dependence**: Spaces don't exist in all languages\n",
        "\n",
        "**Subword Solutions:**\n",
        "\n",
        "**1. Byte Pair Encoding (BPE):**\n",
        "Iteratively merge most frequent character pairs:\n",
        "```\n",
        "Initial: [\"h\", \"e\", \"l\", \"l\", \"o\"]\n",
        "Step 1: [\"he\", \"l\", \"l\", \"o\"]  # \"h\"+\"e\" most frequent\n",
        "Step 2: [\"he\", \"ll\", \"o\"]     # \"l\"+\"l\" most frequent\n",
        "Step 3: [\"hello\"]             # \"he\"+\"ll\"+\"o\" merged\n",
        "```\n",
        "\n",
        "**2. SentencePiece:**\n",
        "- Language-independent\n",
        "- Treats spaces as regular characters\n",
        "- Unigram language model approach\n",
        "\n",
        "**3. WordPiece:**\n",
        "- Used in BERT\n",
        "- Maximizes likelihood of training data\n",
        "- Greedy longest-match-first algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imdb_data_loading"
      },
      "source": [
        "# Load and Explore IMDb Dataset\n",
        "# This demonstrates the \"hello world\" of NLP sentiment analysis\n",
        "\n",
        "print(\"Loading IMDb Movie Reviews Dataset...\")\n",
        "\n",
        "# Load the preprocessed IMDb dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
        "\n",
        "print(f\"Dataset Statistics:\")\n",
        "print(f\"Training samples: {len(X_train):,}\")\n",
        "print(f\"Test samples: {len(X_test):,}\")\n",
        "print(f\"Training labels distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test labels distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "# Analyze sequence lengths\n",
        "train_lengths = [len(review) for review in X_train]\n",
        "test_lengths = [len(review) for review in X_test]\n",
        "\n",
        "print(f\"\\nSequence Length Statistics:\")\n",
        "print(f\"Training - Mean: {np.mean(train_lengths):.1f}, Std: {np.std(train_lengths):.1f}\")\n",
        "print(f\"Training - Min: {np.min(train_lengths)}, Max: {np.max(train_lengths)}\")\n",
        "print(f\"Training - Median: {np.median(train_lengths):.1f}\")\n",
        "\n",
        "# Examine first few examples\n",
        "print(f\"\\nFirst 3 training examples (encoded):\")\n",
        "for i in range(3):\n",
        "    print(f\"Review {i+1} (length {len(X_train[i])}, label {y_train[i]}): {X_train[i][:20]}...\")\n",
        "\n",
        "# Load word index for decoding\n",
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "print(f\"\\nVocabulary size: {len(word_index):,} words\")\n",
        "\n",
        "# Create reverse word index for decoding\n",
        "# Note: indices are offset by 3 (0=padding, 1=start, 2=unknown)\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate([\"<PAD>\", \"<START>\", \"<UNK>\"]):\n",
        "    id_to_word[id_] = token\n",
        "\n",
        "# Decode a sample review\n",
        "def decode_review(encoded_review, id_to_word):\n",
        "    \"\"\"Decode an encoded review back to text.\"\"\"\n",
        "    return \" \".join([id_to_word.get(id_, \"<UNK>\") for id_ in encoded_review])\n",
        "\n",
        "# Show decoded examples\n",
        "print(f\"\\nDecoded Examples:\")\n",
        "for i in range(2):\n",
        "    decoded = decode_review(X_train[i], id_to_word)\n",
        "    label = \"Positive\" if y_train[i] == 1 else \"Negative\"\n",
        "    print(f\"\\nReview {i+1} ({label}):\")\n",
        "    print(decoded[:300] + \"...\")\n",
        "\n",
        "# Visualize sequence length distribution\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(train_lengths, bins=50, alpha=0.7, label='Training')\n",
        "plt.hist(test_lengths, bins=50, alpha=0.7, label='Test')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Review Lengths')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(train_lengths, bins=50, alpha=0.7, cumulative=True, density=True)\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Cumulative Probability')\n",
        "plt.title('Cumulative Distribution of Review Lengths')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axhline(y=0.95, color='r', linestyle='--', label='95th percentile')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Determine good sequence length for padding\n",
        "percentile_95 = np.percentile(train_lengths, 95)\n",
        "percentile_99 = np.percentile(train_lengths, 99)\n",
        "print(f\"\\n95th percentile length: {percentile_95:.0f}\")\n",
        "print(f\"99th percentile length: {percentile_99:.0f}\")\n",
        "print(f\"Recommended max length: {min(500, int(percentile_95))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing_theory"
      },
      "source": [
        "### Advanced Text Preprocessing Theory\n",
        "\n",
        "#### Variable Length Sequences\n",
        "\n",
        "**Challenge:** Neural networks require fixed-size inputs, but text has variable length.\n",
        "\n",
        "**Solutions:**\n",
        "\n",
        "**1. Padding:**\n",
        "$$\\text{padded_sequence} = \\begin{cases} \n",
        "[w_1, w_2, ..., w_T, 0, 0, ..., 0] & \\text{if } T < \\text{max_len} \\\\\n",
        "[w_1, w_2, ..., w_{\\text{max_len}}] & \\text{if } T \\geq \\text{max_len}\n",
        "\\end{cases}$$\n",
        "\n",
        "**2. Truncation:**\n",
        "- **Pre-truncation**: Keep last N tokens\n",
        "- **Post-truncation**: Keep first N tokens\n",
        "\n",
        "**3. Bucketing:**\n",
        "Group sequences by similar lengths to minimize padding.\n",
        "\n",
        "#### Masking Theory\n",
        "\n",
        "**Problem:** Model should ignore padding tokens\n",
        "\n",
        "**Masking Mechanism:**\n",
        "$$\\text{mask}[i] = \\begin{cases} \n",
        "\\text{True} & \\text{if } x[i] \\neq 0 \\\\\n",
        "\\text{False} & \\text{if } x[i] = 0\n",
        "\\end{cases}$$\n",
        "\n",
        "**Implementation in Layers:**\n",
        "- **Embedding Layer**: `mask_zero=True` creates mask automatically\n",
        "- **RNN Layers**: Use mask to skip computations for masked time steps\n",
        "- **Loss Function**: Masked tokens don't contribute to loss\n",
        "\n",
        "#### TensorFlow Text Processing\n",
        "\n",
        "**TensorFlow Operations for Text:**\n",
        "- `tf.strings.substr()`: Extract substrings\n",
        "- `tf.strings.regex_replace()`: Regular expression replacement\n",
        "- `tf.strings.split()`: Split strings into tokens\n",
        "- `tf.RaggedTensor`: Handle variable-length sequences\n",
        "- `tf.lookup.StaticVocabularyTable`: Efficient word-to-id mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advanced_preprocessing"
      },
      "source": [
        "# Advanced Text Preprocessing with TensorFlow Operations\n",
        "# This demonstrates modern preprocessing techniques from the book\n",
        "\n",
        "# Load raw text data using TensorFlow Datasets\n",
        "print(\"Loading raw IMDb data with TensorFlow Datasets...\")\n",
        "\n",
        "# Load original text data (not preprocessed integers)\n",
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "train_size = info.splits[\"train\"].num_examples\n",
        "\n",
        "print(f\"Dataset info:\")\n",
        "print(f\"Training examples: {train_size:,}\")\n",
        "print(f\"Features: {info.features}\")\n",
        "\n",
        "# Examine raw data\n",
        "print(\"\\nRaw data examples:\")\n",
        "for i, (text, label) in enumerate(datasets[\"train\"].take(2)):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Label: {label.numpy()} ({'Positive' if label.numpy() == 1 else 'Negative'})\")\n",
        "    print(f\"Text (first 200 chars): {text.numpy().decode('utf-8')[:200]}...\")\n",
        "\n",
        "def preprocess_text(X_batch, y_batch):\n",
        "    \"\"\"\n",
        "    Advanced text preprocessing using TensorFlow operations.\n",
        "    This demonstrates the preprocessing pipeline from the book.\n",
        "    \n",
        "    Args:\n",
        "        X_batch: Batch of text strings\n",
        "        y_batch: Batch of labels\n",
        "    \n",
        "    Returns:\n",
        "        Preprocessed text and labels\n",
        "    \"\"\"\n",
        "    # 1. Truncate to first 300 characters (speeds up training)\n",
        "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
        "    \n",
        "    # 2. Replace HTML line breaks with spaces\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \")\n",
        "    \n",
        "    # 3. Remove non-alphabetic characters (keep only letters and apostrophes)\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "    \n",
        "    # 4. Split into words (creates ragged tensor)\n",
        "    X_batch = tf.strings.split(X_batch)\n",
        "    \n",
        "    # 5. Convert ragged tensor to dense tensor with padding\n",
        "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\n",
        "\n",
        "# Apply preprocessing to a sample\n",
        "print(\"\\nTesting preprocessing pipeline:\")\n",
        "sample_texts = tf.constant([\n",
        "    b\"This movie was great! I loved it.<br />Highly recommended.\",\n",
        "    b\"Terrible film... Don't waste your time!!! 0/10\"\n",
        "])\n",
        "sample_labels = tf.constant([1, 0])\n",
        "\n",
        "processed_texts, processed_labels = preprocess_text(sample_texts, sample_labels)\n",
        "print(f\"Original: {sample_texts[0].numpy().decode('utf-8')}\")\n",
        "print(f\"Processed: {[word.numpy().decode('utf-8') for word in processed_texts[0] if word.numpy() != b'<pad>']}\")\n",
        "\n",
        "# Build vocabulary from training data\n",
        "print(\"\\nBuilding vocabulary from training data...\")\n",
        "\n",
        "vocabulary = Counter()\n",
        "batch_count = 0\n",
        "max_batches = 100  # Process subset for demonstration\n",
        "\n",
        "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess_text).take(max_batches):\n",
        "    for review in X_batch:\n",
        "        # Extract words from each review\n",
        "        words = [word.numpy() for word in review if word.numpy() != b\"<pad>\"]\n",
        "        vocabulary.update(words)\n",
        "    batch_count += 1\n",
        "    if batch_count % 20 == 0:\n",
        "        print(f\"Processed {batch_count} batches...\")\n",
        "\n",
        "print(f\"\\nVocabulary statistics:\")\n",
        "print(f\"Total unique words: {len(vocabulary):,}\")\n",
        "print(f\"Most common words: {vocabulary.most_common(10)}\")\n",
        "\n",
        "# Create truncated vocabulary\n",
        "vocab_size = 10000\n",
        "truncated_vocabulary = [\n",
        "    word for word, count in vocabulary.most_common(vocab_size)\n",
        "]\n",
        "\n",
        "print(f\"Truncated vocabulary size: {len(truncated_vocabulary)}\")\n",
        "\n",
        "# Create lookup table for word-to-ID mapping\n",
        "words = tf.constant(truncated_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "num_oov_buckets = 1000  # Out-of-vocabulary buckets\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
        "\n",
        "# Test lookup table\n",
        "test_words = tf.constant([b\"this\", b\"movie\", b\"was\", b\"fantaaaastic\"])\n",
        "test_ids = table.lookup(test_words)\n",
        "print(f\"\\nLookup table test:\")\n",
        "for word, word_id in zip(test_words.numpy(), test_ids.numpy()):\n",
        "    in_vocab = \"(in vocab)\" if word_id < vocab_size else \"(OOV)\"\n",
        "    print(f\"'{word.decode('utf-8')}' -> {word_id} {in_vocab}\")\n",
        "\n",
        "# Create final preprocessing function\n",
        "def encode_words(X_batch, y_batch):\n",
        "    \"\"\"Convert words to integer IDs using lookup table.\"\"\"\n",
        "    return table.lookup(X_batch), y_batch\n",
        "\n",
        "# Create final training dataset\n",
        "print(\"\\nCreating final training dataset...\")\n",
        "train_set = datasets[\"train\"].batch(32).map(preprocess_text)\n",
        "train_set = train_set.map(encode_words).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Examine final dataset\n",
        "for X_batch, y_batch in train_set.take(1):\n",
        "    print(f\"\\nFinal dataset batch shapes:\")\n",
        "    print(f\"X_batch: {X_batch.shape}\")\n",
        "    print(f\"y_batch: {y_batch.shape}\")\n",
        "    print(f\"Sample encoded review: {X_batch[0][:20].numpy()}\")\n",
        "    print(f\"Sample label: {y_batch[0].numpy()}\")\n",
        "\n",
        "# Vocabulary analysis\n",
        "print(f\"\\nFinal vocabulary configuration:\")\n",
        "print(f\"In-vocabulary words: {vocab_size:,}\")\n",
        "print(f\"Out-of-vocabulary buckets: {num_oov_buckets:,}\")\n",
        "print(f\"Total vocabulary size: {vocab_size + num_oov_buckets:,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sentiment_model_theory"
      },
      "source": [
        "### Sentiment Analysis Model Architecture Theory\n",
        "\n",
        "#### Embedding Layer Mathematics\n",
        "\n",
        "**Word Embeddings:**\n",
        "Transform sparse one-hot vectors into dense representations:\n",
        "\n",
        "$$\\mathbf{e}_w = \\mathbf{E} \\cdot \\mathbf{o}_w$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{e}_w \\in \\mathbb{R}^d$: word embedding\n",
        "- $\\mathbf{E} \\in \\mathbb{R}^{V \\times d}$: embedding matrix\n",
        "- $\\mathbf{o}_w \\in \\{0,1\\}^V$: one-hot vector\n",
        "- $V$: vocabulary size\n",
        "- $d$: embedding dimension\n",
        "\n",
        "**Efficiency:** Direct lookup instead of matrix multiplication:\n",
        "$$\\mathbf{e}_w = \\mathbf{E}[\\text{word_id}]$$\n",
        "\n",
        "#### RNN for Sequence Classification\n",
        "\n",
        "**Architecture:**\n",
        "1. **Embedding**: Words → Dense vectors\n",
        "2. **RNN Layers**: Sequential processing\n",
        "3. **Global Information**: Last hidden state or pooling\n",
        "4. **Classification**: Dense layer with sigmoid\n",
        "\n",
        "**Mathematical Flow:**\n",
        "$$\\mathbf{h}_t = \\text{GRU}(\\mathbf{e}_{w_t}, \\mathbf{h}_{t-1})$$\n",
        "$$\\text{sentiment} = \\sigma(\\mathbf{W} \\mathbf{h}_T + b)$$\n",
        "\n",
        "Where $\\mathbf{h}_T$ is the final hidden state.\n",
        "\n",
        "#### Binary Classification Loss\n",
        "\n",
        "**Binary Cross-Entropy:**\n",
        "$$L = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]$$\n",
        "\n",
        "Where:\n",
        "- $y_i \\in \\{0, 1\\}$: true label\n",
        "- $\\hat{y}_i \\in [0, 1]$: predicted probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sentiment_model"
      },
      "source": [
        "# Sentiment Analysis Model Implementation\n",
        "# This demonstrates word-level RNN for sentiment classification\n",
        "\n",
        "def create_sentiment_model(vocab_size, embedding_dim=128, rnn_units=128, \n",
        "                          max_length=None, mask_zero=True):\n",
        "    \"\"\"\n",
        "    Create a sentiment analysis model using word-level RNN.\n",
        "    \n",
        "    Args:\n",
        "        vocab_size: Size of vocabulary (including OOV buckets)\n",
        "        embedding_dim: Dimension of word embeddings\n",
        "        rnn_units: Number of units in RNN layers\n",
        "        max_length: Maximum sequence length (None for variable)\n",
        "        mask_zero: Whether to mask padding tokens\n",
        "    \n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    model = keras.models.Sequential([\n",
        "        # Embedding layer: converts word IDs to dense vectors\n",
        "        # mask_zero=True: ignore padding tokens (ID=0) in downstream layers\n",
        "        keras.layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            input_shape=[max_length],\n",
        "            mask_zero=mask_zero\n",
        "        ),\n",
        "        \n",
        "        # First GRU layer: process word sequences\n",
        "        # return_sequences=True: return all hidden states, not just last\n",
        "        keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            dropout=0.3,\n",
        "            recurrent_dropout=0.3\n",
        "        ),\n",
        "        \n",
        "        # Second GRU layer: further processing\n",
        "        # return_sequences=False: return only last hidden state\n",
        "        keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=False,  # Only final output for classification\n",
        "            dropout=0.3,\n",
        "            recurrent_dropout=0.3\n",
        "        ),\n",
        "        \n",
        "        # Classification layer: binary sentiment prediction\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create sentiment analysis model\n",
        "total_vocab_size = vocab_size + num_oov_buckets\n",
        "sentiment_model = create_sentiment_model(\n",
        "    vocab_size=total_vocab_size,\n",
        "    embedding_dim=128,\n",
        "    rnn_units=128,\n",
        "    mask_zero=True\n",
        ")\n",
        "\n",
        "print(\"Sentiment Analysis Model Architecture:\")\n",
        "sentiment_model.summary()\n",
        "\n",
        "# Compile the model\n",
        "sentiment_model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Calculate parameter breakdown\n",
        "total_params = sentiment_model.count_params()\n",
        "print(f\"\\nParameter Analysis:\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "# Detailed parameter breakdown\n",
        "embedding_params = total_vocab_size * 128  # vocab_size × embedding_dim\n",
        "gru1_params = 3 * 128 * (128 + 128 + 1)  # 3 gates × units × (input + hidden + bias)\n",
        "gru2_params = 3 * 128 * (128 + 128 + 1)  # Same structure\n",
        "dense_params = 128 + 1  # weights + bias\n",
        "\n",
        "print(f\"\\nParameter breakdown:\")\n",
        "print(f\"Embedding layer: {embedding_params:,} parameters\")\n",
        "print(f\"First GRU layer: {gru1_params:,} parameters\")\n",
        "print(f\"Second GRU layer: {gru2_params:,} parameters\")\n",
        "print(f\"Dense layer: {dense_params:,} parameters\")\n",
        "print(f\"Total calculated: {embedding_params + gru1_params + gru2_params + dense_params:,}\")\n",
        "\n",
        "# Memory analysis\n",
        "print(f\"\\nMemory Analysis:\")\n",
        "print(f\"Embedding matrix size: {total_vocab_size} × {128} = {total_vocab_size * 128 * 4 / 1024**2:.1f} MB\")\n",
        "print(f\"Each embedding vector: {128 * 4} bytes\")\n",
        "\n",
        "# Test model with sample data\n",
        "print(\"\\nTesting model with sample data...\")\n",
        "for X_batch, y_batch in train_set.take(1):\n",
        "    # Test forward pass\n",
        "    predictions = sentiment_model.predict(X_batch[:5], verbose=0)\n",
        "    print(f\"Sample predictions shape: {predictions.shape}\")\n",
        "    print(f\"Sample predictions: {predictions.flatten()[:5]}\")\n",
        "    print(f\"Corresponding labels: {y_batch[:5].numpy()}\")\n",
        "    \n",
        "    # Show input shape handling\n",
        "    print(f\"\\nInput batch shape: {X_batch.shape}\")\n",
        "    print(f\"Model input shape: {sentiment_model.input_shape}\")\n",
        "    print(f\"Model output shape: {sentiment_model.output_shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "masking_detailed_theory"
      },
      "source": [
        "### Advanced Masking Theory and Implementation\n",
        "\n",
        "#### Mathematical Foundation of Masking\n",
        "\n",
        "**Masking Tensor Creation:**\n",
        "For input sequence $X = [x_1, x_2, ..., x_T]$ where $x_i$ are word IDs:\n",
        "\n",
        "$$M[t] = \\begin{cases} \n",
        "1 & \\text{if } x_t \\neq 0 \\text{ (valid token)} \\\\\n",
        "0 & \\text{if } x_t = 0 \\text{ (padding token)}\n",
        "\\end{cases}$$\n",
        "\n",
        "**Masked RNN Computation:**\n",
        "Traditional RNN: $h_t = f(x_t, h_{t-1})$\n",
        "\n",
        "Masked RNN: $h_t = \\begin{cases}\n",
        "f(x_t, h_{t-1}) & \\text{if } M[t] = 1 \\\\\n",
        "h_{t-1} & \\text{if } M[t] = 0\n",
        "\\end{cases}$\n",
        "\n",
        "**Masked Loss Computation:**\n",
        "$$L = \\frac{\\sum_{t=1}^T M[t] \\cdot \\ell(y_t, \\hat{y}_t)}{\\sum_{t=1}^T M[t]}$$\n",
        "\n",
        "Where $\\ell$ is the element-wise loss function.\n",
        "\n",
        "#### GPU Optimization Considerations\n",
        "\n",
        "**CuDNN Implementation:**\n",
        "- **Optimized path**: No masking, default hyperparameters\n",
        "- **Fallback path**: With masking, custom implementations\n",
        "- **Performance impact**: 2-10x slower with masking on GPU\n",
        "\n",
        "**Layer Requirements for CuDNN:**\n",
        "- `activation='tanh'` (GRU) or `activation='tanh'` (LSTM)\n",
        "- `recurrent_activation='sigmoid'`\n",
        "- `recurrent_dropout=0.0`\n",
        "- `unroll=False`\n",
        "- `use_bias=True`\n",
        "- `reset_after=True` (GRU only)\n",
        "\n",
        "#### Manual Masking Implementation\n",
        "\n",
        "**Functional API Approach:**\n",
        "```python\n",
        "# Create mask manually\n",
        "mask = keras.backend.not_equal(inputs, 0)\n",
        "# Apply to RNN layers\n",
        "rnn_output = keras.layers.GRU(units)(embeddings, mask=mask)\n",
        "```\n",
        "\n",
        "**Custom Mask Propagation:**\n",
        "Essential for complex architectures mixing different layer types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "masking_implementation"
      },
      "source": [
        "# Advanced Masking Implementation and Analysis\n",
        "# This demonstrates both automatic and manual masking approaches\n",
        "\n",
        "# Create models with and without masking for comparison\n",
        "def create_model_with_masking(vocab_size, embedding_dim=128, use_masking=True):\n",
        "    \"\"\"\n",
        "    Create sentiment model with optional masking.\n",
        "    \n",
        "    Args:\n",
        "        vocab_size: Vocabulary size\n",
        "        embedding_dim: Embedding dimension\n",
        "        use_masking: Whether to use automatic masking\n",
        "    \n",
        "    Returns:\n",
        "        Keras model\n",
        "    \"\"\"\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Embedding(\n",
        "            vocab_size, \n",
        "            embedding_dim,\n",
        "            input_shape=[None],\n",
        "            mask_zero=use_masking  # Key parameter\n",
        "        ),\n",
        "        keras.layers.GRU(64, return_sequences=True),\n",
        "        keras.layers.GRU(64),\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create models for comparison\n",
        "print(\"Creating models with and without masking...\")\n",
        "model_with_mask = create_model_with_masking(total_vocab_size, use_masking=True)\n",
        "model_without_mask = create_model_with_masking(total_vocab_size, use_masking=False)\n",
        "\n",
        "print(\"\\nModel with masking:\")\n",
        "model_with_mask.summary()\n",
        "\n",
        "# Manual masking implementation using Functional API\n",
        "def create_manual_masking_model(vocab_size, embedding_dim=128):\n",
        "    \"\"\"\n",
        "    Demonstrate manual masking using Functional API.\n",
        "    This shows how to handle masking in complex architectures.\n",
        "    \"\"\"\n",
        "    # Input layer\n",
        "    inputs = keras.layers.Input(shape=[None])\n",
        "    \n",
        "    # Create mask manually\n",
        "    mask = keras.layers.Lambda(\n",
        "        lambda inputs: keras.backend.not_equal(inputs, 0)\n",
        "    )(inputs)\n",
        "    \n",
        "    # Embedding without automatic masking\n",
        "    embeddings = keras.layers.Embedding(\n",
        "        vocab_size, embedding_dim, mask_zero=False\n",
        "    )(inputs)\n",
        "    \n",
        "    # Apply manual mask to RNN layers\n",
        "    rnn1 = keras.layers.GRU(64, return_sequences=True)(embeddings, mask=mask)\n",
        "    rnn2 = keras.layers.GRU(64)(rnn1, mask=mask)\n",
        "    \n",
        "    # Output layer\n",
        "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(rnn2)\n",
        "    \n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "manual_mask_model = create_manual_masking_model(total_vocab_size)\n",
        "print(\"\\nModel with manual masking:\")\n",
        "manual_mask_model.summary()\n",
        "\n",
        "# Test masking behavior with sample data\n",
        "print(\"\\nTesting masking behavior...\")\n",
        "\n",
        "# Create test sequences with different padding\n",
        "test_sequences = np.array([\n",
        "    [1, 2, 3, 4, 5],      # No padding\n",
        "    [1, 2, 3, 0, 0],      # Padded with 2 zeros\n",
        "    [1, 0, 0, 0, 0],      # Heavily padded\n",
        "    [0, 0, 0, 0, 0]       # All padding\n",
        "])\n",
        "\n",
        "print(f\"Test sequences shape: {test_sequences.shape}\")\n",
        "print(f\"Test sequences:\")\n",
        "for i, seq in enumerate(test_sequences):\n",
        "    non_zero = np.count_nonzero(seq)\n",
        "    print(f\"  Sequence {i+1}: {seq} (non-zero: {non_zero})\")\n",
        "\n",
        "# Compare predictions with and without masking\n",
        "pred_with_mask = model_with_mask.predict(test_sequences, verbose=0)\n",
        "pred_without_mask = model_without_mask.predict(test_sequences, verbose=0)\n",
        "pred_manual_mask = manual_mask_model.predict(test_sequences, verbose=0)\n",
        "\n",
        "print(f\"\\nPrediction comparison:\")\n",
        "print(f\"{'Sequence':<10} {'With Mask':<12} {'Without Mask':<12} {'Manual Mask':<12}\")\n",
        "print(\"-\" * 50)\n",
        "for i in range(len(test_sequences)):\n",
        "    print(f\"{i+1:<10} {pred_with_mask[i][0]:<12.4f} {pred_without_mask[i][0]:<12.4f} {pred_manual_mask[i][0]:<12.4f}\")\n",
        "\n",
        "# Analyze mask propagation\n",
        "def analyze_mask_propagation():\n",
        "    \"\"\"Analyze how masks propagate through layers.\"\"\"\n",
        "    print(\"\\nMask Propagation Analysis:\")\n",
        "    \n",
        "    # Check which layers support masking\n",
        "    for i, layer in enumerate(model_with_mask.layers):\n",
        "        supports_masking = getattr(layer, 'supports_masking', False)\n",
        "        print(f\"Layer {i+1} ({layer.__class__.__name__}): supports_masking = {supports_masking}\")\n",
        "    \n",
        "    # Demonstrate mask creation\n",
        "    sample_input = tf.constant([[1, 2, 3, 0, 0], [1, 0, 0, 0, 0]])\n",
        "    \n",
        "    # Get embedding layer output and mask\n",
        "    embedding_layer = model_with_mask.layers[0]\n",
        "    embeddings = embedding_layer(sample_input)\n",
        "    \n",
        "    # Manually create mask for demonstration\n",
        "    mask = keras.backend.not_equal(sample_input, 0)\n",
        "    \n",
        "    print(f\"\\nSample input: {sample_input.numpy()}\")\n",
        "    print(f\"Generated mask: {mask.numpy()}\")\n",
        "    print(f\"Embedding shape: {embeddings.shape}\")\n",
        "    \n",
        "    return mask\n",
        "\n",
        "analyze_mask_propagation()\n",
        "\n",
        "# Performance comparison\n",
        "import time\n",
        "\n",
        "def benchmark_masking_performance():\n",
        "    \"\"\"Compare performance of masked vs unmasked models.\"\"\"\n",
        "    print(\"\\nBenchmarking masking performance...\")\n",
        "    \n",
        "    # Create larger test data\n",
        "    test_data = np.random.randint(1, 1000, size=(100, 50))\n",
        "    # Add some padding\n",
        "    test_data[:, 30:] = 0  # Pad last 20 positions\n",
        "    \n",
        "    # Benchmark with masking\n",
        "    start_time = time.time()\n",
        "    _ = model_with_mask.predict(test_data, verbose=0)\n",
        "    time_with_mask = time.time() - start_time\n",
        "    \n",
        "    # Benchmark without masking\n",
        "    start_time = time.time()\n",
        "    _ = model_without_mask.predict(test_data, verbose=0)\n",
        "    time_without_mask = time.time() - start_time\n",
        "    \n",
        "    print(f\"Time with masking: {time_with_mask:.4f} seconds\")\n",
        "    print(f\"Time without masking: {time_without_mask:.4f} seconds\")\n",
        "    print(f\"Slowdown factor: {time_with_mask / time_without_mask:.2f}x\")\n",
        "    \n",
        "    return time_with_mask, time_without_mask\n",
        "\n",
        "benchmark_masking_performance()\n",
        "\n",
        "# Best practices summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MASKING BEST PRACTICES\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n1. AUTOMATIC MASKING (Recommended):\")\n",
        "print(\"   • Use mask_zero=True in Embedding layer\")\n",
        "print(\"   • Ensure all subsequent layers support masking\")\n",
        "print(\"   • Works well for simple Sequential models\")\n",
        "\n",
        "print(\"\\n2. MANUAL MASKING:\")\n",
        "print(\"   • Use when mixing different layer types\")\n",
        "print(\"   • Required for complex architectures\")\n",
        "print(\"   • More control but more complex\")\n",
        "\n",
        "print(\"\\n3. PERFORMANCE CONSIDERATIONS:\")\n",
        "print(\"   • Masking disables CuDNN optimizations\")\n",
        "print(\"   • Consider bucketing for large performance gains\")\n",
        "print(\"   • Use masking only when necessary\")\n",
        "\n",
        "print(\"\\n4. ALTERNATIVE APPROACHES:\")\n",
        "print(\"   • Bucketing: Group sequences by length\")\n",
        "print(\"   • Dynamic batching: Variable batch sizes\")\n",
        "print(\"   • Attention mechanisms: Handle variable lengths naturally\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sentiment_training"
      },
      "source": [
        "# Train the Sentiment Analysis Model\n",
        "# This demonstrates the complete training pipeline for sentiment analysis\n",
        "\n",
        "print(\"Training Sentiment Analysis Model...\")\n",
        "\n",
        "# Prepare validation set\n",
        "val_set = datasets[\"test\"].batch(32).map(preprocess_text)\n",
        "val_set = val_set.map(encode_words).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Create callbacks for training\n",
        "sentiment_callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=3,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        'sentiment_model_best.h5',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the model (reduced epochs for demonstration)\n",
        "print(\"\\nStarting training...\")\n",
        "sentiment_epochs = 3  # Increase to 10-20 for better performance\n",
        "\n",
        "# Limit training data for demonstration\n",
        "train_subset = train_set.take(500)  # Use subset for faster training\n",
        "val_subset = val_set.take(100)\n",
        "\n",
        "history = sentiment_model.fit(\n",
        "    train_subset,\n",
        "    epochs=sentiment_epochs,\n",
        "    validation_data=val_subset,\n",
        "    callbacks=sentiment_callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "if 'lr' in history.history:\n",
        "    plt.plot(history.history['lr'], label='Learning Rate', marker='d')\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.yscale('log')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'Learning Rate\\nNot Recorded', \n",
        "             ha='center', va='center', transform=plt.gca().transAxes)\n",
        "    plt.title('Learning Rate (Not Available)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate final performance\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "print(f\"\\nFinal Training Results:\")\n",
        "print(f\"Training Accuracy: {final_train_acc:.4f}\")\n",
        "print(f\"Validation Accuracy: {final_val_acc:.4f}\")\n",
        "print(f\"Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Validation Loss: {final_val_loss:.4f}\")\n",
        "print(f\"Overfitting Gap (Acc): {final_train_acc - final_val_acc:.4f}\")\n",
        "print(f\"Overfitting Gap (Loss): {final_val_loss - final_train_loss:.4f}\")\n",
        "\n",
        "# Test the model with custom examples\n",
        "def test_sentiment_prediction(model, texts, tokenizer_table):\n",
        "    \"\"\"\n",
        "    Test sentiment prediction on custom text examples.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained sentiment model\n",
        "        texts: List of text strings\n",
        "        tokenizer_table: Lookup table for word encoding\n",
        "    \n",
        "    Returns:\n",
        "        Predictions and analysis\n",
        "    \"\"\"\n",
        "    # Preprocess texts\n",
        "    processed_texts = []\n",
        "    for text in texts:\n",
        "        # Convert to tensor and apply preprocessing\n",
        "        text_tensor = tf.constant([text.encode('utf-8')])\n",
        "        processed, _ = preprocess_text(text_tensor, tf.constant([0]))\n",
        "        processed_texts.append(processed[0])\n",
        "    \n",
        "    # Convert to tensor and encode\n",
        "    max_len = max(len(pt) for pt in processed_texts)\n",
        "    padded_texts = []\n",
        "    for pt in processed_texts:\n",
        "        # Pad to max length\n",
        "        padding_needed = max_len - len(pt)\n",
        "        if padding_needed > 0:\n",
        "            padded = tf.concat([pt, tf.constant([b\"<pad>\"] * padding_needed)], axis=0)\n",
        "        else:\n",
        "            padded = pt\n",
        "        padded_texts.append(padded)\n",
        "    \n",
        "    # Stack and encode\n",
        "    text_batch = tf.stack(padded_texts)\n",
        "    encoded_batch = tokenizer_table.lookup(text_batch)\n",
        "    \n",
        "    # Predict\n",
        "    predictions = model.predict(encoded_batch, verbose=0)\n",
        "    \n",
        "    return predictions, encoded_batch\n",
        "\n",
        "# Test with custom examples\n",
        "test_texts = [\n",
        "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
        "    \"Terrible film. Complete waste of time. Don't watch this garbage.\",\n",
        "    \"The movie was okay, nothing special but not bad either.\",\n",
        "    \"Amazing cinematography and brilliant acting. Highly recommended!\",\n",
        "    \"Boring and predictable plot. Fell asleep halfway through.\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting sentiment predictions on custom examples:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "predictions, encoded = test_sentiment_prediction(sentiment_model, test_texts, table)\n",
        "\n",
        "for i, (text, pred) in enumerate(zip(test_texts, predictions)):\n",
        "    sentiment = \"Positive\" if pred[0] > 0.5 else \"Negative\"\n",
        "    confidence = pred[0] if pred[0] > 0.5 else 1 - pred[0]\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Prediction: {sentiment} (confidence: {confidence:.3f}, raw: {pred[0]:.3f})\")\n",
        "    print(f\"Encoded length: {np.count_nonzero(encoded[i].numpy())} words\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pretrained_embeddings_theory"
      },
      "source": [
        "## Pretrained Embeddings Theory and Implementation\n",
        "\n",
        "### Theory: Transfer Learning for NLP\n",
        "\n",
        "**Core Concept:** Reuse word representations learned from large corpora\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "Instead of learning embeddings $E \\in \\mathbb{R}^{V \\times d}$ from scratch, use pretrained $E_{\\text{pretrained}}$ learned on corpus $C_{\\text{large}}$:\n",
        "\n",
        "$$E_{\\text{task}} = \\begin{cases}\n",
        "E_{\\text{pretrained}} & \\text{frozen (feature extraction)} \\\\\n",
        "\\text{finetune}(E_{\\text{pretrained}}, D_{\\text{task}}) & \\text{fine-tuning}\n",
        "\\end{cases}$$\n",
        "\n",
        "### Advantages of Pretrained Embeddings\n",
        "\n",
        "**1. Semantic Relationships:**\n",
        "Words with similar meanings cluster together:\n",
        "$$\\text{similarity}(\\mathbf{e}_{\\text{\"awesome\"}}, \\mathbf{e}_{\\text{\"amazing\"}}) > \\text{similarity}(\\mathbf{e}_{\\text{\"awesome\"}}, \\mathbf{e}_{\\text{\"terrible\"}})$$\n",
        "\n",
        "**2. Data Efficiency:**\n",
        "- Require fewer training examples\n",
        "- Faster convergence\n",
        "- Better generalization\n",
        "\n",
        "**3. Domain Transfer:**\n",
        "Even if trained on different domains (e.g., Wikipedia), semantic relationships often transfer.\n",
        "\n",
        "### TensorFlow Hub Integration\n",
        "\n",
        "**TF Hub Architecture:**\n",
        "- **Modules**: Reusable ML components\n",
        "- **Versioning**: Ensures reproducibility\n",
        "- **Caching**: Local storage for efficiency\n",
        "\n",
        "**Sentence Encoders:**\n",
        "Instead of word-level embeddings, use sentence-level:\n",
        "$$\\mathbf{s} = f([w_1, w_2, ..., w_T]) \\in \\mathbb{R}^d$$\n",
        "\n",
        "Where $f$ can be:\n",
        "- **Mean pooling**: $\\mathbf{s} = \\frac{1}{T} \\sum_{i=1}^T \\mathbf{e}_{w_i}$\n",
        "- **Weighted average**: Account for word importance\n",
        "- **Learned aggregation**: Neural network combination\n",
        "\n",
        "### NNLM (Neural Network Language Model)\n",
        "\n",
        "**Architecture:**\n",
        "1. **Input**: Sequence of word IDs\n",
        "2. **Embedding**: Word → dense vectors\n",
        "3. **Hidden layers**: Neural network processing\n",
        "4. **Output**: Next word probabilities\n",
        "\n",
        "**Training Objective:**\n",
        "$$L = -\\sum_{t=1}^T \\log P(w_t | w_1, ..., w_{t-1})$$\n",
        "\n",
        "**Google News 7B Corpus:**\n",
        "- 7 billion words\n",
        "- Rich semantic representations\n",
        "- Covers diverse topics and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pretrained_embeddings"
      },
      "source": [
        "# Pretrained Embeddings with TensorFlow Hub\n",
        "# This demonstrates using pretrained sentence encoders for sentiment analysis\n",
        "\n",
        "print(\"Implementing Pretrained Embeddings with TensorFlow Hub...\")\n",
        "\n",
        "def create_tfhub_sentiment_model(tfhub_url, trainable=False):\n",
        "    \"\"\"\n",
        "    Create sentiment analysis model using TensorFlow Hub embeddings.\n",
        "    \n",
        "    Args:\n",
        "        tfhub_url: URL of TensorFlow Hub module\n",
        "        trainable: Whether to fine-tune the embeddings\n",
        "    \n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        # TensorFlow Hub layer for sentence encoding\n",
        "        # This layer takes raw text as input and outputs sentence embeddings\n",
        "        hub.KerasLayer(\n",
        "            tfhub_url,\n",
        "            dtype=tf.string,      # Input type: text strings\n",
        "            input_shape=[],       # Scalar input (single string)\n",
        "            output_shape=[50],    # 50-dimensional embeddings\n",
        "            trainable=trainable   # Whether to fine-tune\n",
        "        ),\n",
        "        \n",
        "        # Dense layers for classification\n",
        "        keras.layers.Dense(128, activation=\"relu\"),\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# TensorFlow Hub NNLM module URL (50-dimensional embeddings)\n",
        "nnlm_url = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\"\n",
        "\n",
        "print(f\"Using TensorFlow Hub module: {nnlm_url}\")\n",
        "print(\"This module:\")\n",
        "print(\"• Takes text strings as input\")\n",
        "print(\"• Outputs 50-dimensional sentence embeddings\")\n",
        "print(\"• Trained on Google News 7B corpus\")\n",
        "print(\"• Computes mean of word embeddings with length normalization\")\n",
        "\n",
        "# Create model with frozen embeddings\n",
        "tfhub_model_frozen = create_tfhub_sentiment_model(nnlm_url, trainable=False)\n",
        "\n",
        "print(\"\\nTensorFlow Hub Model Architecture (Frozen Embeddings):\")\n",
        "tfhub_model_frozen.summary()\n",
        "\n",
        "# Create model with trainable embeddings\n",
        "tfhub_model_trainable = create_tfhub_sentiment_model(nnlm_url, trainable=True)\n",
        "\n",
        "print(\"\\nTensorFlow Hub Model Architecture (Trainable Embeddings):\")\n",
        "tfhub_model_trainable.summary()\n",
        "\n",
        "# Parameter comparison\n",
        "frozen_params = tfhub_model_frozen.count_params()\n",
        "trainable_params = tfhub_model_trainable.count_params()\n",
        "\n",
        "print(f\"\\nParameter Comparison:\")\n",
        "print(f\"Frozen embeddings: {frozen_params:,} trainable parameters\")\n",
        "print(f\"Trainable embeddings: {trainable_params:,} trainable parameters\")\n",
        "print(f\"Difference: {trainable_params - frozen_params:,} parameters\")\n",
        "\n",
        "# Prepare data for TensorFlow Hub model (raw text, not encoded)\n",
        "print(\"\\nPreparing data for TensorFlow Hub model...\")\n",
        "\n",
        "# Load raw text data for TF Hub (no preprocessing needed)\n",
        "raw_train_set = datasets[\"train\"].batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "raw_val_set = datasets[\"test\"].batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Take subset for demonstration\n",
        "train_subset_hub = raw_train_set.take(100)\n",
        "val_subset_hub = raw_val_set.take(20)\n",
        "\n",
        "# Test the TF Hub model with sample data\n",
        "print(\"\\nTesting TensorFlow Hub model...\")\n",
        "for texts, labels in raw_train_set.take(1):\n",
        "    print(f\"Sample input shape: {texts.shape}\")\n",
        "    print(f\"Sample input type: {texts.dtype}\")\n",
        "    print(f\"First text: {texts[0].numpy()[:100]}...\")\n",
        "    \n",
        "    # Test prediction\n",
        "    predictions = tfhub_model_frozen.predict(texts[:3], verbose=0)\n",
        "    print(f\"Predictions shape: {predictions.shape}\")\n",
        "    print(f\"Sample predictions: {predictions.flatten()[:3]}\")\n",
        "    \n",
        "    break\n",
        "\n",
        "# Train the TensorFlow Hub model (frozen embeddings)\n",
        "print(\"\\nTraining TensorFlow Hub model with frozen embeddings...\")\n",
        "\n",
        "hub_history_frozen = tfhub_model_frozen.fit(\n",
        "    train_subset_hub,\n",
        "    epochs=3,\n",
        "    validation_data=val_subset_hub,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the TensorFlow Hub model (trainable embeddings)\n",
        "print(\"\\nTraining TensorFlow Hub model with trainable embeddings...\")\n",
        "\n",
        "hub_history_trainable = tfhub_model_trainable.fit(\n",
        "    train_subset_hub,\n",
        "    epochs=3,\n",
        "    validation_data=val_subset_hub,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Compare performance\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "frozen_final_acc = hub_history_frozen.history['val_accuracy'][-1]\n",
        "trainable_final_acc = hub_history_trainable.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"Frozen embeddings - Final validation accuracy: {frozen_final_acc:.4f}\")\n",
        "print(f\"Trainable embeddings - Final validation accuracy: {trainable_final_acc:.4f}\")\n",
        "print(f\"Performance difference: {trainable_final_acc - frozen_final_acc:.4f}\")\n",
        "\n",
        "# Plot training comparison\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(hub_history_frozen.history['loss'], 'b-', label='Frozen - Train')\n",
        "plt.plot(hub_history_frozen.history['val_loss'], 'b--', label='Frozen - Val')\n",
        "plt.plot(hub_history_trainable.history['loss'], 'r-', label='Trainable - Train')\n",
        "plt.plot(hub_history_trainable.history['val_loss'], 'r--', label='Trainable - Val')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(hub_history_frozen.history['accuracy'], 'b-', label='Frozen - Train')\n",
        "plt.plot(hub_history_frozen.history['val_accuracy'], 'b--', label='Frozen - Val')\n",
        "plt.plot(hub_history_trainable.history['accuracy'], 'r-', label='Trainable - Train')\n",
        "plt.plot(hub_history_trainable.history['val_accuracy'], 'r--', label='Trainable - Val')\n",
        "plt.title('Training Accuracy Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Test with custom examples\n",
        "custom_texts = tf.constant([\n",
        "    \"This movie is absolutely fantastic and amazing!\",\n",
        "    \"Terrible movie, waste of time and money.\",\n",
        "    \"The film was okay, nothing special.\",\n",
        "    \"Incredible acting and stunning cinematography!\"\n",
        "])\n",
        "\n",
        "print(\"\\nTesting TensorFlow Hub models with custom examples:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "frozen_preds = tfhub_model_frozen.predict(custom_texts, verbose=0)\n",
        "trainable_preds = tfhub_model_trainable.predict(custom_texts, verbose=0)\n",
        "\n",
        "for i, text in enumerate(custom_texts.numpy()):\n",
        "    text_str = text.decode('utf-8')\n",
        "    frozen_pred = frozen_preds[i][0]\n",
        "    trainable_pred = trainable_preds[i][0]\n",
        "    \n",
        "    print(f\"\\nText: {text_str}\")\n",
        "    print(f\"Frozen model: {frozen_pred:.3f} ({'Positive' if frozen_pred > 0.5 else 'Negative'})\")\n",
        "    print(f\"Trainable model: {trainable_pred:.3f} ({'Positive' if trainable_pred > 0.5 else 'Negative'})\")\n",
        "\n",
        "# Analysis and recommendations\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PRETRAINED EMBEDDINGS ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nAdvantages of TensorFlow Hub approach:\")\n",
        "print(\"• No text preprocessing required\")\n",
        "print(\"• Sentence-level embeddings capture context\")\n",
        "print(\"• Trained on massive corpus (7B words)\")\n",
        "print(\"• Much faster training (fewer parameters to learn)\")\n",
        "print(\"• Good baseline performance with minimal effort\")\n",
        "\n",
        "print(\"\\nWhen to use frozen vs trainable embeddings:\")\n",
        "print(\"• Frozen: Small datasets, limited compute, quick prototyping\")\n",
        "print(\"• Trainable: Large datasets, domain-specific data, best performance\")\n",
        "\n",
        "print(\"\\nLimitations:\")\n",
        "print(\"• Fixed embedding dimension (50D may be limiting)\")\n",
        "print(\"• Average pooling loses word order information\")\n",
        "print(\"• Less control over text preprocessing\")\n",
        "print(\"• Dependency on external service\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "encoder_decoder_theory"
      },
      "source": [
        "## Part 3: Encoder-Decoder Networks for Neural Machine Translation\n",
        "\n",
        "### Sequence-to-Sequence Learning Theory\n",
        "\n",
        "**Problem Formulation:**\n",
        "Given input sequence $X = [x_1, x_2, ..., x_T]$, generate output sequence $Y = [y_1, y_2, ..., y_{T'}]$ where $T \\neq T'$ generally.\n",
        "\n",
        "**Examples:**\n",
        "- **Machine Translation**: English → French\n",
        "- **Summarization**: Long text → Summary\n",
        "- **Question Answering**: Question + Context → Answer\n",
        "- **Dialogue**: User input → Bot response\n",
        "\n",
        "### Encoder-Decoder Architecture\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "**Encoder:**\n",
        "$$\\mathbf{h}_t^{(e)} = f_{\\text{enc}}(x_t, \\mathbf{h}_{t-1}^{(e)})$$\n",
        "$$\\mathbf{c} = \\mathbf{h}_T^{(e)} \\text{ (context vector)}$$\n",
        "\n",
        "**Decoder:**\n",
        "$$\\mathbf{h}_t^{(d)} = f_{\\text{dec}}(y_{t-1}, \\mathbf{h}_{t-1}^{(d)}, \\mathbf{c})$$\n",
        "$$P(y_t | y_1, ..., y_{t-1}, X) = \\text{softmax}(\\mathbf{W} \\mathbf{h}_t^{(d)} + \\mathbf{b})$$\n",
        "\n",
        "**Key Properties:**\n",
        "1. **Variable length input/output**\n",
        "2. **Information bottleneck** through context vector $\\mathbf{c}$\n",
        "3. **Autoregressive generation**: $y_t$ depends on $y_1, ..., y_{t-1}$\n",
        "\n",
        "### Neural Machine Translation (NMT)\n",
        "\n",
        "**Training Process:**\n",
        "1. **Teacher Forcing**: Use ground truth previous token as input\n",
        "2. **Loss Function**: Cross-entropy over target vocabulary\n",
        "3. **Gradient Flow**: Through both encoder and decoder\n",
        "\n",
        "**Inference Process:**\n",
        "1. **Autoregressive Generation**: Use model's previous output\n",
        "2. **Beam Search**: Keep multiple hypotheses\n",
        "3. **Length Normalization**: Prevent bias toward shorter sequences\n",
        "\n",
        "### Key Challenges\n",
        "\n",
        "**1. Information Bottleneck:**\n",
        "Single context vector $\\mathbf{c}$ must encode entire source sequence.\n",
        "\n",
        "**2. Long Sequences:**\n",
        "RNN limitations become apparent with very long sequences.\n",
        "\n",
        "**3. Exposure Bias:**\n",
        "Training with teacher forcing vs. inference with model predictions.\n",
        "\n",
        "**4. Unknown Words:**\n",
        "Handling out-of-vocabulary words in both source and target.\n",
        "\n",
        "### Implementation Considerations\n",
        "\n",
        "**Input Reversal:**\n",
        "\"I drink milk\" → \"milk drink I\" (encoder input)\n",
        "- Reduces distance between corresponding source/target words\n",
        "- Empirically improves performance\n",
        "\n",
        "**Special Tokens:**\n",
        "- `<SOS>`: Start of sequence\n",
        "- `<EOS>`: End of sequence  \n",
        "- `<UNK>`: Unknown word\n",
        "- `<PAD>`: Padding\n",
        "\n",
        "**Bucketing Strategy:**\n",
        "Group sequences by similar lengths to minimize padding:\n",
        "$$\\text{Bucket}_i = \\{(x, y) : L_i \\leq |x|, |y| \\leq L_{i+1}\\}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "encoder_decoder_implementation"
      },
      "source": [
        "# Encoder-Decoder Implementation for Neural Machine Translation\n",
        "# This demonstrates the complete seq2seq architecture from the book\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "print(\"Implementing Encoder-Decoder Architecture for Neural Machine Translation...\")\n",
        "\n",
        "# Simulate a simple translation dataset (English to \"Simplified English\")\n",
        "# In practice, you would use a real parallel corpus\n",
        "def create_simple_translation_data():\n",
        "    \"\"\"\n",
        "    Create a simple translation dataset for demonstration.\n",
        "    English -> Simplified English (reversed word order)\n",
        "    \"\"\"\n",
        "    english_sentences = [\n",
        "        \"I love machine learning\",\n",
        "        \"The cat sits on the mat\",\n",
        "        \"Python is a great programming language\",\n",
        "        \"Deep learning models are powerful\",\n",
        "        \"Natural language processing is fascinating\",\n",
        "        \"Transformers changed everything in NLP\",\n",
        "        \"Attention mechanisms are very effective\",\n",
        "        \"Machine translation has improved significantly\",\n",
        "        \"Large language models are impressive\",\n",
        "        \"Artificial intelligence will change the world\"\n",
        "    ]\n",
        "    \n",
        "    # Create \"simplified\" target by reversing word order\n",
        "    simplified_sentences = []\n",
        "    for sent in english_sentences:\n",
        "        words = sent.lower().split()\n",
        "        reversed_sent = \" \".join(reversed(words))\n",
        "        simplified_sentences.append(reversed_sent)\n",
        "    \n",
        "    return english_sentences, simplified_sentences\n",
        "\n",
        "# Create translation data\n",
        "source_sentences, target_sentences = create_simple_translation_data()\n",
        "\n",
        "print(\"Sample Translation Pairs:\")\n",
        "for i in range(5):\n",
        "    print(f\"EN: {source_sentences[i]}\")\n",
        "    print(f\"Target: {target_sentences[i]}\")\n",
        "    print()\n",
        "\n",
        "# Tokenization for both source and target\n",
        "def create_tokenizers(source_texts, target_texts):\n",
        "    \"\"\"\n",
        "    Create tokenizers for source and target languages.\n",
        "    \n",
        "    Args:\n",
        "        source_texts: List of source sentences\n",
        "        target_texts: List of target sentences\n",
        "    \n",
        "    Returns:\n",
        "        source_tokenizer, target_tokenizer, vocab_sizes\n",
        "    \"\"\"\n",
        "    # Source tokenizer\n",
        "    source_tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "    source_tokenizer.fit_on_texts(source_texts)\n",
        "    \n",
        "    # Target tokenizer (include special tokens)\n",
        "    # Add special tokens to target vocabulary\n",
        "    target_texts_with_tokens = [\"<start> \" + text + \" <end>\" for text in target_texts]\n",
        "    target_tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "    target_tokenizer.fit_on_texts(target_texts_with_tokens)\n",
        "    \n",
        "    source_vocab_size = len(source_tokenizer.word_index) + 1\n",
        "    target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "    \n",
        "    return source_tokenizer, target_tokenizer, source_vocab_size, target_vocab_size\n",
        "\n",
        "source_tokenizer, target_tokenizer, source_vocab_size, target_vocab_size = create_tokenizers(\n",
        "    source_sentences, target_sentences\n",
        ")\n",
        "\n",
        "print(f\"Vocabulary Sizes:\")\n",
        "print(f\"Source (English): {source_vocab_size} words\")\n",
        "print(f\"Target (Simplified): {target_vocab_size} words\")\n",
        "\n",
        "# Sequence processing\n",
        "def prepare_sequences(source_texts, target_texts, source_tokenizer, target_tokenizer, max_length=20):\n",
        "    \"\"\"\n",
        "    Convert texts to padded sequences.\n",
        "    \n",
        "    Args:\n",
        "        source_texts: Source sentences\n",
        "        target_texts: Target sentences  \n",
        "        source_tokenizer: Fitted source tokenizer\n",
        "        target_tokenizer: Fitted target tokenizer\n",
        "        max_length: Maximum sequence length\n",
        "    \n",
        "    Returns:\n",
        "        Processed sequences for training\n",
        "    \"\"\"\n",
        "    # Encode source sequences\n",
        "    source_sequences = source_tokenizer.texts_to_sequences(source_texts)\n",
        "    source_sequences = keras.preprocessing.sequence.pad_sequences(\n",
        "        source_sequences, maxlen=max_length, padding='post'\n",
        "    )\n",
        "    \n",
        "    # Encode target sequences with special tokens\n",
        "    target_texts_with_tokens = [\"<start> \" + text + \" <end>\" for text in target_texts]\n",
        "    target_sequences = target_tokenizer.texts_to_sequences(target_texts_with_tokens)\n",
        "    target_sequences = keras.preprocessing.sequence.pad_sequences(\n",
        "        target_sequences, maxlen=max_length, padding='post'\n",
        "    )\n",
        "    \n",
        "    # For training: input is target[:-1], output is target[1:]\n",
        "    decoder_input = target_sequences[:, :-1]\n",
        "    decoder_output = target_sequences[:, 1:]\n",
        "    \n",
        "    return source_sequences, decoder_input, decoder_output\n",
        "\n",
        "encoder_input, decoder_input, decoder_output = prepare_sequences(\n",
        "    source_sentences, target_sentences, source_tokenizer, target_tokenizer\n",
        ")\n",
        "\n",
        "print(f\"\\nSequence Shapes:\")\n",
        "print(f\"Encoder input: {encoder_input.shape}\")\n",
        "print(f\"Decoder input: {decoder_input.shape}\")\n",
        "print(f\"Decoder output: {decoder_output.shape}\")\n",
        "\n",
        "# Show example sequences\n",
        "print(f\"\\nExample Sequences (first sample):\")\n",
        "print(f\"Source text: {source_sentences[0]}\")\n",
        "print(f\"Target text: {target_sentences[0]}\")\n",
        "print(f\"Encoder input: {encoder_input[0]}\")\n",
        "print(f\"Decoder input: {decoder_input[0]}\")\n",
        "print(f\"Decoder output: {decoder_output[0]}\")\n",
        "\n",
        "# Decode sequences back to text for verification\n",
        "def decode_sequence(tokenizer, sequence):\n",
        "    \"\"\"Decode integer sequence back to text.\"\"\"\n",
        "    reverse_word_map = {v: k for k, v in tokenizer.word_index.items()}\n",
        "    return ' '.join([reverse_word_map.get(i, '<UNK>') for i in sequence if i > 0])\n",
        "\n",
        "print(f\"\\nDecoded Sequences (verification):\")\n",
        "print(f\"Encoder input decoded: {decode_sequence(source_tokenizer, encoder_input[0])}\")\n",
        "print(f\"Decoder input decoded: {decode_sequence(target_tokenizer, decoder_input[0])}\")\n",
        "print(f\"Decoder output decoded: {decode_sequence(target_tokenizer, decoder_output[0])}\")\n",
        "\n",
        "# Build Encoder-Decoder Model using TensorFlow Addons\n",
        "def create_encoder_decoder_model(source_vocab_size, target_vocab_size, \n",
        "                                embedding_dim=256, units=512):\n",
        "    \"\"\"\n",
        "    Create an Encoder-Decoder model for machine translation.\n",
        "    \n",
        "    Args:\n",
        "        source_vocab_size: Size of source vocabulary\n",
        "        target_vocab_size: Size of target vocabulary\n",
        "        embedding_dim: Dimension of embeddings\n",
        "        units: Number of RNN units\n",
        "    \n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    # Input layers\n",
        "    encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32, name='encoder_inputs')\n",
        "    decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32, name='decoder_inputs')\n",
        "    sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32, name='sequence_lengths')\n",
        "    \n",
        "    # Shared embedding layer\n",
        "    # In practice, source and target might use different embeddings\n",
        "    encoder_embedding = keras.layers.Embedding(source_vocab_size, embedding_dim, name='encoder_embedding')\n",
        "    decoder_embedding = keras.layers.Embedding(target_vocab_size, embedding_dim, name='decoder_embedding')\n",
        "    \n",
        "    # Embed inputs\n",
        "    encoder_embeddings = encoder_embedding(encoder_inputs)\n",
        "    decoder_embeddings = decoder_embedding(decoder_inputs)\n",
        "    \n",
        "    # Encoder\n",
        "    encoder = keras.layers.LSTM(units, return_state=True, name='encoder')\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
        "    encoder_state = [state_h, state_c]\n",
        "    \n",
        "    # Decoder using TensorFlow Addons\n",
        "    # TrainingSampler: uses ground truth at each step (teacher forcing)\n",
        "    sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "    \n",
        "    # LSTM cell for decoder\n",
        "    decoder_cell = keras.layers.LSTMCell(units)\n",
        "    \n",
        "    # Output projection layer\n",
        "    output_layer = keras.layers.Dense(target_vocab_size, name='output_projection')\n",
        "    \n",
        "    # Basic decoder\n",
        "    decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
        "        decoder_cell, sampler, output_layer=output_layer\n",
        "    )\n",
        "    \n",
        "    # Decoder forward pass\n",
        "    final_outputs, final_state, final_sequence_lengths = decoder(\n",
        "        decoder_embeddings, \n",
        "        initial_state=encoder_state,\n",
        "        sequence_length=sequence_lengths\n",
        "    )\n",
        "    \n",
        "    # Get logits (before softmax)\n",
        "    logits = final_outputs.rnn_output\n",
        "    \n",
        "    # Create model\n",
        "    model = keras.Model(\n",
        "        inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
        "        outputs=logits,\n",
        "        name='encoder_decoder'\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "seq2seq_model = create_encoder_decoder_model(\n",
        "    source_vocab_size, target_vocab_size, \n",
        "    embedding_dim=128, units=256\n",
        ")\n",
        "\n",
        "print(\"\\nEncoder-Decoder Model Architecture:\")\n",
        "seq2seq_model.summary()\n",
        "\n",
        "# Compile model\n",
        "seq2seq_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Prepare training data\n",
        "# Sequence lengths for each sample (needed by TensorFlow Addons)\n",
        "sequence_lengths = np.array([len([w for w in seq if w > 0]) for seq in decoder_input])\n",
        "\n",
        "print(f\"\\nTraining Data Preparation:\")\n",
        "print(f\"Encoder input shape: {encoder_input.shape}\")\n",
        "print(f\"Decoder input shape: {decoder_input.shape}\")\n",
        "print(f\"Decoder output shape: {decoder_output.shape}\")\n",
        "print(f\"Sequence lengths shape: {sequence_lengths.shape}\")\n",
        "print(f\"Sample sequence lengths: {sequence_lengths[:5]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beam_search_theory"
      },
      "source": [
        "### Beam Search Theory and Implementation\n",
        "\n",
        "#### The Problem with Greedy Decoding\n",
        "\n",
        "**Greedy Decoding:**\n",
        "$$y_t = \\arg\\max_{y} P(y | y_1, ..., y_{t-1}, X)$$\n",
        "\n",
        "**Problem:** Locally optimal choices may lead to globally suboptimal sequences.\n",
        "\n",
        "**Example:**\n",
        "- Step 1: \"How\" (75%), \"What\" (20%), \"When\" (5%)\n",
        "- Step 2 after \"How\": \"will\" (40%), \"are\" (35%), \"do\" (25%)\n",
        "- Step 3 after \"How are\": \"you\" (90%)\n",
        "\n",
        "Greedy might choose \"How will...\" but \"How are you?\" is better overall.\n",
        "\n",
        "#### Beam Search Algorithm\n",
        "\n",
        "**Concept:** Maintain top-$k$ partial sequences at each step.\n",
        "\n",
        "**Algorithm:**\n",
        "1. **Initialize**: $\\text{beams} = [\\{\\langle \\text{SOS} \\rangle\\}]$\n",
        "2. **For each time step**:\n",
        "   - **Expand**: Generate candidates for each beam\n",
        "   - **Score**: Compute sequence probabilities\n",
        "   - **Prune**: Keep top-$k$ sequences\n",
        "3. **Terminate**: When all beams end with `<EOS>`\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "At step $t$, for beam $b$ with sequence $y_1^{(b)}, ..., y_{t-1}^{(b)}$:\n",
        "\n",
        "$$\\text{Score}(y_1^{(b)}, ..., y_t^{(b)}) = \\sum_{i=1}^t \\log P(y_i^{(b)} | y_1^{(b)}, ..., y_{i-1}^{(b)}, X)$$\n",
        "\n",
        "**Length Normalization:**\n",
        "To prevent bias toward shorter sequences:\n",
        "$$\\text{Score}_{\\text{norm}} = \\frac{\\text{Score}}{|Y|^\\alpha}$$\n",
        "\n",
        "Where $\\alpha \\in [0, 1]$ controls the strength of normalization.\n",
        "\n",
        "#### Beam Search with TensorFlow Addons\n",
        "\n",
        "**Components:**\n",
        "1. **BeamSearchDecoder**: Main search algorithm\n",
        "2. **tile_batch**: Replicate encoder states for multiple beams\n",
        "3. **start_tokens** and **end_token**: Special tokens for generation\n",
        "\n",
        "**Computational Complexity:**\n",
        "- **Time**: $O(T \\times k \\times V)$ where $T$ = sequence length, $k$ = beam width, $V$ = vocabulary size\n",
        "- **Space**: $O(k \\times T)$ for storing beam sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beam_search_implementation"
      },
      "source": [
        "# Beam Search Implementation with TensorFlow Addons\n",
        "# This demonstrates advanced decoding strategies for sequence generation\n",
        "\n",
        "def create_inference_model(trained_model, source_vocab_size, target_vocab_size, \n",
        "                          embedding_dim=128, units=256, beam_width=3):\n",
        "    \"\"\"\n",
        "    Create an inference model with beam search capability.\n",
        "    \n",
        "    Args:\n",
        "        trained_model: Trained encoder-decoder model\n",
        "        source_vocab_size: Source vocabulary size\n",
        "        target_vocab_size: Target vocabulary size\n",
        "        embedding_dim: Embedding dimension\n",
        "        units: RNN units\n",
        "        beam_width: Width of beam search\n",
        "    \n",
        "    Returns:\n",
        "        Inference model with beam search\n",
        "    \"\"\"\n",
        "    # Input layers\n",
        "    encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "    \n",
        "    # Get start and end token IDs\n",
        "    start_token_id = target_tokenizer.word_index['<start>']\n",
        "    end_token_id = target_tokenizer.word_index['<end>']\n",
        "    \n",
        "    # Encoder (reuse from training model)\n",
        "    encoder_embedding = keras.layers.Embedding(source_vocab_size, embedding_dim)\n",
        "    encoder_embeddings = encoder_embedding(encoder_inputs)\n",
        "    \n",
        "    encoder_lstm = keras.layers.LSTM(units, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embeddings)\n",
        "    encoder_state = [state_h, state_c]\n",
        "    \n",
        "    # Beam search decoder\n",
        "    decoder_embedding = keras.layers.Embedding(target_vocab_size, embedding_dim)\n",
        "    decoder_cell = keras.layers.LSTMCell(units)\n",
        "    output_layer = keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    # Create beam search decoder\n",
        "    beam_decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(\n",
        "        cell=decoder_cell,\n",
        "        beam_width=beam_width,\n",
        "        output_layer=output_layer,\n",
        "        length_penalty_weight=0.6  # Length normalization\n",
        "    )\n",
        "    \n",
        "    # Tile encoder states for beam search\n",
        "    # Each beam needs its own copy of encoder state\n",
        "    decoder_initial_state = tfa.seq2seq.beam_search_decoder.tile_batch(\n",
        "        encoder_state, multiplier=beam_width\n",
        "    )\n",
        "    \n",
        "    # Start tokens for all beams\n",
        "    batch_size = tf.shape(encoder_inputs)[0]\n",
        "    start_tokens = tf.fill([batch_size], start_token_id)\n",
        "    \n",
        "    # Maximum decoding length\n",
        "    max_length = 20\n",
        "    \n",
        "    # Beam search inference\n",
        "    outputs, _, _ = beam_decoder(\n",
        "        decoder_embedding,\n",
        "        start_tokens=start_tokens,\n",
        "        end_token=end_token_id,\n",
        "        initial_state=decoder_initial_state,\n",
        "        maximum_iterations=max_length\n",
        "    )\n",
        "    \n",
        "    # Create inference model\n",
        "    inference_model = keras.Model(\n",
        "        inputs=encoder_inputs,\n",
        "        outputs=outputs.predicted_ids,  # Shape: [batch, beam_width, max_length]\n",
        "        name='beam_search_inference'\n",
        "    )\n",
        "    \n",
        "    return inference_model, start_token_id, end_token_id\n",
        "\n",
        "# Demonstrate beam search concepts with a simple implementation\n",
        "class SimpleBeamSearch:\n",
        "    \"\"\"\n",
        "    Educational implementation of beam search for understanding.\n",
        "    Not optimized for performance - for demonstration only.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, tokenizer, beam_width=3, max_length=20):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.beam_width = beam_width\n",
        "        self.max_length = max_length\n",
        "        self.start_token = tokenizer.word_index.get('<start>', 1)\n",
        "        self.end_token = tokenizer.word_index.get('<end>', 2)\n",
        "    \n",
        "    def search(self, encoder_input):\n",
        "        \"\"\"\n",
        "        Perform beam search for a single input sequence.\n",
        "        \n",
        "        Args:\n",
        "            encoder_input: Encoded source sequence\n",
        "        \n",
        "        Returns:\n",
        "            List of (sequence, score) tuples\n",
        "        \"\"\"\n",
        "        # Initialize beams with start token\n",
        "        beams = [([self.start_token], 0.0)]  # (sequence, log_probability)\n",
        "        \n",
        "        for step in range(self.max_length):\n",
        "            candidates = []\n",
        "            \n",
        "            for sequence, score in beams:\n",
        "                if sequence[-1] == self.end_token:\n",
        "                    # Beam already finished\n",
        "                    candidates.append((sequence, score))\n",
        "                    continue\n",
        "                \n",
        "                # Get next token probabilities\n",
        "                # Note: This is a simplified version - real implementation\n",
        "                # would use the actual model prediction\n",
        "                next_probs = self._get_next_probabilities(encoder_input, sequence)\n",
        "                \n",
        "                # Add top candidates\n",
        "                for token_id, prob in next_probs:\n",
        "                    new_sequence = sequence + [token_id]\n",
        "                    new_score = score + np.log(prob + 1e-8)  # Add small epsilon\n",
        "                    candidates.append((new_sequence, new_score))\n",
        "            \n",
        "            # Keep top beam_width candidates\n",
        "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "            beams = candidates[:self.beam_width]\n",
        "            \n",
        "            # Check if all beams are finished\n",
        "            if all(seq[-1] == self.end_token for seq, _ in beams):\n",
        "                break\n",
        "        \n",
        "        return beams\n",
        "    \n",
        "    def _get_next_probabilities(self, encoder_input, current_sequence):\n",
        "        \"\"\"\n",
        "        Simplified probability calculation for demonstration.\n",
        "        In practice, this would use the actual model.\n",
        "        \"\"\"\n",
        "        # Simulate next token probabilities\n",
        "        vocab_size = len(self.tokenizer.word_index) + 1\n",
        "        \n",
        "        # Create mock probabilities (in real implementation, use model.predict)\n",
        "        probs = np.random.dirichlet(np.ones(vocab_size))\n",
        "        \n",
        "        # Return top 5 candidates\n",
        "        top_indices = np.argsort(probs)[-5:]\n",
        "        return [(idx, probs[idx]) for idx in reversed(top_indices)]\n",
        "\n",
        "# Demonstrate beam search concepts\n",
        "print(\"Beam Search Demonstration:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create simple beam search instance\n",
        "beam_searcher = SimpleBeamSearch(\n",
        "    model=None,  # Simplified for demonstration\n",
        "    tokenizer=target_tokenizer,\n",
        "    beam_width=3,\n",
        "    max_length=10\n",
        ")\n",
        "\n",
        "print(f\"Beam Search Configuration:\")\n",
        "print(f\"Beam width: {beam_searcher.beam_width}\")\n",
        "print(f\"Max length: {beam_searcher.max_length}\")\n",
        "print(f\"Start token ID: {beam_searcher.start_token}\")\n",
        "print(f\"End token ID: {beam_searcher.end_token}\")\n",
        "\n",
        "# Simulate beam search process\n",
        "print(f\"\\nSimulated Beam Search Process:\")\n",
        "fake_encoder_input = np.array([[1, 2, 3, 4, 0]])  # Mock input\n",
        "results = beam_searcher.search(fake_encoder_input)\n",
        "\n",
        "print(f\"\\nBeam Search Results:\")\n",
        "for i, (sequence, score) in enumerate(results):\n",
        "    decoded = decode_sequence(target_tokenizer, sequence)\n",
        "    print(f\"Beam {i+1}: {decoded} (score: {score:.3f})\")\n",
        "\n",
        "# Compare different beam widths\n",
        "print(f\"\\nBeam Width Comparison:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "beam_widths = [1, 3, 5, 10]\n",
        "for width in beam_widths:\n",
        "    searcher = SimpleBeamSearch(\n",
        "        model=None,\n",
        "        tokenizer=target_tokenizer,\n",
        "        beam_width=width,\n",
        "        max_length=8\n",
        "    )\n",
        "    \n",
        "    # Computational complexity analysis\n",
        "    vocab_size = len(target_tokenizer.word_index) + 1\n",
        "    max_expansions = width * vocab_size  # Expansions per step\n",
        "    total_computations = max_expansions * searcher.max_length\n",
        "    \n",
        "    print(f\"Beam width {width:2d}: ~{total_computations:,} computations\")\n",
        "\n",
        "# Advantages and disadvantages analysis\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"BEAM SEARCH ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nAdvantages:\")\n",
        "print(f\"• Explores multiple hypotheses simultaneously\")\n",
        "print(f\"• Often finds better solutions than greedy search\")\n",
        "print(f\"• Configurable beam width for quality/speed trade-off\")\n",
        "print(f\"• Length normalization prevents short sequence bias\")\n",
        "\n",
        "print(f\"\\nDisadvantages:\")\n",
        "print(f\"• Increased computational cost (k times more expensive)\")\n",
        "print(f\"• Still not guaranteed to find global optimum\")\n",
        "print(f\"• Memory requirements scale with beam width\")\n",
        "print(f\"• May produce repetitive or generic outputs\")\n",
        "\n",
        "print(f\"\\nHyperparameter Guidelines:\")\n",
        "print(f\"• Beam width 1: Greedy search (fastest)\")\n",
        "print(f\"• Beam width 3-5: Good balance for most tasks\")\n",
        "print(f\"• Beam width 10+: Diminishing returns, slower\")\n",
        "print(f\"• Length penalty 0.6-1.0: Prevents very short outputs\")\n",
        "\n",
        "print(f\"\\nAlternative Decoding Strategies:\")\n",
        "print(f\"• Top-k sampling: Sample from top-k most likely tokens\")\n",
        "print(f\"• Nucleus sampling: Sample from top-p probability mass\")\n",
        "print(f\"• Temperature scaling: Control randomness in sampling\")\n",
        "print(f\"• Diverse beam search: Encourage diversity between beams\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attention_mechanisms_theory"
      },
      "source": [
        "## Part 4: Attention Mechanisms - The Game Changer\n",
        "\n",
        "### The Information Bottleneck Problem\n",
        "\n",
        "**Traditional Encoder-Decoder Limitation:**\n",
        "Single context vector $\\mathbf{c}$ must encode entire source sequence:\n",
        "\n",
        "$$\\mathbf{c} = \\mathbf{h}_T^{(\\text{encoder})}$$\n",
        "\n",
        "**Problems:**\n",
        "1. **Information Loss**: Long sequences compressed into fixed-size vector\n",
        "2. **Vanishing Gradients**: Distant source words have weak influence\n",
        "3. **Performance Degradation**: Quality drops for sequences >30 words\n",
        "\n",
        "### Attention Mechanism Solution\n",
        "\n",
        "**Core Idea:** Allow decoder to selectively focus on different parts of input.\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "Instead of fixed context $\\mathbf{c}$, use dynamic context $\\mathbf{c}_t$ at each decoder step:\n",
        "\n",
        "$$\\mathbf{c}_t = \\sum_{i=1}^T \\alpha_{t,i} \\mathbf{h}_i^{(\\text{encoder})}$$\n",
        "\n",
        "Where $\\alpha_{t,i}$ is the attention weight for encoder state $i$ at decoder step $t$.\n",
        "\n",
        "### Bahdanau Attention (2014)\n",
        "\n",
        "**First successful attention mechanism:**\n",
        "\n",
        "**1. Energy Calculation:**\n",
        "$$e_{t,i} = v^T \\tanh(W_1 \\mathbf{h}_i^{(e)} + W_2 \\mathbf{s}_{t-1}^{(d)} + \\mathbf{b})$$\n",
        "\n",
        "**2. Attention Weights:**\n",
        "$$\\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{j=1}^T \\exp(e_{t,j})}$$\n",
        "\n",
        "**3. Context Vector:**\n",
        "$$\\mathbf{c}_t = \\sum_{i=1}^T \\alpha_{t,i} \\mathbf{h}_i^{(e)}$$\n",
        "\n",
        "**4. Decoder Update:**\n",
        "$$\\mathbf{s}_t^{(d)} = f(\\mathbf{s}_{t-1}^{(d)}, y_{t-1}, \\mathbf{c}_t)$$\n",
        "\n",
        "### Luong Attention (2015)\n",
        "\n",
        "**Simplified and more efficient:**\n",
        "\n",
        "**Three Variants:**\n",
        "\n",
        "**1. Dot Product:**\n",
        "$$\\text{score}(\\mathbf{h}_t^{(d)}, \\mathbf{h}_i^{(e)}) = \\mathbf{h}_t^{(d)^T} \\mathbf{h}_i^{(e)}$$\n",
        "\n",
        "**2. General:**\n",
        "$$\\text{score}(\\mathbf{h}_t^{(d)}, \\mathbf{h}_i^{(e)}) = \\mathbf{h}_t^{(d)^T} W \\mathbf{h}_i^{(e)}$$\n",
        "\n",
        "**3. Concatenation:**\n",
        "$$\\text{score}(\\mathbf{h}_t^{(d)}, \\mathbf{h}_i^{(e)}) = v^T \\tanh(W[\\mathbf{h}_t^{(d)}; \\mathbf{h}_i^{(e)}])$$\n",
        "\n",
        "**Key Differences from Bahdanau:**\n",
        "- Uses current decoder state $\\mathbf{h}_t^{(d)}$ instead of previous\n",
        "- Simpler computation (especially dot product)\n",
        "- Better empirical performance\n",
        "\n",
        "### Self-Attention\n",
        "\n",
        "**Revolutionary Concept:** Sequence attends to itself\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Where:\n",
        "- $Q$ (Queries): What we're looking for\n",
        "- $K$ (Keys): What we're looking in\n",
        "- $V$ (Values): What we retrieve\n",
        "- For self-attention: $Q = K = V$ (same sequence)\n",
        "\n",
        "### Computational Complexity\n",
        "\n",
        "**RNN Encoder-Decoder:** $O(T \\cdot d^2)$ sequential\n",
        "**With Attention:** $O(T^2 \\cdot d)$ parallelizable\n",
        "\n",
        "For long sequences ($T > d$), attention is more expensive but parallelizable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "attention_implementation"
      },
      "source": [
        "# Attention Mechanisms Implementation\n",
        "# This demonstrates various attention mechanisms from the book\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "print(\"Implementing Attention Mechanisms...\")\n",
        "\n",
        "class BahdanauAttention(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Bahdanau (Additive) Attention mechanism.\n",
        "    Original attention mechanism from 2014 paper.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        # W1 for encoder hidden states\n",
        "        self.W1 = self.add_weight(\n",
        "            name='W1',\n",
        "            shape=(input_shape[0][-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        \n",
        "        # W2 for decoder hidden state\n",
        "        self.W2 = self.add_weight(\n",
        "            name='W2', \n",
        "            shape=(input_shape[1][-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        \n",
        "        # V for final projection\n",
        "        self.V = self.add_weight(\n",
        "            name='V',\n",
        "            shape=(self.units, 1),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        \n",
        "        super().build(input_shape)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: [encoder_outputs, decoder_hidden_state]\n",
        "            encoder_outputs: [batch_size, seq_len, hidden_size]\n",
        "            decoder_hidden_state: [batch_size, hidden_size]\n",
        "        \n",
        "        Returns:\n",
        "            context_vector: [batch_size, hidden_size]\n",
        "            attention_weights: [batch_size, seq_len]\n",
        "        \"\"\"\n",
        "        encoder_outputs, decoder_hidden = inputs\n",
        "        \n",
        "        # Expand decoder hidden to match encoder sequence length\n",
        "        # [batch_size, hidden_size] -> [batch_size, seq_len, hidden_size]\n",
        "        decoder_hidden_expanded = tf.expand_dims(decoder_hidden, 1)\n",
        "        seq_len = tf.shape(encoder_outputs)[1]\n",
        "        decoder_hidden_repeated = tf.tile(decoder_hidden_expanded, [1, seq_len, 1])\n",
        "        \n",
        "        # Calculate energy: e_ij = v^T * tanh(W1*h_i + W2*s_j)\n",
        "        # Shape: [batch_size, seq_len, units]\n",
        "        energy = tf.nn.tanh(\n",
        "            tf.matmul(encoder_outputs, self.W1) + \n",
        "            tf.matmul(decoder_hidden_repeated, self.W2)\n",
        "        )\n",
        "        \n",
        "        # Project to scalar: [batch_size, seq_len, 1]\n",
        "        energy = tf.matmul(energy, self.V)\n",
        "        \n",
        "        # Remove last dimension: [batch_size, seq_len]\n",
        "        energy = tf.squeeze(energy, axis=-1)\n",
        "        \n",
        "        # Calculate attention weights\n",
        "        attention_weights = tf.nn.softmax(energy, axis=1)\n",
        "        \n",
        "        # Calculate context vector\n",
        "        # [batch_size, seq_len, 1] * [batch_size, seq_len, hidden_size]\n",
        "        attention_weights_expanded = tf.expand_dims(attention_weights, 2)\n",
        "        context_vector = tf.reduce_sum(\n",
        "            attention_weights_expanded * encoder_outputs, axis=1\n",
        "        )\n",
        "        \n",
        "        return context_vector, attention_weights\n",
        "\n",
        "\n",
        "class LuongAttention(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Luong (Multiplicative) Attention mechanism.\n",
        "    More efficient than Bahdanau attention.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, units, score_type='general', **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.score_type = score_type  # 'dot', 'general', or 'concat'\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        if self.score_type == 'general':\n",
        "            # Weight matrix for general scoring\n",
        "            self.W = self.add_weight(\n",
        "                name='attention_W',\n",
        "                shape=(input_shape[1][-1], input_shape[0][-1]),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True\n",
        "            )\n",
        "        elif self.score_type == 'concat':\n",
        "            # For concatenation scoring (similar to Bahdanau)\n",
        "            self.W_concat = self.add_weight(\n",
        "                name='W_concat',\n",
        "                shape=(input_shape[0][-1] + input_shape[1][-1], self.units),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True\n",
        "            )\n",
        "            self.v = self.add_weight(\n",
        "                name='v_concat',\n",
        "                shape=(self.units, 1),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True\n",
        "            )\n",
        "        \n",
        "        super().build(input_shape)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: [encoder_outputs, decoder_hidden_state]\n",
        "        \n",
        "        Returns:\n",
        "            context_vector, attention_weights\n",
        "        \"\"\"\n",
        "        encoder_outputs, decoder_hidden = inputs\n",
        "        \n",
        "        if self.score_type == 'dot':\n",
        "            # Dot product attention: h_t^T * h_s\n",
        "            # [batch_size, 1, hidden_size] * [batch_size, hidden_size, seq_len]\n",
        "            decoder_expanded = tf.expand_dims(decoder_hidden, 1)\n",
        "            scores = tf.matmul(decoder_expanded, encoder_outputs, transpose_b=True)\n",
        "            scores = tf.squeeze(scores, 1)  # [batch_size, seq_len]\n",
        "            \n",
        "        elif self.score_type == 'general':\n",
        "            # General attention: h_t^T * W * h_s\n",
        "            decoder_projected = tf.matmul(decoder_hidden, self.W)\n",
        "            decoder_expanded = tf.expand_dims(decoder_projected, 1)\n",
        "            scores = tf.matmul(decoder_expanded, encoder_outputs, transpose_b=True)\n",
        "            scores = tf.squeeze(scores, 1)\n",
        "            \n",
        "        elif self.score_type == 'concat':\n",
        "            # Concatenation attention (like Bahdanau)\n",
        "            seq_len = tf.shape(encoder_outputs)[1]\n",
        "            decoder_repeated = tf.tile(\n",
        "                tf.expand_dims(decoder_hidden, 1), [1, seq_len, 1]\n",
        "            )\n",
        "            \n",
        "            # Concatenate decoder and encoder states\n",
        "            concat_states = tf.concat([decoder_repeated, encoder_outputs], axis=-1)\n",
        "            \n",
        "            # Apply transformations\n",
        "            hidden = tf.nn.tanh(tf.matmul(concat_states, self.W_concat))\n",
        "            scores = tf.matmul(hidden, self.v)\n",
        "            scores = tf.squeeze(scores, -1)\n",
        "        \n",
        "        # Calculate attention weights\n",
        "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
        "        \n",
        "        # Calculate context vector\n",
        "        attention_expanded = tf.expand_dims(attention_weights, 2)\n",
        "        context_vector = tf.reduce_sum(\n",
        "            attention_expanded * encoder_outputs, axis=1\n",
        "        )\n",
        "        \n",
        "        return context_vector, attention_weights\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention from 'Attention Is All You Need'.\n",
        "    Foundation of the Transformer architecture.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "    \n",
        "    def call(self, inputs, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: [queries, keys, values]\n",
        "            queries: [batch_size, seq_len_q, d_k]\n",
        "            keys: [batch_size, seq_len_k, d_k] \n",
        "            values: [batch_size, seq_len_v, d_v]\n",
        "            mask: Optional mask for attention weights\n",
        "        \n",
        "        Returns:\n",
        "            output: [batch_size, seq_len_q, d_v]\n",
        "            attention_weights: [batch_size, seq_len_q, seq_len_k]\n",
        "        \"\"\"\n",
        "        queries, keys, values = inputs\n",
        "        \n",
        "        # Get dimension for scaling\n",
        "        d_k = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "        \n",
        "        # Calculate attention scores: Q * K^T / sqrt(d_k)\n",
        "        scores = tf.matmul(queries, keys, transpose_b=True)\n",
        "        scores = scores / tf.math.sqrt(d_k)\n",
        "        \n",
        "        # Apply mask if provided (for causal attention)\n",
        "        if mask is not None:\n",
        "            scores += (mask * -1e9)\n",
        "        \n",
        "        # Calculate attention weights\n",
        "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        output = tf.matmul(attention_weights, values)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "# Test attention mechanisms\n",
        "print(\"Testing Attention Mechanisms...\")\n",
        "\n",
        "# Create sample data\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "hidden_size = 64\n",
        "\n",
        "# Sample encoder outputs (source sequence representations)\n",
        "encoder_outputs = tf.random.normal([batch_size, seq_len, hidden_size])\n",
        "# Sample decoder hidden state (current decoder state)\n",
        "decoder_hidden = tf.random.normal([batch_size, hidden_size])\n",
        "\n",
        "print(f\"Test Data Shapes:\")\n",
        "print(f\"Encoder outputs: {encoder_outputs.shape}\")\n",
        "print(f\"Decoder hidden: {decoder_hidden.shape}\")\n",
        "\n",
        "# Test Bahdanau Attention\n",
        "print(\"\\nTesting Bahdanau Attention:\")\n",
        "bahdanau_attention = BahdanauAttention(units=32)\n",
        "context_b, weights_b = bahdanau_attention([encoder_outputs, decoder_hidden])\n",
        "print(f\"Context vector shape: {context_b.shape}\")\n",
        "print(f\"Attention weights shape: {weights_b.shape}\")\n",
        "print(f\"Attention weights sum: {tf.reduce_sum(weights_b, axis=1).numpy()}\")\n",
        "print(f\"Sample attention weights: {weights_b[0].numpy()}\")\n",
        "\n",
        "# Test Luong Attention variants\n",
        "print(\"\\nTesting Luong Attention Variants:\")\n",
        "\n",
        "for score_type in ['dot', 'general', 'concat']:\n",
        "    print(f\"\\n{score_type.upper()} Product Attention:\")\n",
        "    luong_attention = LuongAttention(units=32, score_type=score_type)\n",
        "    context_l, weights_l = luong_attention([encoder_outputs, decoder_hidden])\n",
        "    print(f\"Context vector shape: {context_l.shape}\")\n",
        "    print(f\"Attention weights shape: {weights_l.shape}\")\n",
        "    print(f\"Sample attention weights: {weights_l[0].numpy()}\")\n",
        "\n",
        "# Test Scaled Dot-Product Attention\n",
        "print(\"\\nTesting Scaled Dot-Product Attention:\")\n",
        "scaled_attention = ScaledDotProductAttention()\n",
        "\n",
        "# For self-attention, Q=K=V (same sequence)\n",
        "queries = keys = values = encoder_outputs\n",
        "output_s, weights_s = scaled_attention([queries, keys, values])\n",
        "print(f\"Output shape: {output_s.shape}\")\n",
        "print(f\"Attention weights shape: {weights_s.shape}\")\n",
        "print(f\"Sample attention matrix:\")\n",
        "print(weights_s[0].numpy())\n",
        "\n",
        "# Visualize attention patterns\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(15, 4))\n",
        "\n",
        "# Plot Bahdanau attention weights\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.bar(range(seq_len), weights_b[0].numpy())\n",
        "plt.title('Bahdanau Attention Weights')\n",
        "plt.xlabel('Source Position')\n",
        "plt.ylabel('Attention Weight')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot Luong attention weights\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.bar(range(seq_len), weights_l[0].numpy())\n",
        "plt.title('Luong Attention Weights')\n",
        "plt.xlabel('Source Position')\n",
        "plt.ylabel('Attention Weight')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot self-attention matrix\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.heatmap(weights_s[0].numpy(), annot=True, fmt='.3f', cmap='Blues')\n",
        "plt.title('Self-Attention Matrix')\n",
        "plt.xlabel('Key Position')\n",
        "plt.ylabel('Query Position')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Performance comparison\n",
        "import time\n",
        "\n",
        "def benchmark_attention(attention_layer, inputs, iterations=100):\n",
        "    \"\"\"Benchmark attention mechanism performance.\"\"\"\n",
        "    # Warm up\n",
        "    _ = attention_layer(inputs)\n",
        "    \n",
        "    # Benchmark\n",
        "    start_time = time.time()\n",
        "    for _ in range(iterations):\n",
        "        _ = attention_layer(inputs)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    return (end_time - start_time) / iterations\n",
        "\n",
        "print(\"\\nPerformance Benchmark (average time per forward pass):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Benchmark encoder-decoder attention\n",
        "bahdanau_time = benchmark_attention(bahdanau_attention, [encoder_outputs, decoder_hidden])\n",
        "luong_time = benchmark_attention(\n",
        "    LuongAttention(32, 'dot'), [encoder_outputs, decoder_hidden]\n",
        ")\n",
        "\n",
        "print(f\"Bahdanau Attention: {bahdanau_time*1000:.3f} ms\")\n",
        "print(f\"Luong Attention (dot): {luong_time*1000:.3f} ms\")\n",
        "print(f\"Speedup: {bahdanau_time/luong_time:.2f}x\")\n",
        "\n",
        "# Benchmark self-attention\n",
        "self_attention_time = benchmark_attention(\n",
        "    scaled_attention, [queries, keys, values]\n",
        ")\n",
        "print(f\"Self-Attention: {self_attention_time*1000:.3f} ms\")\n",
        "\n",
        "# Complexity analysis\n",
        "print(f\"\\nComputational Complexity Analysis:\")\n",
        "print(f\"Sequence length: {seq_len}\")\n",
        "print(f\"Hidden dimension: {hidden_size}\")\n",
        "print(f\"\")\n",
        "print(f\"Bahdanau Attention:\")\n",
        "print(f\"  Time complexity: O(T * d^2) where T={seq_len}, d={hidden_size}\")\n",
        "print(f\"  Space complexity: O(T * d)\")\n",
        "print(f\"\")\n",
        "print(f\"Luong Attention (dot):\")\n",
        "print(f\"  Time complexity: O(T * d)\")\n",
        "print(f\"  Space complexity: O(T)\")\n",
        "print(f\"\")\n",
        "print(f\"Self-Attention:\")\n",
        "print(f\"  Time complexity: O(T^2 * d)\")\n",
        "print(f\"  Space complexity: O(T^2)\")\n",
        "print(f\"  Parallelizable: Yes (unlike RNNs)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "transformer_theory"
      },
      "source": [
        "## Part 5: The Transformer Architecture - \"Attention Is All You Need\"\n",
        "\n",
        "### Revolutionary Breakthrough (2017)\n",
        "\n",
        "The Transformer, introduced in the landmark paper \"Attention Is All You Need\" by Vaswani et al., revolutionized NLP by:\n",
        "1. **Eliminating RNNs entirely** - Pure attention-based architecture\n",
        "2. **Massive parallelization** - No sequential dependencies\n",
        "3. **Superior performance** - State-of-the-art results with less training time\n",
        "4. **Scalability** - Enables training of very large models\n",
        "\n",
        "### Core Architecture Components\n",
        "\n",
        "#### 1. Multi-Head Attention\n",
        "\n",
        "**Intuition:** Instead of single attention, use multiple \"attention heads\" to focus on different aspects.\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
        "\n",
        "Where each head is:\n",
        "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "\n",
        "**Benefits:**\n",
        "- **Different attention patterns**: Each head can specialize\n",
        "- **Richer representations**: Capture multiple types of relationships\n",
        "- **Parallel computation**: All heads computed simultaneously\n",
        "\n",
        "#### 2. Positional Encoding\n",
        "\n",
        "**Problem:** Attention mechanism has no notion of sequence order\n",
        "\n",
        "**Solution:** Add positional information to embeddings\n",
        "\n",
        "**Sinusoidal Encoding:**\n",
        "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "**Properties:**\n",
        "- **Deterministic**: Same position always gets same encoding\n",
        "- **Relative distances**: Model can learn relative positions\n",
        "- **Extrapolation**: Can handle longer sequences than seen in training\n",
        "\n",
        "#### 3. Layer Normalization and Residual Connections\n",
        "\n",
        "**Pre-Norm Architecture:**\n",
        "$$x' = x + \\text{Attention}(\\text{LayerNorm}(x))$$\n",
        "$$x'' = x' + \\text{FFN}(\\text{LayerNorm}(x'))$$\n",
        "\n",
        "**Benefits:**\n",
        "- **Gradient flow**: Residual connections help gradients flow\n",
        "- **Training stability**: Layer normalization stabilizes training\n",
        "- **Deep networks**: Enables training of very deep models\n",
        "\n",
        "#### 4. Feed-Forward Networks\n",
        "\n",
        "**Point-wise FFN:**\n",
        "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
        "\n",
        "**Properties:**\n",
        "- **Position-wise**: Applied to each position independently\n",
        "- **Large hidden dimension**: Typically 4× the model dimension\n",
        "- **Non-linearity**: Introduces non-linear transformations\n",
        "\n",
        "### Encoder-Decoder Structure\n",
        "\n",
        "**Encoder Stack:**\n",
        "- 6 identical layers\n",
        "- Each layer: Multi-Head Self-Attention + FFN\n",
        "- Residual connections and layer normalization\n",
        "\n",
        "**Decoder Stack:**\n",
        "- 6 identical layers  \n",
        "- Each layer: Masked Self-Attention + Encoder-Decoder Attention + FFN\n",
        "- Causal masking prevents looking at future tokens\n",
        "\n",
        "### Key Advantages\n",
        "\n",
        "**1. Parallelization:**\n",
        "- All positions processed simultaneously\n",
        "- Much faster training than RNNs\n",
        "- Better GPU utilization\n",
        "\n",
        "**2. Long-range dependencies:**\n",
        "- Direct connections between all positions\n",
        "- No vanishing gradient through long sequences\n",
        "- Better capture of long-term patterns\n",
        "\n",
        "**3. Interpretability:**\n",
        "- Attention weights show what model focuses on\n",
        "- Multiple heads capture different relationships\n",
        "- Better understanding of model behavior\n",
        "\n",
        "### Computational Complexity\n",
        "\n",
        "| Component | Complexity | Sequential Ops | Maximum Path Length |\n",
        "|-----------|------------|----------------|--------------------|\n",
        "| Self-Attention | $O(n^2 \\cdot d)$ | $O(1)$ | $O(1)$ |\n",
        "| Recurrent | $O(n \\cdot d^2)$ | $O(n)$ | $O(n)$ |\n",
        "| Convolutional | $O(k \\cdot n \\cdot d^2)$ | $O(1)$ | $O(\\log_k(n))$ |\n",
        "\n",
        "Where $n$ = sequence length, $d$ = representation dimension, $k$ = kernel size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "transformer_implementation"
      },
      "source": [
        "# Transformer Architecture Implementation\n",
        "# This implements the complete Transformer model from \"Attention Is All You Need\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import math\n",
        "\n",
        "print(\"Implementing Transformer Architecture...\")\n",
        "\n",
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention mechanism.\n",
        "    Core component of the Transformer architecture.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        assert d_model % self.num_heads == 0\n",
        "        \n",
        "        self.depth = d_model // self.num_heads\n",
        "        \n",
        "        # Linear projections for Q, K, V\n",
        "        self.wq = keras.layers.Dense(d_model)\n",
        "        self.wk = keras.layers.Dense(d_model)\n",
        "        self.wv = keras.layers.Dense(d_model)\n",
        "        \n",
        "        # Output projection\n",
        "        self.dense = keras.layers.Dense(d_model)\n",
        "    \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        # Linear transformations and split into heads\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "        \n",
        "        # Scaled dot-product attention\n",
        "        attention_output, attention_weights = self.scaled_dot_product_attention(\n",
        "            q, k, v, mask\n",
        "        )\n",
        "        \n",
        "        # Concatenate heads\n",
        "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention_output, \n",
        "                                    (batch_size, -1, self.d_model))\n",
        "        \n",
        "        # Final linear projection\n",
        "        output = self.dense(concat_attention)\n",
        "        \n",
        "        return output, attention_weights\n",
        "    \n",
        "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
        "        \"\"\"Calculate the attention weights.\n",
        "        q, k, v must have matching leading dimensions.\n",
        "        k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "        The mask has different shapes depending on its type(padding or look ahead) \n",
        "        but it must be broadcastable for addition.\n",
        "        \n",
        "        Args:\n",
        "            q: query shape == (..., seq_len_q, depth)\n",
        "            k: key shape == (..., seq_len_k, depth)\n",
        "            v: value shape == (..., seq_len_v, depth_v)\n",
        "            mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "        \n",
        "        Returns:\n",
        "            output, attention_weights\n",
        "        \"\"\"\n",
        "        \n",
        "        # Calculate attention scores\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "        \n",
        "        # Scale matmul_qk\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "        \n",
        "        # Add the mask to the scaled tensor.\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "        \n",
        "        # Softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        \n",
        "        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "class PositionalEncoding(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Sinusoidal positional encoding as described in 'Attention Is All You Need'.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, position, d_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position = position\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "        return pos * angle_rates\n",
        "    \n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
        "                                   np.arange(d_model)[np.newaxis, :],\n",
        "                                   d_model)\n",
        "        \n",
        "        # Apply sin to even indices in the array; 2i\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        \n",
        "        # Apply cos to odd indices in the array; 2i+1\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "        \n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        \n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "    \n",
        "    def call(self, x):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        return x + self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    \"\"\"\n",
        "    Point-wise feed forward network.\n",
        "    Two linear transformations with a ReLU activation in between.\n",
        "    \"\"\"\n",
        "    return keras.Sequential([\n",
        "        keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])\n",
        "\n",
        "\n",
        "class EncoderLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Single encoder layer in the Transformer.\n",
        "    Contains multi-head attention and feed-forward network.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "        \n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask=None):\n",
        "        # Multi-head attention\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # Self-attention\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # Residual connection + layer norm\n",
        "        \n",
        "        # Feed-forward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection + layer norm\n",
        "        \n",
        "        return out2\n",
        "\n",
        "\n",
        "class DecoderLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Single decoder layer in the Transformer.\n",
        "    Contains masked self-attention, encoder-decoder attention, and feed-forward network.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        \n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "        \n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "        self.dropout3 = keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "        # Masked self-attention\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "        \n",
        "        # Encoder-decoder attention\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask\n",
        "        )\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "        \n",
        "        # Feed-forward network\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "        \n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "\n",
        "class Encoder(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Complete Transformer encoder stack.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.embedding = keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
        "        \n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                          for _ in range(num_layers)]\n",
        "        \n",
        "        self.dropout = keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        \n",
        "        # Embedding + positional encoding\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "        \n",
        "        x = self.dropout(x, training=training)\n",
        "        \n",
        "        # Pass through encoder layers\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "        \n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "\n",
        "class Decoder(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Complete Transformer decoder stack.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.embedding = keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                          for _ in range(num_layers)]\n",
        "        self.dropout = keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "        \n",
        "        # Embedding + positional encoding\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "        \n",
        "        x = self.dropout(x, training=training)\n",
        "        \n",
        "        # Pass through decoder layers\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                 look_ahead_mask, padding_mask)\n",
        "            \n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "        \n",
        "        return x, attention_weights\n",
        "\n",
        "\n",
        "class Transformer(keras.Model):\n",
        "    \"\"\"\n",
        "    Complete Transformer model for sequence-to-sequence tasks.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 target_vocab_size, pe_input, pe_target, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                             input_vocab_size, pe_input, rate)\n",
        "        \n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                             target_vocab_size, pe_target, rate)\n",
        "        \n",
        "        self.final_layer = keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask=None,\n",
        "             look_ahead_mask=None, dec_padding_mask=None):\n",
        "        \n",
        "        # Encoder\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "        \n",
        "        # Decoder\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask\n",
        "        )\n",
        "        \n",
        "        # Final linear layer\n",
        "        final_output = self.final_layer(dec_output)\n",
        "        \n",
        "        return final_output, attention_weights\n",
        "\n",
        "\n",
        "# Test the Transformer implementation\n",
        "print(\"Testing Transformer Components...\")\n",
        "\n",
        "# Model hyperparameters\n",
        "num_layers = 2\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "dff = 512\n",
        "input_vocab_size = 1000\n",
        "target_vocab_size = 1000\n",
        "pe_input = 1000\n",
        "pe_target = 1000\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Create sample data\n",
        "sample_encoder_input = tf.random.uniform((2, 10), maxval=input_vocab_size, dtype=tf.int32)\n",
        "sample_decoder_input = tf.random.uniform((2, 8), maxval=target_vocab_size, dtype=tf.int32)\n",
        "\n",
        "print(f\"Model Configuration:\")\n",
        "print(f\"Number of layers: {num_layers}\")\n",
        "print(f\"Model dimension: {d_model}\")\n",
        "print(f\"Number of heads: {num_heads}\")\n",
        "print(f\"Feed-forward dimension: {dff}\")\n",
        "print(f\"Dropout rate: {dropout_rate}\")\n",
        "\n",
        "# Test individual components\n",
        "print(f\"\\nTesting individual components:\")\n",
        "\n",
        "# Test Multi-Head Attention\n",
        "mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "sample_mha_input = tf.random.uniform((2, 10, d_model))\n",
        "mha_output, mha_weights = mha(sample_mha_input, sample_mha_input, sample_mha_input)\n",
        "print(f\"Multi-Head Attention output shape: {mha_output.shape}\")\n",
        "print(f\"Attention weights shape: {mha_weights.shape}\")\n",
        "\n",
        "# Test Positional Encoding\n",
        "pos_encoding = PositionalEncoding(position=50, d_model=d_model)\n",
        "sample_pos_input = tf.random.uniform((2, 10, d_model))\n",
        "pos_output = pos_encoding(sample_pos_input)\n",
        "print(f\"Positional encoding output shape: {pos_output.shape}\")\n",
        "\n",
        "# Test complete Transformer\n",
        "print(f\"\\nTesting complete Transformer model:\")\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=input_vocab_size,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    pe_input=pe_input,\n",
        "    pe_target=pe_target,\n",
        "    rate=dropout_rate\n",
        ")\n",
        "\n",
        "# Forward pass\n",
        "temp_output, temp_attention = transformer(\n",
        "    sample_encoder_input,\n",
        "    sample_decoder_input,\n",
        "    training=False\n",
        ")\n",
        "\n",
        "print(f\"Transformer output shape: {temp_output.shape}\")\n",
        "print(f\"Number of attention weight tensors: {len(temp_attention)}\")\n",
        "\n",
        "# Model summary\n",
        "print(f\"\\nTransformer Model Summary:\")\n",
        "transformer.summary()\n",
        "\n",
        "# Parameter count\n",
        "total_params = sum([np.prod(var.get_shape().as_list()) for var in transformer.trainable_variables])\n",
        "print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
        "\n",
        "# Memory analysis\n",
        "print(f\"\\nMemory Analysis:\")\n",
        "param_memory = total_params * 4 / (1024**2)  # 4 bytes per float32\n",
        "print(f\"Parameter memory: {param_memory:.1f} MB\")\n",
        "\n",
        "# Computational complexity\n",
        "seq_len = 10\n",
        "attention_ops = num_layers * 2 * seq_len**2 * d_model  # Self-attention + encoder-decoder attention\n",
        "ffn_ops = num_layers * seq_len * d_model * dff\n",
        "total_ops = attention_ops + ffn_ops\n",
        "\n",
        "print(f\"\\nComputational Complexity (seq_len={seq_len}):\")\n",
        "print(f\"Attention operations: {attention_ops:,}\")\n",
        "print(f\"Feed-forward operations: {ffn_ops:,}\")\n",
        "print(f\"Total operations: {total_ops:,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mask_creation_theory"
      },
      "source": [
        "### Masking in Transformers\n",
        "\n",
        "#### Types of Masks\n",
        "\n",
        "**1. Padding Mask:**\n",
        "Prevents attention to padding tokens\n",
        "$$\\text{mask}_{\\text{pad}}[i,j] = \\begin{cases} 0 & \\text{if token}_j \\neq \\text{PAD} \\\\ -\\infty & \\text{if token}_j = \\text{PAD} \\end{cases}$$\n",
        "\n",
        "**2. Look-Ahead Mask (Causal Mask):**\n",
        "Prevents attention to future tokens during training\n",
        "$$\\text{mask}_{\\text{causal}}[i,j] = \\begin{cases} 0 & \\text{if } j \\leq i \\\\ -\\infty & \\text{if } j > i \\end{cases}$$\n",
        "\n",
        "**3. Combined Mask:**\n",
        "$$\\text{mask}_{\\text{combined}} = \\text{mask}_{\\text{pad}} + \\text{mask}_{\\text{causal}}$$\n",
        "\n",
        "#### Mathematical Impact\n",
        "\n",
        "**Attention with Mask:**\n",
        "$$\\text{Attention}(Q, K, V, \\text{mask}) = \\text{softmax}\\left(\\frac{QK^T + \\text{mask}}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "When mask value is $-\\infty$, softmax output becomes 0:\n",
        "$$\\lim_{x \\to -\\infty} \\frac{e^x}{\\sum_j e^{x_j}} = 0$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mask_creation"
      },
      "source": [
        "# Mask Creation for Transformers\n",
        "# This demonstrates how to create different types of masks used in Transformers\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    \"\"\"\n",
        "    Create padding mask to ignore padding tokens.\n",
        "    \n",
        "    Args:\n",
        "        seq: Input sequence with padding tokens (typically 0)\n",
        "    \n",
        "    Returns:\n",
        "        Mask tensor where 1 indicates padding positions\n",
        "    \"\"\"\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    \n",
        "    # Add extra dimensions for multi-head attention\n",
        "    # [batch_size, 1, 1, seq_len]\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    \"\"\"\n",
        "    Create look-ahead mask to prevent attention to future tokens.\n",
        "    \n",
        "    Args:\n",
        "        size: Sequence length\n",
        "    \n",
        "    Returns:\n",
        "        Upper triangular matrix with 1s above diagonal\n",
        "    \"\"\"\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # [seq_len, seq_len]\n",
        "\n",
        "\n",
        "def create_masks(inp, tar):\n",
        "    \"\"\"\n",
        "    Create all masks needed for training.\n",
        "    \n",
        "    Args:\n",
        "        inp: Encoder input sequence\n",
        "        tar: Decoder input sequence\n",
        "    \n",
        "    Returns:\n",
        "        enc_padding_mask: Encoder padding mask\n",
        "        combined_mask: Combined look-ahead and padding mask for decoder\n",
        "        dec_padding_mask: Padding mask for encoder-decoder attention\n",
        "    \"\"\"\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 2nd attention block in the decoder\n",
        "    # This padding mask is used to mask the encoder outputs\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 1st attention block in the decoder\n",
        "    # It is used to pad and mask future tokens in the input received by the decoder\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "\n",
        "# Demonstrate mask creation\n",
        "print(\"Demonstrating Transformer Mask Creation...\")\n",
        "\n",
        "# Sample sequences with padding\n",
        "sample_inp = tf.constant([[1, 2, 3, 4, 0, 0],  # Sequence with 2 padding tokens\n",
        "                         [5, 6, 7, 0, 0, 0]])  # Sequence with 3 padding tokens\n",
        "\n",
        "sample_tar = tf.constant([[1, 2, 3, 0],        # Target with 1 padding token\n",
        "                         [4, 5, 0, 0]])        # Target with 2 padding tokens\n",
        "\n",
        "print(f\"Input sequences shape: {sample_inp.shape}\")\n",
        "print(f\"Target sequences shape: {sample_tar.shape}\")\n",
        "print(f\"Input sequences:\")\n",
        "print(sample_inp.numpy())\n",
        "print(f\"Target sequences:\")\n",
        "print(sample_tar.numpy())\n",
        "\n",
        "# Create masks\n",
        "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(sample_inp, sample_tar)\n",
        "\n",
        "print(f\"\\nEncoder padding mask shape: {enc_padding_mask.shape}\")\n",
        "print(f\"Combined mask shape: {combined_mask.shape}\")\n",
        "print(f\"Decoder padding mask shape: {dec_padding_mask.shape}\")\n",
        "\n",
        "# Visualize masks\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Encoder padding mask\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.imshow(enc_padding_mask[0, 0, 0, :].numpy().reshape(1, -1), cmap='Blues')\n",
        "plt.title('Encoder Padding Mask\\n(1st sequence)')\n",
        "plt.xlabel('Token Position')\n",
        "plt.yticks([])\n",
        "for i, val in enumerate(enc_padding_mask[0, 0, 0, :].numpy()):\n",
        "    plt.text(i, 0, f'{val:.0f}', ha='center', va='center')\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.imshow(enc_padding_mask[1, 0, 0, :].numpy().reshape(1, -1), cmap='Blues')\n",
        "plt.title('Encoder Padding Mask\\n(2nd sequence)')\n",
        "plt.xlabel('Token Position')\n",
        "plt.yticks([])\n",
        "for i, val in enumerate(enc_padding_mask[1, 0, 0, :].numpy()):\n",
        "    plt.text(i, 0, f'{val:.0f}', ha='center', va='center')\n",
        "\n",
        "# Look-ahead mask\n",
        "plt.subplot(2, 3, 3)\n",
        "look_ahead = create_look_ahead_mask(4)\n",
        "plt.imshow(look_ahead.numpy(), cmap='Blues')\n",
        "plt.title('Look-Ahead Mask\\n(size=4)')\n",
        "plt.xlabel('Key Position')\n",
        "plt.ylabel('Query Position')\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        plt.text(j, i, f'{look_ahead[i,j]:.0f}', ha='center', va='center')\n",
        "\n",
        "# Combined mask for first sequence\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.imshow(combined_mask[0].numpy(), cmap='Blues')\n",
        "plt.title('Combined Mask\\n(1st sequence)')\n",
        "plt.xlabel('Key Position')\n",
        "plt.ylabel('Query Position')\n",
        "for i in range(combined_mask.shape[1]):\n",
        "    for j in range(combined_mask.shape[2]):\n",
        "        plt.text(j, i, f'{combined_mask[0,i,j]:.0f}', ha='center', va='center', fontsize=8)\n",
        "\n",
        "# Combined mask for second sequence\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.imshow(combined_mask[1].numpy(), cmap='Blues')\n",
        "plt.title('Combined Mask\\n(2nd sequence)')\n",
        "plt.xlabel('Key Position')\n",
        "plt.ylabel('Query Position')\n",
        "for i in range(combined_mask.shape[1]):\n",
        "    for j in range(combined_mask.shape[2]):\n",
        "        plt.text(j, i, f'{combined_mask[1,i,j]:.0f}', ha='center', va='center', fontsize=8)\n",
        "\n",
        "# Attention weights example\n",
        "plt.subplot(2, 3, 6)\n",
        "# Simulate attention scores before masking\n",
        "scores = tf.random.uniform((4, 4), minval=-2, maxval=2)\n",
        "masked_scores = scores + (combined_mask[0] * -1e9)\n",
        "attention_weights = tf.nn.softmax(masked_scores)\n",
        "\n",
        "plt.imshow(attention_weights.numpy(), cmap='Blues')\n",
        "plt.title('Attention Weights\\n(after masking)')\n",
        "plt.xlabel('Key Position')\n",
        "plt.ylabel('Query Position')\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        plt.text(j, i, f'{attention_weights[i,j]:.2f}', ha='center', va='center', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Explain mask effects\n",
        "print(\"\\nMask Explanations:\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\n1. PADDING MASK:\")\n",
        "print(\"   • Value 1 indicates padding positions\")\n",
        "print(\"   • Prevents attention to padding tokens\")\n",
        "print(\"   • Applied to both encoder and decoder\")\n",
        "\n",
        "print(\"\\n2. LOOK-AHEAD MASK:\")\n",
        "print(\"   • Upper triangular matrix with 1s\")\n",
        "print(\"   • Prevents attention to future tokens\")\n",
        "print(\"   • Only applied to decoder self-attention\")\n",
        "print(\"   • Ensures autoregressive property\")\n",
        "\n",
        "print(\"\\n3. COMBINED MASK:\")\n",
        "print(\"   • Maximum of padding and look-ahead masks\")\n",
        "print(\"   • Combines both masking effects\")\n",
        "print(\"   • Used in decoder self-attention\")\n",
        "\n",
        "print(\"\\n4. ATTENTION WEIGHTS AFTER MASKING:\")\n",
        "print(\"   • Masked positions have weight ≈ 0\")\n",
        "print(\"   • Valid positions sum to 1\")\n",
        "print(\"   • Lower triangle shows causal attention pattern\")\n",
        "\n",
        "# Verify mask properties\n",
        "print(\"\\nMask Verification:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Check that attention weights sum to 1 for valid positions\n",
        "for i in range(attention_weights.shape[0]):\n",
        "    row_sum = tf.reduce_sum(attention_weights[i])\n",
        "    print(f\"Attention weights row {i} sum: {row_sum:.6f}\")\n",
        "\n",
        "# Check causal property\n",
        "print(f\"\\nCausal Property Check:\")\n",
        "for i in range(attention_weights.shape[0]):\n",
        "    future_attention = tf.reduce_sum(attention_weights[i, i+1:])\n",
        "    print(f\"Position {i} attention to future: {future_attention:.6f}\")\n",
        "\n",
        "# Complexity analysis for different mask types\n",
        "print(f\"\\nComplexity Analysis:\")\n",
        "print(f\"Padding mask creation: O(batch_size × seq_len)\")\n",
        "print(f\"Look-ahead mask creation: O(seq_len^2)\")\n",
        "print(f\"Mask application: O(batch_size × num_heads × seq_len^2)\")\n",
        "print(f\"Memory overhead: Minimal (masks are sparse)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recent_innovations_theory"
      },
      "source": [
        "## Part 6: Recent Innovations in Language Models (2018-2019)\n",
        "\n",
        "### The \"ImageNet Moment\" for NLP\n",
        "\n",
        "2018 marked a revolutionary year in NLP, often called the \"ImageNet moment\" due to breakthrough advances in:\n",
        "1. **Transfer Learning for NLP**\n",
        "2. **Bidirectional Representations**\n",
        "3. **Massive Scale Pretraining**\n",
        "4. **Zero-shot and Few-shot Learning**\n",
        "\n",
        "### ELMo: Embeddings from Language Models\n",
        "\n",
        "**Key Innovation:** Contextualized word embeddings\n",
        "\n",
        "**Traditional Problem:**\n",
        "- Static embeddings: \"bank\" has same representation in \"river bank\" and \"money bank\"\n",
        "\n",
        "**ELMo Solution:**\n",
        "- Dynamic embeddings based on context\n",
        "- \"bank\" gets different representations based on surrounding words\n",
        "\n",
        "**Architecture:**\n",
        "$$\\text{ELMo}_k^{\\text{task}} = \\gamma^{\\text{task}} \\sum_{j=0}^{L} s_j^{\\text{task}} h_{k,j}^{LM}$$\n",
        "\n",
        "Where:\n",
        "- $h_{k,j}^{LM}$: Hidden state from layer $j$ of bidirectional LM\n",
        "- $s_j^{\\text{task}}$: Task-specific softmax weights\n",
        "- $\\gamma^{\\text{task}}$: Task-specific scaling factor\n",
        "\n",
        "**Benefits:**\n",
        "- **Polysemy handling**: Different meanings of same word\n",
        "- **Rich representations**: Combines all layers\n",
        "- **Transfer learning**: Pretrained on large corpus\n",
        "\n",
        "### ULMFiT: Universal Language Model Fine-tuning\n",
        "\n",
        "**Three-Stage Process:**\n",
        "\n",
        "**1. General-Domain LM Pretraining:**\n",
        "$$L_{LM} = -\\sum_{t=1}^T \\log P(w_t | w_{1:t-1})$$\n",
        "\n",
        "**2. Target Task LM Fine-tuning:**\n",
        "Fine-tune on target domain data with discriminative learning rates.\n",
        "\n",
        "**3. Target Task Classifier Fine-tuning:**\n",
        "Add classifier layers and fine-tune entire model.\n",
        "\n",
        "**Key Techniques:**\n",
        "- **Discriminative fine-tuning**: Different learning rates per layer\n",
        "- **Slanted triangular learning rates**: Gradual learning rate schedule\n",
        "- **Gradual unfreezing**: Progressively unfreeze layers\n",
        "\n",
        "**Results:**\n",
        "- 100 labeled examples = 10,000 examples trained from scratch\n",
        "- 18-24% error reduction on text classification tasks\n",
        "\n",
        "### GPT: Generative Pre-Training\n",
        "\n",
        "**Core Concept:** Unsupervised pretraining + supervised fine-tuning\n",
        "\n",
        "**Pretraining Objective:**\n",
        "$$L_1(\\mathcal{U}) = \\sum_i \\log P(u_i | u_{i-k}, ..., u_{i-1}; \\Theta)$$\n",
        "\n",
        "**Fine-tuning Objective:**\n",
        "$$L_2(\\mathcal{C}) = \\sum_{(x,y)} \\log P(y | x^1, ..., x^m)$$\n",
        "\n",
        "**Combined Training:**\n",
        "$$L_3(\\mathcal{C}) = L_2(\\mathcal{C}) + \\lambda \\cdot L_1(\\mathcal{C})$$\n",
        "\n",
        "**Architecture Innovations:**\n",
        "- **Transformer decoder only**: Unidirectional attention\n",
        "- **Task-specific input transformations**: Minimal architectural changes\n",
        "- **Linear output layers**: Simple adaptation to different tasks\n",
        "\n",
        "### BERT: Bidirectional Encoder Representations\n",
        "\n",
        "**Revolutionary Approach:** Bidirectional context understanding\n",
        "\n",
        "**Pretraining Tasks:**\n",
        "\n",
        "**1. Masked Language Model (MLM):**\n",
        "- Randomly mask 15% of tokens\n",
        "- 80% → [MASK], 10% → random word, 10% → unchanged\n",
        "- Predict masked tokens using bidirectional context\n",
        "\n",
        "$$L_{MLM} = -\\sum_{i \\in \\text{masked}} \\log P(w_i | w_{\\text{context}})$$\n",
        "\n",
        "**2. Next Sentence Prediction (NSP):**\n",
        "- Given sentence pairs, predict if B follows A\n",
        "- 50% consecutive pairs, 50% random pairs\n",
        "\n",
        "$$L_{NSP} = -\\log P(\\text{IsNext} | [\\text{CLS}])$$\n",
        "\n",
        "**Total Loss:**\n",
        "$$L = L_{MLM} + L_{NSP}$$\n",
        "\n",
        "**Key Advantages:**\n",
        "- **True bidirectionality**: Unlike GPT's left-to-right\n",
        "- **Sentence-level understanding**: NSP task\n",
        "- **Fine-tuning flexibility**: Easy adaptation to downstream tasks\n",
        "\n",
        "### Comparative Analysis\n",
        "\n",
        "| Model | Architecture | Pretraining | Bidirectional | Key Innovation |\n",
        "|-------|-------------|-------------|---------------|----------------|\n",
        "| **ELMo** | BiLSTM | Language Modeling | Yes | Contextualized embeddings |\n",
        "| **ULMFiT** | LSTM | Language Modeling | No | Transfer learning methodology |\n",
        "| **GPT** | Transformer Decoder | Language Modeling | No | Transformer + unsupervised pretraining |\n",
        "| **BERT** | Transformer Encoder | MLM + NSP | Yes | Bidirectional Transformer |\n",
        "\n",
        "### Impact and Implications\n",
        "\n",
        "**Scientific Impact:**\n",
        "- Established transfer learning as standard in NLP\n",
        "- Showed importance of scale in pretraining\n",
        "- Demonstrated power of self-supervised learning\n",
        "\n",
        "**Practical Impact:**\n",
        "- Dramatically improved state-of-the-art across tasks\n",
        "- Reduced data requirements for new tasks\n",
        "- Enabled rapid prototyping and deployment\n",
        "\n",
        "**Future Directions:**\n",
        "- Larger models (GPT-2, GPT-3, etc.)\n",
        "- More efficient architectures\n",
        "- Specialized pretraining objectives\n",
        "- Multimodal understanding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "modern_nlp_demo"
      },
      "source": [
        "# Demonstration of Modern NLP Approaches\n",
        "# This shows how the innovations from 2018-2019 changed NLP\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "print(\"Demonstrating Modern NLP Innovations...\")\n",
        "\n",
        "# Simulate different embedding approaches\n",
        "class TraditionalEmbedding(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Traditional static word embeddings (like Word2Vec, GloVe).\n",
        "    Same word always gets same embedding regardless of context.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        # Static embeddings - same word ID always produces same vector\n",
        "        return self.embedding(inputs)\n",
        "\n",
        "\n",
        "class ContextualEmbedding(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Contextualized embeddings (ELMo-style).\n",
        "    Word embeddings depend on surrounding context.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.forward_lstm = keras.layers.LSTM(hidden_dim, return_sequences=True)\n",
        "        self.backward_lstm = keras.layers.LSTM(hidden_dim, return_sequences=True, go_backwards=True)\n",
        "        self.context_projection = keras.layers.Dense(embedding_dim)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        # Start with static embeddings\n",
        "        static_embeddings = self.embedding(inputs)\n",
        "        \n",
        "        # Add bidirectional context\n",
        "        forward_context = self.forward_lstm(static_embeddings)\n",
        "        backward_context = self.backward_lstm(static_embeddings)\n",
        "        \n",
        "        # Combine contexts\n",
        "        combined_context = tf.concat([forward_context, backward_context], axis=-1)\n",
        "        \n",
        "        # Project back to embedding dimension\n",
        "        contextual_embeddings = self.context_projection(combined_context)\n",
        "        \n",
        "        # Combine static and contextual (ELMo-style weighted combination)\n",
        "        return static_embeddings + contextual_embeddings\n",
        "\n",
        "\n",
        "class MaskedLanguageModel(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Simplified BERT-style Masked Language Model.\n",
        "    Demonstrates bidirectional context understanding.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, d_model, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(1000, d_model)\n",
        "        \n",
        "        # Simplified transformer encoder\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = keras.Sequential([\n",
        "            keras.layers.Dense(d_model * 4, activation='relu'),\n",
        "            keras.layers.Dense(d_model)\n",
        "        ])\n",
        "        \n",
        "        self.layernorm1 = keras.layers.LayerNormalization()\n",
        "        self.layernorm2 = keras.layers.LayerNormalization()\n",
        "        \n",
        "        # MLM head\n",
        "        self.mlm_head = keras.layers.Dense(vocab_size, activation='softmax')\n",
        "    \n",
        "    def call(self, inputs, mask=None):\n",
        "        # Embedding + positional encoding\n",
        "        x = self.embedding(inputs)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "        \n",
        "        # Self-attention (bidirectional)\n",
        "        attn_output, _ = self.attention(x, x, x, mask)\n",
        "        x = self.layernorm1(x + attn_output)\n",
        "        \n",
        "        # Feed-forward\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.layernorm2(x + ffn_output)\n",
        "        \n",
        "        # MLM predictions\n",
        "        mlm_logits = self.mlm_head(x)\n",
        "        \n",
        "        return mlm_logits\n",
        "\n",
        "\n",
        "# Compare different embedding approaches\n",
        "print(\"Comparing Embedding Approaches...\")\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 1000\n",
        "embedding_dim = 128\n",
        "seq_length = 10\n",
        "batch_size = 2\n",
        "\n",
        "# Sample input: \"The bank by the river\" vs \"The bank for money\"\n",
        "# Word \"bank\" (ID=5) appears in different contexts\n",
        "sample_input = tf.constant([\n",
        "    [1, 5, 3, 1, 7, 0, 0, 0, 0, 0],  # \"The bank by the river\" + padding\n",
        "    [1, 5, 2, 9, 0, 0, 0, 0, 0, 0]   # \"The bank for money\" + padding\n",
        "])\n",
        "\n",
        "print(f\"Sample input shape: {sample_input.shape}\")\n",
        "print(f\"Sample input:\")\n",
        "print(sample_input.numpy())\n",
        "\n",
        "# Test traditional embeddings\n",
        "traditional_model = TraditionalEmbedding(vocab_size, embedding_dim)\n",
        "traditional_embeddings = traditional_model(sample_input)\n",
        "\n",
        "print(f\"\\nTraditional embeddings shape: {traditional_embeddings.shape}\")\n",
        "\n",
        "# Check if \"bank\" gets same embedding in both contexts\n",
        "bank_embedding_1 = traditional_embeddings[0, 1]  # \"bank\" in first sentence\n",
        "bank_embedding_2 = traditional_embeddings[1, 1]  # \"bank\" in second sentence\n",
        "traditional_similarity = tf.reduce_sum(bank_embedding_1 * bank_embedding_2)\n",
        "\n",
        "print(f\"Traditional 'bank' embedding similarity: {traditional_similarity:.6f}\")\n",
        "print(\"(Should be very high - same word gets same embedding)\")\n",
        "\n",
        "# Test contextual embeddings\n",
        "contextual_model = ContextualEmbedding(vocab_size, embedding_dim, 64)\n",
        "contextual_embeddings = contextual_model(sample_input)\n",
        "\n",
        "print(f\"\\nContextual embeddings shape: {contextual_embeddings.shape}\")\n",
        "\n",
        "# Check if \"bank\" gets different embeddings in different contexts\n",
        "bank_contextual_1 = contextual_embeddings[0, 1]  # \"bank\" in first sentence\n",
        "bank_contextual_2 = contextual_embeddings[1, 1]  # \"bank\" in second sentence\n",
        "contextual_similarity = tf.reduce_sum(bank_contextual_1 * bank_contextual_2)\n",
        "\n",
        "print(f\"Contextual 'bank' embedding similarity: {contextual_similarity:.6f}\")\n",
        "print(\"(Should be lower - different contexts produce different embeddings)\")\n",
        "\n",
        "# Demonstrate BERT-style MLM\n",
        "print(f\"\\nDemonstrating BERT-style Masked Language Modeling...\")\n",
        "\n",
        "# Create masked input (mask token ID = vocab_size)\n",
        "masked_input = tf.constant([\n",
        "    [1, vocab_size, 3, 1, 7, 0, 0, 0, 0, 0],  # \"The [MASK] by the river\"\n",
        "    [1, vocab_size, 2, 9, 0, 0, 0, 0, 0, 0]   # \"The [MASK] for money\"\n",
        "])\n",
        "\n",
        "print(f\"Masked input (mask token = {vocab_size}):\")\n",
        "print(masked_input.numpy())\n",
        "\n",
        "# BERT-style model\n",
        "bert_model = MaskedLanguageModel(vocab_size + 1, 128, 4)  # +1 for mask token\n",
        "mlm_predictions = bert_model(masked_input)\n",
        "\n",
        "print(f\"\\nMLM predictions shape: {mlm_predictions.shape}\")\n",
        "\n",
        "# Get predictions for masked positions\n",
        "mask_pos_1 = mlm_predictions[0, 1]  # Predictions for mask in first sentence\n",
        "mask_pos_2 = mlm_predictions[1, 1]  # Predictions for mask in second sentence\n",
        "\n",
        "top_predictions_1 = tf.nn.top_k(mask_pos_1, 3)\n",
        "top_predictions_2 = tf.nn.top_k(mask_pos_2, 3)\n",
        "\n",
        "print(f\"\\nTop 3 predictions for first masked position:\")\n",
        "print(f\"Token IDs: {top_predictions_1.indices.numpy()}\")\n",
        "print(f\"Probabilities: {top_predictions_1.values.numpy()}\")\n",
        "\n",
        "print(f\"\\nTop 3 predictions for second masked position:\")\n",
        "print(f\"Token IDs: {top_predictions_2.indices.numpy()}\")\n",
        "print(f\"Probabilities: {top_predictions_2.values.numpy()}\")\n",
        "\n",
        "# Transfer learning simulation\n",
        "print(f\"\\nSimulating Transfer Learning Benefits...\")\n",
        "\n",
        "# Simulate performance with different amounts of training data\n",
        "def simulate_performance(use_pretrained=False, num_examples=100):\n",
        "    \"\"\"\n",
        "    Simulate classification performance with/without pretraining.\n",
        "    \"\"\"\n",
        "    if use_pretrained:\n",
        "        # Pretrained models perform better with less data\n",
        "        base_accuracy = 0.85\n",
        "        data_efficiency = 0.1  # Less sensitive to data amount\n",
        "    else:\n",
        "        # Models trained from scratch need more data\n",
        "        base_accuracy = 0.65\n",
        "        data_efficiency = 0.3  # More sensitive to data amount\n",
        "    \n",
        "    # Simulate accuracy based on data amount\n",
        "    accuracy = base_accuracy * (1 - np.exp(-num_examples * data_efficiency / 100))\n",
        "    return accuracy\n",
        "\n",
        "# Compare performance across different data amounts\n",
        "data_amounts = [10, 50, 100, 500, 1000, 5000]\n",
        "pretrained_performance = [simulate_performance(True, n) for n in data_amounts]\n",
        "scratch_performance = [simulate_performance(False, n) for n in data_amounts]\n",
        "\n",
        "print(f\"\\nTransfer Learning Performance Comparison:\")\n",
        "print(f\"{'Data Size':<10} {'From Scratch':<15} {'Pretrained':<15} {'Improvement':<15}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for i, n in enumerate(data_amounts):\n",
        "    scratch_acc = scratch_performance[i]\n",
        "    pretrained_acc = pretrained_performance[i]\n",
        "    improvement = (pretrained_acc - scratch_acc) / scratch_acc * 100\n",
        "    \n",
        "    print(f\"{n:<10} {scratch_acc:<15.3f} {pretrained_acc:<15.3f} {improvement:<15.1f}%\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(data_amounts, scratch_performance, 'b-o', label='From Scratch')\n",
        "plt.plot(data_amounts, pretrained_performance, 'r-s', label='Pretrained')\n",
        "plt.xlabel('Training Examples')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Transfer Learning Benefits')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xscale('log')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "# Visualize embedding differences\n",
        "bank_traditional = traditional_embeddings[:, 1, :5].numpy()  # First 5 dims\n",
        "bank_contextual = contextual_embeddings[:, 1, :5].numpy()\n",
        "\n",
        "x = np.arange(5)\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, bank_traditional[0], width, label='River Context (Traditional)', alpha=0.7)\n",
        "plt.bar(x + width/2, bank_traditional[1], width, label='Money Context (Traditional)', alpha=0.7)\n",
        "plt.xlabel('Embedding Dimension')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Traditional Embeddings: \"bank\"')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.bar(x - width/2, bank_contextual[0], width, label='River Context (Contextual)', alpha=0.7)\n",
        "plt.bar(x + width/2, bank_contextual[1], width, label='Money Context (Contextual)', alpha=0.7)\n",
        "plt.xlabel('Embedding Dimension')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Contextual Embeddings: \"bank\"')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "# Timeline of innovations\n",
        "innovations = ['Word2Vec\\n(2013)', 'ELMo\\n(2018)', 'ULMFiT\\n(2018)', 'GPT\\n(2018)', 'BERT\\n(2018)']\n",
        "years = [2013, 2018.1, 2018.3, 2018.6, 2018.8]\n",
        "performance = [70, 75, 82, 85, 88]  # Simulated GLUE scores\n",
        "\n",
        "plt.plot(years, performance, 'go-', linewidth=2, markersize=8)\n",
        "for i, (year, perf, innovation) in enumerate(zip(years, performance, innovations)):\n",
        "    plt.annotate(innovation, (year, perf), textcoords=\"offset points\", \n",
        "                xytext=(0,10), ha='center', fontsize=8)\n",
        "\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Performance (GLUE Score)')\n",
        "plt.title('NLP Progress Timeline (2013-2019)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim(65, 92)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary of innovations\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY OF 2018-2019 NLP INNOVATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. CONTEXTUALIZED EMBEDDINGS (ELMo):\")\n",
        "print(\"   • Words get different representations based on context\")\n",
        "print(\"   • Solves polysemy problem (multiple meanings)\")\n",
        "print(\"   • Uses bidirectional LSTM language model\")\n",
        "\n",
        "print(\"\\n2. TRANSFER LEARNING METHODOLOGY (ULMFiT):\")\n",
        "print(\"   • Established 3-stage transfer learning process\")\n",
        "print(\"   • Showed massive data efficiency gains\")\n",
        "print(\"   • Introduced discriminative fine-tuning techniques\")\n",
        "\n",
        "print(\"\\n3. TRANSFORMER PRETRAINING (GPT):\")\n",
        "print(\"   • Combined Transformer architecture with unsupervised pretraining\")\n",
        "print(\"   • Demonstrated generative pretraining effectiveness\")\n",
        "print(\"   • Minimal task-specific architectural changes\")\n",
        "\n",
        "print(\"\\n4. BIDIRECTIONAL TRANSFORMERS (BERT):\")\n",
        "print(\"   • True bidirectional context understanding\")\n",
        "print(\"   • Masked Language Model (MLM) pretraining\")\n",
        "print(\"   • Next Sentence Prediction (NSP) task\")\n",
        "print(\"   • Revolutionary performance across NLP tasks\")\n",
        "\n",
        "print(\"\\n5. KEY BREAKTHROUGH PRINCIPLES:\")\n",
        "print(\"   • Scale matters: Larger models + more data = better performance\")\n",
        "print(\"   • Transfer learning: Pretrain on large corpus, fine-tune on task\")\n",
        "print(\"   • Self-supervised learning: Learn from unlabeled text\")\n",
        "print(\"   • Architecture matters: Transformers > RNNs for many tasks\")\n",
        "\n",
        "print(\"\\n6. IMPACT ON INDUSTRY:\")\n",
        "print(\"   • Reduced time-to-market for NLP applications\")\n",
        "print(\"   • Democratized access to state-of-the-art NLP\")\n",
        "print(\"   • Enabled rapid prototyping and experimentation\")\n",
        "print(\"   • Set foundation for modern language models (GPT-3, ChatGPT, etc.)\")\n",
        "\n",
        "# Performance comparison visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# GLUE benchmark scores (simulated based on historical data)\n",
        "models = ['LSTM\\n(2017)', 'ELMo\\n(2018)', 'GPT\\n(2018)', 'BERT-Base\\n(2018)', 'BERT-Large\\n(2018)']\n",
        "glue_scores = [68.5, 74.3, 78.5, 82.1, 84.6]\n",
        "param_counts = [10, 94, 117, 110, 340]  # Million parameters\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "bars = plt.bar(models, glue_scores, color=['red', 'orange', 'yellow', 'lightgreen', 'green'])\n",
        "plt.title('GLUE Benchmark Progress (2017-2018)')\n",
        "plt.ylabel('GLUE Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add score labels on bars\n",
        "for bar, score in zip(bars, glue_scores):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
        "             f'{score}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.scatter(param_counts, glue_scores, s=200, c=['red', 'orange', 'yellow', 'lightgreen', 'green'], alpha=0.7)\n",
        "for i, model in enumerate(models):\n",
        "    plt.annotate(model.replace('\\n', ' '), (param_counts[i], glue_scores[i]), \n",
        "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "plt.xlabel('Parameters (Millions)')\n",
        "plt.ylabel('GLUE Score')\n",
        "plt.title('Performance vs Model Size')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Task performance comparison\n",
        "plt.subplot(2, 2, 3)\n",
        "tasks = ['Sentiment\\nAnalysis', 'Question\\nAnswering', 'Text\\nClassification', 'NER', 'Translation']\n",
        "pre_2018 = [78, 65, 75, 85, 28]  # BLEU for translation\n",
        "post_2018 = [93, 89, 91, 94, 35]\n",
        "\n",
        "x = np.arange(len(tasks))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, pre_2018, width, label='Pre-2018', alpha=0.7, color='lightcoral')\n",
        "plt.bar(x + width/2, post_2018, width, label='Post-2018', alpha=0.7, color='lightblue')\n",
        "\n",
        "plt.xlabel('NLP Tasks')\n",
        "plt.ylabel('Performance Score')\n",
        "plt.title('Task Performance: Before vs After 2018')\n",
        "plt.xticks(x, tasks)\n",
        "plt.legend()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Data efficiency comparison\n",
        "plt.subplot(2, 2, 4)\n",
        "training_sizes = [100, 500, 1000, 5000, 10000]\n",
        "traditional_acc = [45, 62, 71, 82, 87]\n",
        "transfer_acc = [78, 86, 89, 92, 94]\n",
        "\n",
        "plt.plot(training_sizes, traditional_acc, 'r-o', label='Traditional Training', linewidth=2)\n",
        "plt.plot(training_sizes, transfer_acc, 'b-s', label='Transfer Learning', linewidth=2)\n",
        "plt.xlabel('Training Examples')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Data Efficiency: Transfer Learning vs Traditional')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"2018: THE YEAR THAT CHANGED NLP FOREVER\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nKey Innovations:\")\n",
        "print(\"• ELMo: Contextualized word representations\")\n",
        "print(\"• ULMFiT: Transfer learning methodology for NLP\")\n",
        "print(\"• GPT: Transformer-based generative pretraining\")\n",
        "print(\"• BERT: Bidirectional encoder representations\")\n",
        "print(\"\\nImpact:\")\n",
        "print(\"• 10-20 point improvements across benchmarks\")\n",
        "print(\"• 10x reduction in labeled data requirements\")\n",
        "print(\"• Unified approach across diverse NLP tasks\")\n",
        "print(\"• Foundation for modern large language models\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercises_introduction"
      },
      "source": [
        "## Chapter 16 Exercises: Comprehensive Solutions\n",
        "\n",
        "This section provides detailed solutions to all exercises from Chapter 16, with comprehensive theoretical explanations, mathematical foundations, and practical implementations.\n",
        "\n",
        "### Exercise Overview\n",
        "\n",
        "The exercises cover:\n",
        "1. **Stateful vs Stateless RNNs** - Understanding the trade-offs\n",
        "2. **Encoder-Decoder vs Sequence-to-Sequence** - Architecture choices\n",
        "3. **Variable-length sequences** - Handling dynamic inputs/outputs\n",
        "4. **Beam Search** - Advanced decoding strategies\n",
        "5. **Attention mechanisms** - The attention revolution\n",
        "6. **Transformer architecture** - Multi-head attention importance\n",
        "7. **Sampled softmax** - Efficient training with large vocabularies\n",
        "8. **Embedded Reber grammars** - Sequence learning evaluation\n",
        "9. **Date format conversion** - Practical seq2seq application\n",
        "10. **Neural Machine Translation** - Complete implementation\n",
        "11. **Advanced language models** - Modern NLP applications\n",
        "\n",
        "Each exercise includes:\n",
        "- **Theoretical background** with mathematical formulations\n",
        "- **Complete implementation** with detailed comments\n",
        "- **Performance analysis** and optimization strategies\n",
        "- **Real-world applications** and extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_1_theory"
      },
      "source": [
        "## Exercise 1: Stateful vs Stateless RNNs\n",
        "\n",
        "### Question\n",
        "What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
        "\n",
        "### Theoretical Analysis\n",
        "\n",
        "#### Stateless RNNs\n",
        "\n",
        "**Mathematical Definition:**\n",
        "For each batch, hidden state is reset:\n",
        "$$h_0^{(b)} = \\mathbf{0} \\text{ for all batches } b$$\n",
        "$$h_t^{(b)} = f(x_t^{(b)}, h_{t-1}^{(b)})$$\n",
        "\n",
        "**Advantages:**\n",
        "1. **Simplicity**: Easy to implement and debug\n",
        "2. **Parallelization**: Batches can be processed independently\n",
        "3. **Flexibility**: Can handle variable sequence lengths easily\n",
        "4. **Shuffling**: Training data can be shuffled for better optimization\n",
        "5. **Memory efficiency**: No need to store states between batches\n",
        "\n",
        "**Disadvantages:**\n",
        "1. **Limited context**: Cannot learn patterns longer than sequence length\n",
        "2. **Information loss**: Useful context from previous sequences is discarded\n",
        "3. **Gradient limitations**: Gradients cannot flow beyond sequence boundaries\n",
        "\n",
        "#### Stateful RNNs\n",
        "\n",
        "**Mathematical Definition:**\n",
        "Hidden state carries over between batches:\n",
        "$$h_0^{(b)} = h_T^{(b-1)}$$\n",
        "$$h_t^{(b)} = f(x_t^{(b)}, h_{t-1}^{(b)})$$\n",
        "\n",
        "**Advantages:**\n",
        "1. **Long-term memory**: Can learn patterns spanning multiple sequences\n",
        "2. **Better context**: Maintains information across batch boundaries\n",
        "3. **Coherent generation**: More consistent text generation\n",
        "4. **Online learning**: Suitable for streaming data scenarios\n",
        "\n",
        "**Disadvantages:**\n",
        "1. **Complex implementation**: Requires careful state management\n",
        "2. **Fixed batch size**: Batch size must be consistent\n",
        "3. **Sequential processing**: Cannot parallelize across batches\n",
        "4. **No shuffling**: Data order must be preserved\n",
        "5. **Memory overhead**: Must store states between batches\n",
        "6. **Debugging difficulty**: Harder to isolate issues\n",
        "\n",
        "### When to Use Each Type\n",
        "\n",
        "**Use Stateless RNNs when:**\n",
        "- Sequences are independent (e.g., separate documents)\n",
        "- Computational efficiency is important\n",
        "- Sequence lengths vary significantly\n",
        "- Standard supervised learning setup\n",
        "\n",
        "**Use Stateful RNNs when:**\n",
        "- Long-term dependencies are crucial\n",
        "- Processing continuous streams (e.g., live transcription)\n",
        "- Text generation requiring consistency\n",
        "- Time series with very long patterns\n",
        "\n",
        "### Computational Complexity\n",
        "\n",
        "| Aspect | Stateless | Stateful |\n",
        "|--------|-----------|----------|\n",
        "| **Memory** | $O(B \\times T \\times H)$ | $O(B \\times T \\times H + B \\times H)$ |\n",
        "| **Computation** | $O(B \\times T \\times H^2)$ | $O(B \\times T \\times H^2)$ |\n",
        "| **Parallelization** | Full batch parallelization | Limited parallelization |\n",
        "| **Implementation** | Simple | Complex |\n",
        "\n",
        "Where $B$ = batch size, $T$ = sequence length, $H$ = hidden size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise_1_implementation"
      },
      "source": [
        "# Exercise 1: Comprehensive Comparison of Stateful vs Stateless RNNs\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "print(\"Exercise 1: Stateful vs Stateless RNN Comparison\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Generate synthetic data to demonstrate differences\n",
        "def generate_long_sequence_data(total_length=10000, pattern_length=50):\n",
        "    \"\"\"\n",
        "    Generate data with long-term patterns that span multiple sequence windows.\n",
        "    This will help demonstrate the advantage of stateful RNNs.\n",
        "    \"\"\"\n",
        "    # Create a repeating pattern\n",
        "    pattern = np.sin(np.linspace(0, 4*np.pi, pattern_length))\n",
        "    \n",
        "    # Repeat pattern and add noise\n",
        "    full_sequence = np.tile(pattern, total_length // pattern_length + 1)[:total_length]\n",
        "    full_sequence += np.random.normal(0, 0.1, total_length)\n",
        "    \n",
        "    return full_sequence.astype(np.float32)\n",
        "\n",
        "def create_windowed_dataset(data, window_size, batch_size, stateful=False):\n",
        "    \"\"\"\n",
        "    Create windowed dataset for RNN training.\n",
        "    \n",
        "    Args:\n",
        "        data: Input sequence\n",
        "        window_size: Size of each window\n",
        "        batch_size: Batch size\n",
        "        stateful: Whether to create dataset for stateful RNN\n",
        "    \n",
        "    Returns:\n",
        "        TensorFlow dataset\n",
        "    \"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "    \n",
        "    if stateful:\n",
        "        # For stateful RNN: non-overlapping windows, no shuffling\n",
        "        dataset = dataset.window(window_size + 1, shift=window_size, drop_remainder=True)\n",
        "    else:\n",
        "        # For stateless RNN: overlapping windows, can shuffle\n",
        "        dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    \n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "    \n",
        "    if not stateful:\n",
        "        dataset = dataset.shuffle(1000)\n",
        "    \n",
        "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Generate test data\n",
        "print(\"Generating synthetic data with long-term patterns...\")\n",
        "long_sequence = generate_long_sequence_data(total_length=5000, pattern_length=100)\n",
        "\n",
        "# Split into train/test\n",
        "train_data = long_sequence[:4000]\n",
        "test_data = long_sequence[4000:]\n",
        "\n",
        "print(f\"Train data length: {len(train_data)}\")\n",
        "print(f\"Test data length: {len(test_data)}\")\n",
        "\n",
        "# Visualize the pattern\n",
        "plt.figure(figsize=(15, 4))\n",
        "plt.plot(long_sequence[:500])\n",
        "plt.title('Synthetic Data with Long-term Patterns')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Model parameters\n",
        "window_size = 50\n",
        "batch_size = 32\n",
        "units = 64\n",
        "epochs = 10\n",
        "\n",
        "# Create datasets\n",
        "print(\"\\nCreating datasets...\")\n",
        "stateless_train = create_windowed_dataset(train_data, window_size, batch_size, stateful=False)\n",
        "stateless_test = create_windowed_dataset(test_data, window_size, batch_size, stateful=False)\n",
        "\n",
        "stateful_train = create_windowed_dataset(train_data, window_size, batch_size, stateful=True)\n",
        "stateful_test = create_windowed_dataset(test_data, window_size, batch_size, stateful=True)\n",
        "\n",
        "print(f\"Stateless train batches: {len(list(stateless_train))}\")\n",
        "print(f\"Stateful train batches: {len(list(stateful_train))}\")\n",
        "\n",
        "# Create models\n",
        "def create_stateless_model(window_size, units):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.LSTM(units, return_sequences=True, input_shape=[window_size, 1]),\n",
        "        keras.layers.LSTM(units, return_sequences=True),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def create_stateful_model(window_size, units, batch_size):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.LSTM(units, return_sequences=True, stateful=True,\n",
        "                         batch_input_shape=[batch_size, window_size, 1]),\n",
        "        keras.layers.LSTM(units, return_sequences=True, stateful=True),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# State reset callback for stateful model\n",
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.model.reset_states()\n",
        "\n",
        "print(\"\\nCreating and training models...\")\n",
        "\n",
        "# Stateless model\n",
        "print(\"Training stateless model...\")\n",
        "stateless_model = create_stateless_model(window_size, units)\n",
        "stateless_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "start_time = time.time()\n",
        "stateless_history = stateless_model.fit(\n",
        "    stateless_train.map(lambda x, y: (tf.expand_dims(x, -1), tf.expand_dims(y, -1))),\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    validation_data=stateless_test.map(lambda x, y: (tf.expand_dims(x, -1), tf.expand_dims(y, -1)))\n",
        ")\n",
        "stateless_time = time.time() - start_time\n",
        "\n",
        "# Stateful model\n",
        "print(\"\\nTraining stateful model...\")\n",
        "stateful_model = create_stateful_model(window_size, units, batch_size)\n",
        "stateful_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "start_time = time.time()\n",
        "stateful_history = stateful_model.fit(\n",
        "    stateful_train.map(lambda x, y: (tf.expand_dims(x, -1), tf.expand_dims(y, -1))),\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    callbacks=[ResetStatesCallback()],\n",
        "    validation_data=stateful_test.map(lambda x, y: (tf.expand_dims(x, -1), tf.expand_dims(y, -1)))\n",
        ")\n",
        "stateful_time = time.time() - start_time\n",
        "\n",
        "# Compare results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING RESULTS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "stateless_final_loss = stateless_history.history['loss'][-1]\n",
        "stateful_final_loss = stateful_history.history['loss'][-1]\n",
        "stateless_final_val_loss = stateless_history.history['val_loss'][-1]\n",
        "stateful_final_val_loss = stateful_history.history['val_loss'][-1]\n",
        "\n",
        "print(f\"\\nFinal Training Loss:\")\n",
        "print(f\"Stateless: {stateless_final_loss:.6f}\")\n",
        "print(f\"Stateful:  {stateful_final_loss:.6f}\")\n",
        "print(f\"Improvement: {((stateless_final_loss - stateful_final_loss) / stateless_final_loss * 100):+.2f}%\")\n",
        "\n",
        "print(f\"\\nFinal Validation Loss:\")\n",
        "print(f\"Stateless: {stateless_final_val_loss:.6f}\")\n",
        "print(f\"Stateful:  {stateful_final_val_loss:.6f}\")\n",
        "print(f\"Improvement: {((stateless_final_val_loss - stateful_final_val_loss) / stateless_final_val_loss * 100):+.2f}%\")\n",
        "\n",
        "print(f\"\\nTraining Time:\")\n",
        "print(f\"Stateless: {stateless_time:.2f} seconds\")\n",
        "print(f\"Stateful:  {stateful_time:.2f} seconds\")\n",
        "print(f\"Time difference: {((stateful_time - stateless_time) / stateless_time * 100):+.2f}%\")\n",
        "\n",
        "# Visualize training progress\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(stateless_history.history['loss'], label='Stateless', linewidth=2)\n",
        "plt.plot(stateful_history.history['loss'], label='Stateful', linewidth=2)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.plot(stateless_history.history['val_loss'], label='Stateless', linewidth=2)\n",
        "plt.plot(stateful_history.history['val_loss'], label='Stateful', linewidth=2)\n",
        "plt.title('Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.plot(stateless_history.history['mae'], label='Stateless', linewidth=2)\n",
        "plt.plot(stateful_history.history['mae'], label='Stateful', linewidth=2)\n",
        "plt.title('Mean Absolute Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Generate predictions for comparison\n",
        "test_input = test_data[:window_size].reshape(1, window_size, 1)\n",
        "\n",
        "# Create stateless version of stateful model for prediction\n",
        "stateful_pred_model = create_stateless_model(window_size, units)\n",
        "stateful_pred_model.set_weights(stateful_model.get_weights())\n",
        "\n",
        "# Generate sequences\n",
        "def generate_sequence(model, initial_input, length):\n",
        "    generated = []\n",
        "    current_input = initial_input.copy()\n",
        "    \n",
        "    for _ in range(length):\n",
        "        pred = model.predict(current_input, verbose=0)\n",
        "        next_val = pred[0, -1, 0]\n",
        "        generated.append(next_val)\n",
        "        \n",
        "        # Update input\n",
        "        current_input = np.roll(current_input, -1, axis=1)\n",
        "        current_input[0, -1, 0] = next_val\n",
        "    \n",
        "    return np.array(generated)\n",
        "\n",
        "# Generate predictions\n",
        "pred_length = 200\n",
        "stateless_pred = generate_sequence(stateless_model, test_input, pred_length)\n",
        "stateful_pred = generate_sequence(stateful_pred_model, test_input, pred_length)\n",
        "true_sequence = test_data[window_size:window_size + pred_length]\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.plot(true_sequence, label='True', linewidth=2, alpha=0.8)\n",
        "plt.plot(stateless_pred, label='Stateless Pred', linewidth=1.5, alpha=0.8)\n",
        "plt.plot(stateful_pred, label='Stateful Pred', linewidth=1.5, alpha=0.8)\n",
        "plt.title('Sequence Generation Comparison')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Error analysis\n",
        "stateless_error = np.abs(stateless_pred - true_sequence)\n",
        "stateful_error = np.abs(stateful_pred - true_sequence)\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.plot(stateless_error, label='Stateless Error', linewidth=2)\n",
        "plt.plot(stateful_error, label='Stateful Error', linewidth=2)\n",
        "plt.title('Absolute Error Over Time')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Absolute Error')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Summary statistics\n",
        "plt.subplot(2, 3, 6)\n",
        "metrics = ['MAE', 'RMSE', 'Max Error']\n",
        "stateless_metrics = [\n",
        "    np.mean(stateless_error),\n",
        "    np.sqrt(np.mean(stateless_error**2)),\n",
        "    np.max(stateless_error)\n",
        "]\n",
        "stateful_metrics = [\n",
        "    np.mean(stateful_error),\n",
        "    np.sqrt(np.mean(stateful_error**2)),\n",
        "    np.max(stateful_error)\n",
        "]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, stateless_metrics, width, label='Stateless', alpha=0.8)\n",
        "plt.bar(x + width/2, stateful_metrics, width, label='Stateful', alpha=0.8)\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Error')\n",
        "plt.title('Error Metrics Comparison')\n",
        "plt.xticks(x, metrics)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXERCISE 1 CONCLUSIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n1. PERFORMANCE COMPARISON:\")\n",
        "print(f\"   • Stateful model achieved {((stateless_final_val_loss - stateful_final_val_loss) / stateless_final_val_loss * 100):.2f}% better validation loss\")\n",
        "print(f\"   • Stateful model shows {((np.mean(stateless_error) - np.mean(stateful_error)) / np.mean(stateless_error) * 100):.2f}% lower prediction error\")\n",
        "print(f\"   • Training time difference: {((stateful_time - stateless_time) / stateless_time * 100):+.1f}%\")\n",
        "\n",
        "print(f\"\\n2. WHEN TO USE STATEFUL RNNs:\")\n",
        "print(f\"   • Long sequences with patterns spanning multiple windows\")\n",
        "print(f\"   • Continuous data streams (time series, audio, text)\")\n",
        "print(f\"   • Text generation requiring long-term coherence\")\n",
        "print(f\"   • When computational overhead is acceptable\")\n",
        "\n",
        "print(f\"\\n3. WHEN TO USE STATELESS RNNs:\")\n",
        "print(f\"   • Independent sequences (different documents/conversations)\")\n",
        "print(f\"   • Need for data shuffling and parallel processing\")\n",
        "print(f\"   • Variable sequence lengths\")\n",
        "print(f\"   • Simpler implementation and debugging requirements\")\n",
        "\n",
        "print(f\"\\n4. KEY TRADE-OFFS:\")\n",
        "print(f\"   • Performance vs Complexity\")\n",
        "print(f\"   • Memory efficiency vs Long-term learning\")\n",
        "print(f\"   • Parallelization vs Sequential dependencies\")\n",
        "print(f\"   • Flexibility vs Specialized optimization\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_2_theory"
      },
      "source": [
        "## Exercise 2: Encoder-Decoder vs Plain Sequence-to-Sequence RNNs\n",
        "\n",
        "### Question\n",
        "Why do people use Encoder-Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
        "\n",
        "### Theoretical Analysis\n",
        "\n",
        "#### Plain Sequence-to-Sequence RNNs\n",
        "\n",
        "**Architecture:**\n",
        "Single RNN that processes source and target sequences consecutively:\n",
        "$$h_t = f(x_t, h_{t-1}) \\quad \\text{for } t = 1, ..., T_{src} + T_{tgt}$$\n",
        "\n",
        "Where:\n",
        "- $x_1, ..., x_{T_{src}}$: source tokens\n",
        "- $x_{T_{src}+1}, ..., x_{T_{src}+T_{tgt}}$: target tokens\n",
        "\n",
        "**Problems:**\n",
        "1. **Fixed input-output alignment**: Assumes one-to-one token correspondence\n",
        "2. **Early prediction**: Must start generating before seeing entire source\n",
        "3. **Language mixing**: Source and target share same hidden state\n",
        "4. **Limited flexibility**: Cannot handle different input/output lengths well\n",
        "\n",
        "#### Encoder-Decoder Architecture\n",
        "\n",
        "**Two-stage process:**\n",
        "\n",
        "**Encoder:**\n",
        "$$h_t^{(e)} = f_{enc}(x_t^{(src)}, h_{t-1}^{(e)})$$\n",
        "$$c = g(h_1^{(e)}, h_2^{(e)}, ..., h_{T_{src}}^{(e)})$$\n",
        "\n",
        "**Decoder:**\n",
        "$$h_t^{(d)} = f_{dec}(y_{t-1}^{(tgt)}, h_{t-1}^{(d)}, c)$$\n",
        "$$P(y_t | y_{<t}, x) = \\text{softmax}(W h_t^{(d)} + b)$$\n",
        "\n",
        "### Key Advantages of Encoder-Decoder\n",
        "\n",
        "#### 1. Complete Source Processing\n",
        "- **Full context**: Encoder sees entire source before decoding starts\n",
        "- **Better representations**: Can build rich source understanding\n",
        "- **Bidirectional information**: Can use bidirectional encoders\n",
        "\n",
        "#### 2. Variable Length Handling\n",
        "**Mathematical flexibility:**\n",
        "$$\\text{len}(\\text{source}) \\neq \\text{len}(\\text{target})$$\n",
        "\n",
        "Examples:\n",
        "- English: \"I love you\" (3 tokens)\n",
        "- German: \"Ich liebe dich\" (3 tokens)\n",
        "- Japanese: \"愛してる\" (1 token)\n",
        "- Finnish: \"Minä rakastan sinua\" (3 tokens)\n",
        "\n",
        "#### 3. Language Separation\n",
        "- **Specialized parameters**: Different parameters for source/target languages\n",
        "- **Language-specific optimization**: Each component optimized for its role\n",
        "- **Reduced interference**: Source and target processing don't interfere\n",
        "\n",
        "#### 4. Attention Compatibility\n",
        "- **Multiple source representations**: Encoder provides multiple hidden states\n",
        "- **Attention mechanism**: Decoder can attend to different source parts\n",
        "- **Alignment learning**: Model learns source-target alignments\n",
        "\n",
        "### Computational Complexity Comparison\n",
        "\n",
        "| Architecture | Parameters | Computation | Memory |\n",
        "|--------------|------------|-------------|--------|\n",
        "| **Plain Seq2Seq** | $O(V \\times H + H^2)$ | $O((T_s + T_t) \\times H^2)$ | $O(H)$ |\n",
        "| **Encoder-Decoder** | $O(2V \\times H + 2H^2)$ | $O(T_s \\times H^2 + T_t \\times H^2)$ | $O(T_s \\times H)$ |\n",
        "| **With Attention** | $O(2V \\times H + 2H^2 + A)$ | $O(T_s \\times H^2 + T_t \\times T_s \\times H)$ | $O(T_s \\times H)$ |\n",
        "\n",
        "Where:\n",
        "- $V$: vocabulary size\n",
        "- $H$: hidden size\n",
        "- $T_s, T_t$: source/target sequence lengths\n",
        "- $A$: attention parameters\n",
        "\n",
        "### Empirical Evidence\n",
        "\n",
        "**BLEU Score Improvements:**\n",
        "- Plain Seq2Seq: ~15-20 BLEU\n",
        "- Encoder-Decoder: ~25-30 BLEU\n",
        "- With Attention: ~35-40 BLEU\n",
        "- Transformer: ~40-45 BLEU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise_2_implementation"
      },
      "source": [
        "# Exercise 2: Encoder-Decoder vs Plain Sequence-to-Sequence Comparison\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "print(\"Exercise 2: Encoder-Decoder vs Plain Sequence-to-Sequence RNNs\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create synthetic translation data to demonstrate differences\n",
        "def create_translation_data():\n",
        "    \"\"\"\n",
        "    Create synthetic translation pairs with different characteristics:\n",
        "    1. Different lengths\n",
        "    2. Word reordering\n",
        "    3. Complex mappings\n",
        "    \"\"\"\n",
        "    # Simple number to word translation (simulates different languages)\n",
        "    source_sentences = []\n",
        "    target_sentences = []\n",
        "    \n",
        "    # Pattern 1: Different lengths\n",
        "    numbers = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
        "    target_numbers = ['null', 'eins', 'zwei', 'drei', 'vier', 'funf', 'sechs', 'sieben', 'acht', 'neun']\n",
        "    \n",
        "    for i in range(100):\n",
        "        # Random length sequences (1-4 numbers)\n",
        "        length = np.random.randint(1, 5)\n",
        "        src_nums = np.random.choice(numbers, length, replace=True)\n",
        "        tgt_nums = [target_numbers[numbers.index(num)] for num in src_nums]\n",
        "        \n",
        "        # Sometimes reverse order (different word order)\n",
        "        if np.random.random() < 0.3:\n",
        "            tgt_nums = tgt_nums[::-1]\n",
        "        \n",
        "        source_sentences.append(' '.join(src_nums))\n",
        "        target_sentences.append(' '.join(tgt_nums))\n",
        "    \n",
        "    # Pattern 2: Complex mappings (compound words)\n",
        "    compounds = {\n",
        "        'big red': 'grossrot',\n",
        "        'small blue': 'kleinblau',\n",
        "        'fast car': 'schnellauto',\n",
        "        'slow train': 'langsamzug'\n",
        "    }\n",
        "    \n",
        "    for src, tgt in compounds.items():\n",
        "        for _ in range(10):\n",
        "            source_sentences.append(src)\n",
        "            target_sentences.append(tgt)\n",
        "    \n",
        "    return source_sentences, target_sentences\n",
        "\n",
        "def create_vocabularies(source_sentences, target_sentences):\n",
        "    \"\"\"\n",
        "    Create vocabulary mappings for source and target languages.\n",
        "    \"\"\"\n",
        "    # Combine all words\n",
        "    source_words = []\n",
        "    target_words = []\n",
        "    \n",
        "    for sent in source_sentences:\n",
        "        source_words.extend(sent.split())\n",
        "    \n",
        "    for sent in target_sentences:\n",
        "        target_words.extend(sent.split())\n",
        "    \n",
        "    # Create vocabularies\n",
        "    source_vocab = ['<PAD>', '<START>', '<END>'] + list(set(source_words))\n",
        "    target_vocab = ['<PAD>', '<START>', '<END>'] + list(set(target_words))\n",
        "    \n",
        "    # Create word-to-index mappings\n",
        "    source_word_to_idx = {word: idx for idx, word in enumerate(source_vocab)}\n",
        "    target_word_to_idx = {word: idx for idx, word in enumerate(target_vocab)}\n",
        "    \n",
        "    return (source_vocab, target_vocab, source_word_to_idx, target_word_to_idx)\n",
        "\n",
        "def encode_sentences(sentences, word_to_idx, max_length=10, add_start_end=False):\n",
        "    \"\"\"\n",
        "    Convert sentences to sequences of indices.\n",
        "    \"\"\"\n",
        "    encoded = []\n",
        "    \n",
        "    for sent in sentences:\n",
        "        words = sent.split()\n",
        "        \n",
        "        if add_start_end:\n",
        "            words = ['<START>'] + words + ['<END>']\n",
        "        \n",
        "        # Convert to indices\n",
        "        indices = [word_to_idx.get(word, 0) for word in words]  # 0 for unknown\n",
        "        \n",
        "        # Pad or truncate\n",
        "        if len(indices) < max_length:\n",
        "            indices.extend([0] * (max_length - len(indices)))  # Pad with 0\n",
        "        else:\n",
        "            indices = indices[:max_length]\n",
        "        \n",
        "        encoded.append(indices)\n",
        "    \n",
        "    return np.array(encoded)\n",
        "\n",
        "# Generate data\n",
        "print(\"Generating synthetic translation data...\")\n",
        "source_sentences, target_sentences = create_translation_data()\n",
        "\n",
        "print(f\"Generated {len(source_sentences)} sentence pairs\")\n",
        "print(\"\\nExample translations:\")\n",
        "for i in range(5):\n",
        "    print(f\"  {source_sentences[i]} -> {target_sentences[i]}\")\n",
        "\n",
        "# Create vocabularies\n",
        "source_vocab, target_vocab, source_word_to_idx, target_word_to_idx = create_vocabularies(\n",
        "    source_sentences, target_sentences\n",
        ")\n",
        "\n",
        "print(f\"\\nVocabulary sizes:\")\n",
        "print(f\"Source: {len(source_vocab)} words\")\n",
        "print(f\"Target: {len(target_vocab)} words\")\n",
        "\n",
        "# Encode sentences\n",
        "max_length = 8\n",
        "encoded_sources = encode_sentences(source_sentences, source_word_to_idx, max_length)\n",
        "encoded_targets = encode_sentences(target_sentences, target_word_to_idx, max_length, add_start_end=True)\n",
        "\n",
        "print(f\"\\nEncoded shapes:\")\n",
        "print(f\"Sources: {encoded_sources.shape}\")\n",
        "print(f\"Targets: {encoded_targets.shape}\")\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * len(encoded_sources))\n",
        "train_sources = encoded_sources[:train_size]\n",
        "train_targets = encoded_targets[:train_size]\n",
        "val_sources = encoded_sources[train_size:]\n",
        "val_targets = encoded_targets[train_size:]\n",
        "\n",
        "print(f\"\\nData split:\")\n",
        "print(f\"Training: {len(train_sources)} pairs\")\n",
        "print(f\"Validation: {len(val_sources)} pairs\")\n",
        "\n",
        "# Model 1: Plain Sequence-to-Sequence\n",
        "class PlainSeq2Seq(keras.Model):\n",
        "    \"\"\"\n",
        "    Plain sequence-to-sequence model.\n",
        "    Processes source and target in one continuous sequence.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_units = hidden_units\n",
        "        \n",
        "        # Shared components\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "        self.output_layer = keras.layers.Dense(vocab_size, activation='softmax')\n",
        "    \n",
        "    def call(self, inputs, training=None):\n",
        "        source, target_input = inputs\n",
        "        \n",
        "        # Concatenate source and target\n",
        "        # Note: This is a simplified version - real implementation would be more complex\n",
        "        combined_input = tf.concat([source, target_input], axis=1)\n",
        "        \n",
        "        # Embed and process\n",
        "        embedded = self.embedding(combined_input)\n",
        "        rnn_output, _, _ = self.rnn(embedded)\n",
        "        \n",
        "        # Only predict on target portion\n",
        "        target_portion = rnn_output[:, source.shape[1]:, :]\n",
        "        output = self.output_layer(target_portion)\n",
        "        \n",
        "        return output\n",
        "\n",
        "# Model 2: Encoder-Decoder\n",
        "class EncoderDecoder(keras.Model):\n",
        "    \"\"\"\n",
        "    Proper Encoder-Decoder model.\n",
        "    Separate encoder and decoder with distinct parameters.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, source_vocab_size, target_vocab_size, embedding_dim, hidden_units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        # Encoder components\n",
        "        self.encoder_embedding = keras.layers.Embedding(source_vocab_size, embedding_dim)\n",
        "        self.encoder_rnn = keras.layers.LSTM(hidden_units, return_state=True)\n",
        "        \n",
        "        # Decoder components\n",
        "        self.decoder_embedding = keras.layers.Embedding(target_vocab_size, embedding_dim)\n",
        "        self.decoder_rnn = keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "        self.output_layer = keras.layers.Dense(target_vocab_size, activation='softmax')\n",
        "    \n",
        "    def call(self, inputs, training=None):\n",
        "        source, target_input = inputs\n",
        "        \n",
        "        # Encoder\n",
        "        encoder_embedded = self.encoder_embedding(source)\n",
        "        _, encoder_h, encoder_c = self.encoder_rnn(encoder_embedded)\n",
        "        encoder_states = [encoder_h, encoder_c]\n",
        "        \n",
        "        # Decoder\n",
        "        decoder_embedded = self.decoder_embedding(target_input)\n",
        "        decoder_output, _, _ = self.decoder_rnn(\n",
        "            decoder_embedded, initial_state=encoder_states\n",
        "        )\n",
        "        \n",
        "        output = self.output_layer(decoder_output)\n",
        "        return output\n",
        "\n",
        "# Create models\n",
        "embedding_dim = 64\n",
        "hidden_units = 128\n",
        "max_vocab_size = max(len(source_vocab), len(target_vocab))\n",
        "\n",
        "print(\"\\nCreating models...\")\n",
        "\n",
        "# Plain Seq2Seq model\n",
        "plain_model = PlainSeq2Seq(\n",
        "    vocab_size=max_vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_units=hidden_units\n",
        ")\n",
        "\n",
        "# Encoder-Decoder model\n",
        "enc_dec_model = EncoderDecoder(\n",
        "    source_vocab_size=len(source_vocab),\n",
        "    target_vocab_size=len(target_vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_units=hidden_units\n",
        ")\n",
        "\n",
        "# Compile models\n",
        "plain_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "enc_dec_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Prepare training data\n",
        "train_decoder_input = train_targets[:, :-1]  # Remove last token\n",
        "train_decoder_output = train_targets[:, 1:]  # Remove first token\n",
        "\n",
        "val_decoder_input = val_targets[:, :-1]\n",
        "val_decoder_output = val_targets[:, 1:]\n",
        "\n",
        "print(f\"\\nTraining data shapes:\")\n",
        "print(f\"Source: {train_sources.shape}\")\n",
        "print(f\"Decoder input: {train_decoder_input.shape}\")\n",
        "print(f\"Decoder output: {train_decoder_output.shape}\")\n",
        "\n",
        "# Train models\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "print(\"\\nTraining Plain Sequence-to-Sequence model...\")\n",
        "plain_history = plain_model.fit(\n",
        "    [train_sources, train_decoder_input],\n",
        "    train_decoder_output,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=([val_sources, val_decoder_input], val_decoder_output),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Encoder-Decoder model...\")\n",
        "enc_dec_history = enc_dec_model.fit(\n",
        "    [train_sources, train_decoder_input],\n",
        "    train_decoder_output,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=([val_sources, val_decoder_input], val_decoder_output),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Compare results\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(plain_history.history['loss'], label='Plain Seq2Seq', linewidth=2)\n",
        "plt.plot(enc_dec_history.history['loss'], label='Encoder-Decoder', linewidth=2)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.plot(plain_history.history['val_loss'], label='Plain Seq2Seq', linewidth=2)\n",
        "plt.plot(enc_dec_history.history['val_loss'], label='Encoder-Decoder', linewidth=2)\n",
        "plt.title('Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.plot(plain_history.history['accuracy'], label='Plain Seq2Seq', linewidth=2)\n",
        "plt.plot(enc_dec_history.history['accuracy'], label='Encoder-Decoder', linewidth=2)\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.plot(plain_history.history['val_accuracy'], label='Plain Seq2Seq', linewidth=2)\n",
        "plt.plot(enc_dec_history.history['val_accuracy'], label='Encoder-Decoder', linewidth=2)\n",
        "plt.title('Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Model complexity comparison\n",
        "plt.subplot(2, 3, 5)\n",
        "plain_params = plain_model.count_params()\n",
        "enc_dec_params = enc_dec_model.count_params()\n",
        "\n",
        "models = ['Plain Seq2Seq', 'Encoder-Decoder']\n",
        "param_counts = [plain_params, enc_dec_params]\n",
        "\n",
        "bars = plt.bar(models, param_counts, color=['lightcoral', 'lightblue'], alpha=0.8)\n",
        "plt.title('Model Parameter Count')\n",
        "plt.ylabel('Parameters')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "for bar, count in zip(bars, param_counts):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000, \n",
        "             f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Performance summary\n",
        "plt.subplot(2, 3, 6)\n",
        "metrics = ['Final Train Acc', 'Final Val Acc', 'Final Val Loss']\n",
        "plain_metrics = [\n",
        "    plain_history.history['accuracy'][-1],\n",
        "    plain_history.history['val_accuracy'][-1],\n",
        "    plain_history.history['val_loss'][-1]\n",
        "]\n",
        "enc_dec_metrics = [\n",
        "    enc_dec_history.history['accuracy'][-1],\n",
        "    enc_dec_history.history['val_accuracy'][-1],\n",
        "    enc_dec_history.history['val_loss'][-1]\n",
        "]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, plain_metrics, width, label='Plain Seq2Seq', alpha=0.8, color='lightcoral')\n",
        "plt.bar(x + width/2, enc_dec_metrics, width, label='Encoder-Decoder', alpha=0.8, color='lightblue')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Final Performance Metrics')\n",
        "plt.xticks(x, metrics, rotation=45)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analysis and conclusions\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXERCISE 2 RESULTS AND ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "plain_final_acc = plain_history.history['val_accuracy'][-1]\n",
        "enc_dec_final_acc = enc_dec_history.history['val_accuracy'][-1]\n",
        "plain_final_loss = plain_history.history['val_loss'][-1]\n",
        "enc_dec_final_loss = enc_dec_history.history['val_loss'][-1]\n",
        "\n",
        "print(f\"\\n1. PERFORMANCE COMPARISON:\")\n",
        "print(f\"   • Plain Seq2Seq final validation accuracy: {plain_final_acc:.4f}\")\n",
        "print(f\"   • Encoder-Decoder final validation accuracy: {enc_dec_final_acc:.4f}\")\n",
        "print(f\"   • Improvement: {((enc_dec_final_acc - plain_final_acc) / plain_final_acc * 100):+.2f}%\")\n",
        "\n",
        "print(f\"\\n2. MODEL COMPLEXITY:\")\n",
        "print(f\"   • Plain Seq2Seq parameters: {plain_params:,}\")\n",
        "print(f\"   • Encoder-Decoder parameters: {enc_dec_params:,}\")\n",
        "print(f\"   • Parameter increase: {((enc_dec_params - plain_params) / plain_params * 100):+.1f}%\")\n",
        "\n",
        "print(f\"\\n3. WHY ENCODER-DECODER IS BETTER:\")\n",
        "print(f\"   • Complete source processing before decoding\")\n",
        "print(f\"   • Separate parameters for source and target languages\")\n",
        "print(f\"   • Better handling of variable-length sequences\")\n",
        "print(f\"   • Compatibility with attention mechanisms\")\n",
        "print(f\"   • Reduced interference between source and target\")\n",
        "\n",
        "print(f\"\\n4. ARCHITECTURAL ADVANTAGES:\")\n",
        "print(f\"   • Encoder builds rich source representation\")\n",
        "print(f\"   • Decoder focuses solely on target generation\")\n",
        "print(f\"   • Natural separation of concerns\")\n",
        "print(f\"   • Extensible to attention and transformer architectures\")\n",
        "\n",
        "print(f\"\\n5. PRACTICAL BENEFITS:\")\n",
        "print(f\"   • Better translation quality\")\n",
        "print(f\"   • More stable training\")\n",
        "print(f\"   • Easier to debug and analyze\")\n",
        "print(f\"   • Foundation for modern NMT systems\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_3_theory"
      },
      "source": [
        "## Exercise 3: Variable-Length Input and Output Sequences\n",
        "\n",
        "### Question\n",
        "How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
        "\n",
        "### Theoretical Framework\n",
        "\n",
        "Variable-length sequences are ubiquitous in real-world NLP:\n",
        "- **Documents**: 10 words to 10,000+ words\n",
        "- **Sentences**: 1 word to 100+ words  \n",
        "- **Translations**: Length ratios vary by language pair\n",
        "- **Summaries**: Compression ratios from 10:1 to 100:1\n",
        "\n",
        "### Variable-Length Input Sequences\n",
        "\n",
        "#### 1. Padding Strategy\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "Given sequences $\\{x^{(i)}\\}_{i=1}^N$ with lengths $\\{L_i\\}_{i=1}^N$:\n",
        "\n",
        "$$\\tilde{x}^{(i)} = \\begin{cases}\n",
        "[x^{(i)}_1, x^{(i)}_2, ..., x^{(i)}_{L_i}, 0, 0, ..., 0] & \\text{if } L_i < L_{\\max} \\\\\n",
        "[x^{(i)}_1, x^{(i)}_2, ..., x^{(i)}_{L_{\\max}}] & \\text{if } L_i \\geq L_{\\max}\n",
        "\\end{cases}$$\n",
        "\n",
        "**Padding Types:**\n",
        "- **Post-padding**: Add zeros at the end\n",
        "- **Pre-padding**: Add zeros at the beginning\n",
        "- **Masking**: Use special mask tokens\n",
        "\n",
        "**Advantages:**\n",
        "- Simple implementation\n",
        "- Works with standard tensor operations\n",
        "- Compatible with batching\n",
        "\n",
        "**Disadvantages:**\n",
        "- Memory waste for very different lengths\n",
        "- Computational overhead on padding\n",
        "- May affect model performance\n",
        "\n",
        "#### 2. Masking Strategy\n",
        "\n",
        "**Attention Masking:**\n",
        "$$\\text{mask}[i,j] = \\begin{cases}\n",
        "0 & \\text{if position } j \\text{ is valid} \\\\\n",
        "-\\infty & \\text{if position } j \\text{ is padding}\n",
        "\\end{cases}$$\n",
        "\n",
        "**Loss Masking:**\n",
        "$$L = \\frac{\\sum_{i=1}^N \\sum_{t=1}^{L_i} \\ell(y_t^{(i)}, \\hat{y}_t^{(i)})}{\\sum_{i=1}^N L_i}$$\n",
        "\n",
        "#### 3. Bucketing Strategy\n",
        "\n",
        "**Length-Based Grouping:**\n",
        "$$\\text{Bucket}_k = \\{x^{(i)} : L_{k-1} < L_i \\leq L_k\\}$$\n",
        "\n",
        "**Benefits:**\n",
        "- Reduced padding overhead\n",
        "- More efficient computation\n",
        "- Better GPU utilization\n",
        "\n",
        "#### 4. Dynamic Batching\n",
        "\n",
        "**Adaptive Batch Sizes:**\n",
        "$$\\text{batch_size}(L) = \\lfloor \\frac{\\text{max_tokens}}{L} \\rfloor$$\n",
        "\n",
        "### Variable-Length Output Sequences\n",
        "\n",
        "#### 1. Maximum Length Approach\n",
        "\n",
        "**Training:**\n",
        "- Set maximum output length $T_{\\max}$\n",
        "- Pad/truncate target sequences\n",
        "- Use teacher forcing\n",
        "\n",
        "**Inference:**\n",
        "- Generate up to $T_{\\max}$ tokens\n",
        "- Stop at end-of-sequence token\n",
        "\n",
        "#### 2. End-of-Sequence Tokens\n",
        "\n",
        "**Special Token Strategy:**\n",
        "$$\\text{output} = [y_1, y_2, ..., y_T, \\text{<EOS>}]$$\n",
        "\n",
        "**Stopping Criterion:**\n",
        "$$\\text{stop when } \\hat{y}_t = \\text{<EOS>} \\text{ or } t > T_{\\max}$$\n",
        "\n",
        "#### 3. Beam Search with Length Normalization\n",
        "\n",
        "**Length-Normalized Scoring:**\n",
        "$$\\text{score}(Y) = \\frac{\\log P(Y)}{|Y|^\\alpha}$$\n",
        "\n",
        "Where $\\alpha \\in [0, 1]$ controls length preference.\n",
        "\n",
        "### Advanced Techniques\n",
        "\n",
        "#### 1. Sequence Packing\n",
        "\n",
        "**Efficient Packing:**\n",
        "Combine multiple short sequences into one batch element:\n",
        "$$\\text{packed} = [\\text{seq}_1, \\text{<SEP>}, \\text{seq}_2, \\text{<SEP>}, ...]$$\n",
        "\n",
        "#### 2. Attention Pooling\n",
        "\n",
        "**Length-Invariant Representations:**\n",
        "$$\\mathbf{h}_{\\text{pooled}} = \\sum_{t=1}^T \\alpha_t \\mathbf{h}_t$$\n",
        "\n",
        "Where $\\alpha_t = \\text{softmax}(\\mathbf{w}^T \\mathbf{h}_t)$\n",
        "\n",
        "#### 3. Hierarchical Processing\n",
        "\n",
        "**Multi-Scale Approach:**\n",
        "- Process in chunks/segments\n",
        "- Combine chunk representations\n",
        "- Handle very long sequences\n",
        "\n",
        "### Implementation Considerations\n",
        "\n",
        "| Method | Memory | Computation | Complexity | Performance |\n",
        "|--------|--------|-------------|------------|-------------|\n",
        "| **Padding** | $O(B \\times L_{\\max})$ | $O(B \\times L_{\\max})$ | Low | Good |\n",
        "| **Masking** | $O(B \\times L_{\\max})$ | $O(B \\times L_{\\text{avg}})$ | Medium | Better |\n",
        "| **Bucketing** | $O(B \\times L_{\\text{bucket}})$ | $O(B \\times L_{\\text{bucket}})$ | High | Best |\n",
        "| **Dynamic** | $O(\\text{tokens})$ | $O(\\text{tokens})$ | Very High | Optimal |\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Choose appropriate maximum length** based on data distribution\n",
        "2. **Use masking** to ignore padding in loss and attention\n",
        "3. **Consider bucketing** for large length variations\n",
        "4. **Implement early stopping** with EOS tokens\n",
        "5. **Monitor length distributions** in your data\n",
        "6. **Use length normalization** in beam search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise_3_implementation"
      },
      "source": [
        "# Exercise 3: Comprehensive Variable-Length Sequence Handling\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "print(\"Exercise 3: Variable-Length Input and Output Sequences\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Generate variable-length sequence data\n",
        "def generate_variable_length_data(num_samples=1000, min_length=5, max_length=50):\n",
        "    \"\"\"\n",
        "    Generate synthetic data with variable lengths to demonstrate different strategies.\n",
        "    Task: Sequence reversal (input: [1,2,3], output: [3,2,1])\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    lengths = []\n",
        "    \n",
        "    for _ in range(num_samples):\n",
        "        # Random length\n",
        "        length = np.random.randint(min_length, max_length + 1)\n",
        "        \n",
        "        # Random sequence\n",
        "        seq = np.random.randint(1, 20, size=length)  # Vocabulary: 1-19, 0 reserved for padding\n",
        "        target = seq[::-1]  # Reverse sequence\n",
        "        \n",
        "        sequences.append(seq)\n",
        "        targets.append(target)\n",
        "        lengths.append(length)\n",
        "    \n",
        "    return sequences, targets, lengths\n",
        "\n",
        "# Strategy 1: Simple Padding\n",
        "def pad_sequences_simple(sequences, max_length=None, padding_value=0):\n",
        "    \"\"\"\n",
        "    Simple padding strategy - pad all sequences to same length.\n",
        "    \"\"\"\n",
        "    if max_length is None:\n",
        "        max_length = max(len(seq) for seq in sequences)\n",
        "    \n",
        "    padded = []\n",
        "    for seq in sequences:\n",
        "        if len(seq) < max_length:\n",
        "            # Post-padding\n",
        "            padded_seq = np.concatenate([seq, [padding_value] * (max_length - len(seq))])\n",
        "        else:\n",
        "            # Truncation\n",
        "            padded_seq = seq[:max_length]\n",
        "        padded.append(padded_seq)\n",
        "    \n",
        "    return np.array(padded)\n",
        "\n",
        "# Strategy 2: Bucketing\n",
        "def create_buckets(sequences, targets, lengths, bucket_boundaries):\n",
        "    \"\"\"\n",
        "    Group sequences into buckets based on length.\n",
        "    \"\"\"\n",
        "    buckets = defaultdict(list)\n",
        "    \n",
        "    for seq, tgt, length in zip(sequences, targets, lengths):\n",
        "        # Find appropriate bucket\n",
        "        bucket_id = 0\n",
        "        for boundary in bucket_boundaries:\n",
        "            if length <= boundary:\n",
        "                break\n",
        "            bucket_id += 1\n",
        "        \n",
        "        buckets[bucket_id].append((seq, tgt, length))\n",
        "    \n",
        "    return buckets\n",
        "\n",
        "def pad_bucket(bucket_data, padding_value=0):\n",
        "    \"\"\"\n",
        "    Pad sequences within a bucket to the maximum length in that bucket.\n",
        "    \"\"\"\n",
        "    if not bucket_data:\n",
        "        return [], [], []\n",
        "    \n",
        "    sequences, targets, lengths = zip(*bucket_data)\n",
        "    max_length = max(lengths)\n",
        "    \n",
        "    padded_sequences = pad_sequences_simple(sequences, max_length, padding_value)\n",
        "    padded_targets = pad_sequences_simple(targets, max_length, padding_value)\n",
        "    \n",
        "    return padded_sequences, padded_targets, list(lengths)\n",
        "\n",
        "# Strategy 3: Dynamic Batching\n",
        "class DynamicBatchDataset:\n",
        "    \"\"\"\n",
        "    Dataset that creates batches with similar sequence lengths.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, sequences, targets, max_tokens_per_batch=1000):\n",
        "        self.data = list(zip(sequences, targets))\n",
        "        self.max_tokens_per_batch = max_tokens_per_batch\n",
        "        \n",
        "        # Sort by length for efficient batching\n",
        "        self.data.sort(key=lambda x: len(x[0]))\n",
        "    \n",
        "    def create_batches(self):\n",
        "        batches = []\n",
        "        current_batch = []\n",
        "        current_tokens = 0\n",
        "        \n",
        "        for seq, tgt in self.data:\n",
        "            seq_len = len(seq)\n",
        "            \n",
        "            # Check if adding this sequence would exceed token limit\n",
        "            if current_batch and (current_tokens + seq_len * (len(current_batch) + 1)) > self.max_tokens_per_batch:\n",
        "                # Finalize current batch\n",
        "                batches.append(current_batch)\n",
        "                current_batch = []\n",
        "                current_tokens = 0\n",
        "            \n",
        "            current_batch.append((seq, tgt))\n",
        "            current_tokens = seq_len * len(current_batch)\n",
        "        \n",
        "        if current_batch:\n",
        "            batches.append(current_batch)\n",
        "        \n",
        "        return batches\n",
        "\n",
        "# Generate test data\n",
        "print(\"Generating variable-length sequence data...\")\n",
        "sequences, targets, lengths = generate_variable_length_data(num_samples=500, min_length=5, max_length=30)\n",
        "\n",
        "print(f\"Generated {len(sequences)} sequences\")\n",
        "print(f\"Length statistics:\")\n",
        "print(f\"  Min: {min(lengths)}\")\n",
        "print(f\"  Max: {max(lengths)}\")\n",
        "print(f\"  Mean: {np.mean(lengths):.1f}\")\n",
        "print(f\"  Std: {np.std(lengths):.1f}\")\n",
        "\n",
        "# Visualize length distribution\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "plt.subplot(3, 4, 1)\n",
        "plt.hist(lengths, bins=20, alpha=0.7, edgecolor='black')\n",
        "plt.title('Sequence Length Distribution')\n",
        "plt.xlabel('Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Strategy comparison\n",
        "print(\"\\nComparing different strategies...\")\n",
        "\n",
        "# 1. Simple Padding\n",
        "print(\"\\n1. Simple Padding Strategy:\")\n",
        "start_time = time.time()\n",
        "padded_sequences = pad_sequences_simple(sequences)\n",
        "padded_targets = pad_sequences_simple(targets)\n",
        "padding_time = time.time() - start_time\n",
        "\n",
        "padding_efficiency = np.sum(lengths) / (len(sequences) * padded_sequences.shape[1])\n",
        "print(f\"  Max length: {padded_sequences.shape[1]}\")\n",
        "print(f\"  Memory efficiency: {padding_efficiency:.3f}\")\n",
        "print(f\"  Processing time: {padding_time:.4f} seconds\")\n",
        "print(f\"  Total elements: {padded_sequences.size:,}\")\n",
        "print(f\"  Padding elements: {padded_sequences.size - sum(lengths):,}\")\n",
        "print(f\"  Padding ratio: {(padded_sequences.size - sum(lengths)) / padded_sequences.size:.3f}\")\n",
        "\n",
        "# 2. Bucketing Strategy\n",
        "print(\"\\n2. Bucketing Strategy:\")\n",
        "bucket_boundaries = [10, 15, 20, 25, 30]  # Bucket boundaries\n",
        "start_time = time.time()\n",
        "buckets = create_buckets(sequences, targets, lengths, bucket_boundaries)\n",
        "bucketing_time = time.time() - start_time\n",
        "\n",
        "total_elements_bucketed = 0\n",
        "total_padding_bucketed = 0\n",
        "\n",
        "print(f\"  Number of buckets: {len(buckets)}\")\n",
        "for bucket_id, bucket_data in buckets.items():\n",
        "    if bucket_data:\n",
        "        padded_seqs, padded_tgts, bucket_lengths = pad_bucket(bucket_data)\n",
        "        bucket_max_length = padded_seqs.shape[1] if len(padded_seqs.shape) > 1 else 0\n",
        "        bucket_efficiency = sum(bucket_lengths) / (len(bucket_lengths) * bucket_max_length) if bucket_max_length > 0 else 0\n",
        "        \n",
        "        total_elements_bucketed += padded_seqs.size\n",
        "        total_padding_bucketed += padded_seqs.size - sum(bucket_lengths)\n",
        "        \n",
        "        print(f\"    Bucket {bucket_id}: {len(bucket_data)} sequences, max_len={bucket_max_length}, efficiency={bucket_efficiency:.3f}\")\n",
        "\n",
        "bucketing_efficiency = (total_elements_bucketed - total_padding_bucketed) / total_elements_bucketed if total_elements_bucketed > 0 else 0\n",
        "print(f\"  Overall bucketing efficiency: {bucketing_efficiency:.3f}\")\n",
        "print(f\"  Processing time: {bucketing_time:.4f} seconds\")\n",
        "print(f\"  Memory savings vs padding: {(1 - total_elements_bucketed / padded_sequences.size):.3f}\")\n",
        "\n",
        "# 3. Dynamic Batching\n",
        "print(\"\\n3. Dynamic Batching Strategy:\")\n",
        "start_time = time.time()\n",
        "dynamic_dataset = DynamicBatchDataset(sequences, targets, max_tokens_per_batch=500)\n",
        "dynamic_batches = dynamic_dataset.create_batches()\n",
        "dynamic_time = time.time() - start_time\n",
        "\n",
        "print(f\"  Number of batches: {len(dynamic_batches)}\")\n",
        "batch_sizes = [len(batch) for batch in dynamic_batches]\n",
        "batch_tokens = []\n",
        "for batch in dynamic_batches:\n",
        "    max_len = max(len(seq) for seq, _ in batch)\n",
        "    tokens = len(batch) * max_len\n",
        "    batch_tokens.append(tokens)\n",
        "\n",
        "print(f\"  Batch sizes - Min: {min(batch_sizes)}, Max: {max(batch_sizes)}, Avg: {np.mean(batch_sizes):.1f}\")\n",
        "print(f\"  Tokens per batch - Min: {min(batch_tokens)}, Max: {max(batch_tokens)}, Avg: {np.mean(batch_tokens):.1f}\")\n",
        "print(f\"  Processing time: {dynamic_time:.4f} seconds\")\n",
        "\n",
        "# Visualization of strategies\n",
        "plt.subplot(3, 4, 2)\n",
        "strategies = ['Simple\\nPadding', 'Bucketing', 'Dynamic\\nBatching']\n",
        "efficiencies = [padding_efficiency, bucketing_efficiency, np.mean([sum(len(s) for s, _ in batch) / sum(max(len(s) for s, _ in batch) for _ in batch) for batch in dynamic_batches])]\n",
        "times = [padding_time, bucketing_time, dynamic_time]\n",
        "\n",
        "bars = plt.bar(strategies, efficiencies, alpha=0.7, color=['lightcoral', 'lightblue', 'lightgreen'])\n",
        "plt.title('Memory Efficiency Comparison')\n",
        "plt.ylabel('Efficiency (useful/total)')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "for bar, eff in zip(bars, efficiencies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{eff:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.subplot(3, 4, 3)\n",
        "plt.bar(strategies, times, alpha=0.7, color=['lightcoral', 'lightblue', 'lightgreen'])\n",
        "plt.title('Processing Time Comparison')\n",
        "plt.ylabel('Time (seconds)')\n",
        "\n",
        "for i, time_val in enumerate(times):\n",
        "    plt.text(i, time_val + 0.0001, f'{time_val:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Demonstrate variable-length output handling\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"VARIABLE-LENGTH OUTPUT HANDLING\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# Create a simple model for sequence reversal\n",
        "vocab_size = 20\n",
        "embedding_dim = 32\n",
        "hidden_units = 64\n",
        "\n",
        "class VariableLengthSeq2Seq(keras.Model):\n",
        "    \"\"\"\n",
        "    Sequence-to-sequence model that handles variable-length outputs.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder_embedding = keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
        "        self.encoder_lstm = keras.layers.LSTM(hidden_units, return_state=True)\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder_embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.decoder_lstm = keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "        self.output_layer = keras.layers.Dense(vocab_size, activation='softmax')\n",
        "    \n",
        "    def call(self, inputs, training=None):\n",
        "        encoder_input, decoder_input = inputs\n",
        "        \n",
        "        # Encoder\n",
        "        encoder_embedded = self.encoder_embedding(encoder_input)\n",
        "        _, encoder_h, encoder_c = self.encoder_lstm(encoder_embedded)\n",
        "        encoder_states = [encoder_h, encoder_c]\n",
        "        \n",
        "        # Decoder\n",
        "        decoder_embedded = self.decoder_embedding(decoder_input)\n",
        "        decoder_output, _, _ = self.decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
        "        \n",
        "        output = self.output_layer(decoder_output)\n",
        "        return output\n",
        "    \n",
        "    def generate_sequence(self, input_sequence, max_length=None, end_token=0):\n",
        "        \"\"\"\n",
        "        Generate variable-length output sequence.\n",
        "        \"\"\"\n",
        "        if max_length is None:\n",
        "            max_length = len(input_sequence) + 5  # Default: input length + buffer\n",
        "        \n",
        "        # Encode input\n",
        "        input_tensor = tf.expand_dims(input_sequence, 0)\n",
        "        encoder_embedded = self.encoder_embedding(input_tensor)\n",
        "        _, encoder_h, encoder_c = self.encoder_lstm(encoder_embedded)\n",
        "        \n",
        "        # Initialize decoder state\n",
        "        decoder_state = [encoder_h, encoder_c]\n",
        "        \n",
        "        # Generate sequence\n",
        "        generated = []\n",
        "        decoder_input = tf.constant([[1]])  # Start token\n",
        "        \n",
        "        for _ in range(max_length):\n",
        "            # Predict next token\n",
        "            decoder_embedded = self.decoder_embedding(decoder_input)\n",
        "            decoder_output, decoder_h, decoder_c = self.decoder_lstm(\n",
        "                decoder_embedded, initial_state=decoder_state\n",
        "            )\n",
        "            \n",
        "            predictions = self.output_layer(decoder_output)\n",
        "            predicted_id = tf.argmax(predictions[0, 0]).numpy()\n",
        "            \n",
        "            # Check for end token or reach maximum length\n",
        "            if predicted_id == end_token:\n",
        "                break\n",
        "            \n",
        "            generated.append(predicted_id)\n",
        "            \n",
        "            # Update decoder input and state\n",
        "            decoder_input = tf.constant([[predicted_id]])\n",
        "            decoder_state = [decoder_h, decoder_c]\n",
        "        \n",
        "        return generated\n",
        "\n",
        "# Create and test the model\n",
        "model = VariableLengthSeq2Seq(vocab_size, embedding_dim, hidden_units)\n",
        "\n",
        "# Prepare a small training dataset\n",
        "train_sequences = sequences[:100]\n",
        "train_targets = targets[:100]\n",
        "\n",
        "# Add start and end tokens to targets\n",
        "train_targets_with_tokens = []\n",
        "for target in train_targets:\n",
        "    # Add start token (1) at beginning, end token (0) at end\n",
        "    target_with_tokens = np.concatenate([[1], target, [0]])\n",
        "    train_targets_with_tokens.append(target_with_tokens)\n",
        "\n",
        "# Pad sequences for training\n",
        "max_input_len = max(len(seq) for seq in train_sequences)\n",
        "max_target_len = max(len(seq) for seq in train_targets_with_tokens)\n",
        "\n",
        "padded_inputs = pad_sequences_simple(train_sequences, max_input_len)\n",
        "padded_targets = pad_sequences_simple(train_targets_with_tokens, max_target_len)\n",
        "\n",
        "# Create decoder input and output\n",
        "decoder_input = padded_targets[:, :-1]  # Remove last token\n",
        "decoder_output = padded_targets[:, 1:]  # Remove first token\n",
        "\n",
        "print(f\"\\nTraining data shapes:\")\n",
        "print(f\"Encoder input: {padded_inputs.shape}\")\n",
        "print(f\"Decoder input: {decoder_input.shape}\")\n",
        "print(f\"Decoder output: {decoder_output.shape}\")\n",
        "\n",
        "# Compile and train\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nTraining model for variable-length sequence handling...\")\n",
        "history = model.fit(\n",
        "    [padded_inputs, decoder_input],\n",
        "    decoder_output,\n",
        "    batch_size=16,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Test variable-length generation\n",
        "print(\"\\nTesting variable-length sequence generation:\")\n",
        "test_sequences = sequences[100:105]  # Use unseen sequences\n",
        "test_targets = targets[100:105]\n",
        "\n",
        "for i, (test_seq, true_target) in enumerate(zip(test_sequences, test_targets)):\n",
        "    generated = model.generate_sequence(test_seq, max_length=len(test_seq) + 2)\n",
        "    \n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"  Input:     {test_seq}\")\n",
        "    print(f\"  Expected:  {true_target}\")\n",
        "    print(f\"  Generated: {generated}\")\n",
        "    print(f\"  Length:    Input={len(test_seq)}, Expected={len(true_target)}, Generated={len(generated)}\")\n",
        "    \n",
        "    # Check accuracy\n",
        "    if len(generated) == len(true_target) and np.array_equal(generated, true_target):\n",
        "        print(f\"  Result:    ✓ CORRECT\")\n",
        "    else:\n",
        "        print(f\"  Result:    ✗ INCORRECT\")\n",
        "\n",
        "# Visualize length handling strategies\n",
        "plt.subplot(3, 4, 4)\n",
        "length_ranges = ['5-10', '11-15', '16-20', '21-25', '26-30']\n",
        "bucket_counts = [len([l for l in lengths if 5 <= l <= 10]),\n",
        "                len([l for l in lengths if 11 <= l <= 15]),\n",
        "                len([l for l in lengths if 16 <= l <= 20]),\n",
        "                len([l for l in lengths if 21 <= l <= 25]),\n",
        "                len([l for l in lengths if 26 <= l <= 30])]\n",
        "\n",
        "plt.bar(length_ranges, bucket_counts, alpha=0.7, color='skyblue')\n",
        "plt.title('Distribution Across Length Buckets')\n",
        "plt.xlabel('Length Range')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Memory usage comparison\n",
        "plt.subplot(3, 4, 5)\n",
        "padding_memory = padded_sequences.size\n",
        "bucketing_memory = total_elements_bucketed\n",
        "dynamic_memory = sum(batch_tokens)\n",
        "\n",
        "memory_usage = [padding_memory, bucketing_memory, dynamic_memory]\n",
        "memory_labels = ['Padding', 'Bucketing', 'Dynamic']\n",
        "\n",
        "bars = plt.bar(memory_labels, memory_usage, alpha=0.7, color=['lightcoral', 'lightblue', 'lightgreen'])\n",
        "plt.title('Memory Usage Comparison')\n",
        "plt.ylabel('Total Elements')\n",
        "\n",
        "for bar, mem in zip(bars, memory_usage):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, \n",
        "             f'{mem:,}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
        "\n",
        "# Training curves\n",
        "plt.subplot(3, 4, 6)\n",
        "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "plt.title('Variable-Length Model Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(3, 4, 7)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='green')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Sequence length vs processing time\n",
        "plt.subplot(3, 4, 8)\n",
        "sample_lengths = [5, 10, 15, 20, 25, 30]\n",
        "processing_times = []\n",
        "\n",
        "for length in sample_lengths:\n",
        "    # Simulate processing time (linear with length for RNNs)\n",
        "    time_per_step = 0.001  # milliseconds\n",
        "    proc_time = length * time_per_step\n",
        "    processing_times.append(proc_time)\n",
        "\n",
        "plt.plot(sample_lengths, processing_times, 'bo-', linewidth=2, markersize=6)\n",
        "plt.title('Processing Time vs Sequence Length')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Processing Time (ms)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Advanced techniques visualization\n",
        "plt.subplot(3, 4, 9)\n",
        "techniques = ['Padding', 'Masking', 'Bucketing', 'Dynamic\\nBatching', 'Attention\\nPooling']\n",
        "complexity = [1, 2, 4, 5, 3]  # Implementation complexity (1-5 scale)\n",
        "performance = [3, 4, 5, 5, 4]  # Performance benefit (1-5 scale)\n",
        "\n",
        "x = np.arange(len(techniques))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, complexity, width, label='Implementation Complexity', alpha=0.8, color='orange')\n",
        "plt.bar(x + width/2, performance, width, label='Performance Benefit', alpha=0.8, color='blue')\n",
        "plt.xlabel('Technique')\n",
        "plt.ylabel('Score (1-5)')\n",
        "plt.title('Technique Comparison')\n",
        "plt.xticks(x, techniques, rotation=45, ha='right')\n",
        "plt.legend()\n",
        "\n",
        "# Memory efficiency over different max lengths\n",
        "plt.subplot(3, 4, 10)\n",
        "max_lengths = [10, 15, 20, 25, 30, 35, 40]\n",
        "efficiencies = []\n",
        "\n",
        "for max_len in max_lengths:\n",
        "    # Calculate efficiency for different max lengths\n",
        "    useful_elements = sum(min(l, max_len) for l in lengths)\n",
        "    total_elements = len(lengths) * max_len\n",
        "    efficiency = useful_elements / total_elements\n",
        "    efficiencies.append(efficiency)\n",
        "\n",
        "plt.plot(max_lengths, efficiencies, 'ro-', linewidth=2, markersize=6)\n",
        "plt.title('Memory Efficiency vs Max Length')\n",
        "plt.xlabel('Maximum Length')\n",
        "plt.ylabel('Memory Efficiency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Optimal bucketing analysis\n",
        "plt.subplot(3, 4, 11)\n",
        "bucket_configs = ['[10, 20, 30]', '[10, 15, 20, 25, 30]', '[8, 12, 16, 20, 24, 28]']\n",
        "bucket_efficiencies = [0.85, 0.92, 0.94]  # Simulated efficiencies\n",
        "\n",
        "bars = plt.bar(range(len(bucket_configs)), bucket_efficiencies, alpha=0.7, color='lightgreen')\n",
        "plt.title('Bucketing Strategy Comparison')\n",
        "plt.xlabel('Bucket Configuration')\n",
        "plt.ylabel('Efficiency')\n",
        "plt.xticks(range(len(bucket_configs)), ['3 Buckets', '5 Buckets', '6 Buckets'])\n",
        "\n",
        "for bar, eff in zip(bars, bucket_efficiencies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{eff:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Best practices summary\n",
        "plt.subplot(3, 4, 12)\n",
        "plt.text(0.1, 0.8, 'BEST PRACTICES', fontsize=14, fontweight='bold')\n",
        "practices = [\n",
        "    '• Use padding for simple cases',\n",
        "    '• Implement masking to ignore padding',\n",
        "    '• Consider bucketing for efficiency',\n",
        "    '• Dynamic batching for optimal memory',\n",
        "    '• EOS tokens for variable outputs',\n",
        "    '• Length normalization in beam search'\n",
        "]\n",
        "\n",
        "for i, practice in enumerate(practices):\n",
        "    plt.text(0.1, 0.65 - i*0.08, practice, fontsize=10)\n",
        "\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Comprehensive summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXERCISE 3 COMPREHENSIVE SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. VARIABLE-LENGTH INPUT STRATEGIES:\")\n",
        "print(f\"   a) Simple Padding:\")\n",
        "print(f\"      • Memory efficiency: {padding_efficiency:.3f}\")\n",
        "print(f\"      • Implementation: Easiest\")\n",
        "print(f\"      • Best for: Small length variations\")\n",
        "\n",
        "print(f\"   b) Bucketing:\")\n",
        "print(f\"      • Memory efficiency: {bucketing_efficiency:.3f}\")\n",
        "print(f\"      • Implementation: Moderate complexity\")\n",
        "print(f\"      • Best for: Large length variations\")\n",
        "\n",
        "print(f\"   c) Dynamic Batching:\")\n",
        "print(f\"      • Memory efficiency: Optimal\")\n",
        "print(f\"      • Implementation: Most complex\")\n",
        "print(f\"      • Best for: Production systems\")\n",
        "\n",
        "print(\"\\n2. VARIABLE-LENGTH OUTPUT STRATEGIES:\")\n",
        "print(\"   • End-of-sequence tokens for natural stopping\")\n",
        "print(\"   • Maximum length limits for safety\")\n",
        "print(\"   • Length normalization for beam search\")\n",
        "print(\"   • Early stopping for efficiency\")\n",
        "\n",
        "print(\"\\n3. IMPLEMENTATION RECOMMENDATIONS:\")\n",
        "print(\"   • Start with simple padding for prototyping\")\n",
        "print(\"   • Add masking to ignore padding tokens\")\n",
        "print(\"   • Implement bucketing for production efficiency\")\n",
        "print(\"   • Use dynamic batching for optimal resource usage\")\n",
        "print(\"   • Monitor memory and computational efficiency\")\n",
        "\n",
        "print(\"\\n4. MATHEMATICAL CONSIDERATIONS:\")\n",
        "print(\"   • Memory usage: O(batch_size × max_length)\")\n",
        "print(\"   • Computation: O(batch_size × actual_length)\")\n",
        "print(\"   • Efficiency = Σ(actual_lengths) / (batch_size × max_length)\")\n",
        "print(\"   • Optimal bucketing minimizes padding overhead\")\n",
        "\n",
        "print(\"\\n5. PRACTICAL IMPACT:\")\n",
        "print(f\"   • Memory savings with bucketing: {(1 - total_elements_bucketed / padded_sequences.size)*100:.1f}%\")\n",
        "print(f\"   • Efficiency improvement: {((bucketing_efficiency - padding_efficiency) / padding_efficiency * 100):.1f}%\")\n",
        "print(f\"   • Reduced computational waste\")\n",
        "print(f\"   • Better GPU utilization\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_4_theory"
      },
      "source": [
        "## Exercise 4: Beam Search Implementation and Analysis\n",
        "\n",
        "### Question\n",
        "What is beam search and why would you use it? What tool can you use to implement it?\n",
        "\n",
        "### Theoretical Foundation\n",
        "\n",
        "#### The Problem with Greedy Decoding\n",
        "\n",
        "**Greedy Decoding:**\n",
        "$$y_t = \\arg\\max_{y} P(y | y_1, ..., y_{t-1}, x)$$\n",
        "\n",
        "**Problem:** Locally optimal choices may lead to globally suboptimal sequences.\n",
        "\n",
        "**Example:**\n",
        "Consider translating French \"Comment allez-vous?\" to English:\n",
        "- Step 1: \"How\" (p=0.7) vs \"What\" (p=0.2) vs \"Where\" (p=0.1)\n",
        "- Step 2 after \"How\": \"are\" (p=0.8) vs \"do\" (p=0.2)\n",
        "- Step 3 after \"How are\": \"you\" (p=0.9)\n",
        "\n",
        "Greedy picks \"How are you\" but might miss better overall sequence.\n",
        "\n",
        "#### Beam Search Algorithm\n",
        "\n",
        "**Core Concept:** Maintain $k$ most promising partial sequences at each step.\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "At step $t$, maintain beam $B_t = \\{y^{(1)}_{1:t}, y^{(2)}_{1:t}, ..., y^{(k)}_{1:t}\\}$\n",
        "\n",
        "**Score Function:**\n",
        "$$\\text{score}(y_{1:t}) = \\sum_{i=1}^t \\log P(y_i | y_{1:i-1}, x)$$\n",
        "\n",
        "**Length Normalization:**\n",
        "$$\\text{normalized_score}(y_{1:t}) = \\frac{\\text{score}(y_{1:t})}{t^\\alpha}$$\n",
        "\n",
        "Where $\\alpha \\in [0, 1]$ controls length preference:\n",
        "- $\\alpha = 0$: No normalization (favors shorter sequences)\n",
        "- $\\alpha = 1$: Full normalization (neutral)\n",
        "- $\\alpha = 0.6$: Common empirical choice\n",
        "\n",
        "#### Beam Search Variants\n",
        "\n",
        "**1. Standard Beam Search:**\n",
        "- Fixed beam width $k$\n",
        "- Keep top-$k$ sequences at each step\n",
        "\n",
        "**2. Diverse Beam Search:**\n",
        "- Encourage diversity among beams\n",
        "- Add diversity penalty: $\\text{score} - \\lambda \\times \\text{similarity}$\n",
        "\n",
        "**3. Constrained Beam Search:**\n",
        "- Force inclusion of specific tokens/phrases\n",
        "- Useful for controlled generation\n",
        "\n",
        "**4. Coverage-based Beam Search:**\n",
        "- Penalize repetition\n",
        "- Encourage attention coverage\n",
        "\n",
        "### Computational Complexity\n",
        "\n",
        "**Time Complexity:** $O(T \\times k \\times V)$\n",
        "- $T$: Maximum sequence length\n",
        "- $k$: Beam width\n",
        "- $V$: Vocabulary size\n",
        "\n",
        "**Space Complexity:** $O(k \\times T)$\n",
        "- Store $k$ sequences of length up to $T$\n",
        "\n",
        "**Comparison with Alternatives:**\n",
        "\n",
        "| Method | Time | Space | Quality | Diversity |\n",
        "|--------|------|-------|---------|----------|\n",
        "| **Greedy** | $O(T \\times V)$ | $O(T)$ | Poor | None |\n",
        "| **Beam Search** | $O(T \\times k \\times V)$ | $O(k \\times T)$ | Good | Limited |\n",
        "| **Random Sampling** | $O(T \\times V)$ | $O(T)$ | Variable | High |\n",
        "| **Top-k Sampling** | $O(T \\times V)$ | $O(T)$ | Good | High |\n",
        "\n",
        "### Implementation Tools\n",
        "\n",
        "#### TensorFlow Addons\n",
        "```python\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# BeamSearchDecoder\n",
        "decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(\n",
        "    cell=decoder_cell,\n",
        "    beam_width=beam_width,\n",
        "    output_layer=output_layer,\n",
        "    length_penalty_weight=length_penalty\n",
        ")\n",
        "```\n",
        "\n",
        "#### Hugging Face Transformers\n",
        "```python\n",
        "# Built-in beam search\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    num_beams=5,\n",
        "    length_penalty=0.6,\n",
        "    max_length=100\n",
        ")\n",
        "```\n",
        "\n",
        "#### Custom Implementation\n",
        "- Full control over search process\n",
        "- Custom scoring functions\n",
        "- Domain-specific constraints\n",
        "\n",
        "### When to Use Beam Search\n",
        "\n",
        "**Ideal Applications:**\n",
        "1. **Machine Translation**: Better translation quality\n",
        "2. **Text Summarization**: More coherent summaries\n",
        "3. **Image Captioning**: More accurate descriptions\n",
        "4. **Question Answering**: Better answer formulation\n",
        "\n",
        "**When NOT to Use:**\n",
        "1. **Creative Writing**: May reduce diversity\n",
        "2. **Dialogue Systems**: Can make responses too formal\n",
        "3. **Real-time Applications**: Computational overhead\n",
        "4. **Very Large Vocabularies**: Memory constraints\n",
        "\n",
        "### Hyperparameter Guidelines\n",
        "\n",
        "**Beam Width ($k$):**\n",
        "- $k = 1$: Greedy search\n",
        "- $k = 3-5$: Good balance for most tasks\n",
        "- $k = 10+$: Diminishing returns, computational overhead\n",
        "\n",
        "**Length Penalty ($\\alpha$):**\n",
        "- $\\alpha = 0.0$: Strong bias toward shorter sequences\n",
        "- $\\alpha = 0.6$: Empirically good for many tasks\n",
        "- $\\alpha = 1.0$: No length bias\n",
        "\n",
        "**Maximum Length:**\n",
        "- Should be reasonable upper bound\n",
        "- Too high: Inefficient\n",
        "- Too low: Truncated outputs\n",
        "\n",
        "### Advanced Techniques\n",
        "\n",
        "#### Coverage Mechanism\n",
        "$$\\text{coverage}_t = \\sum_{i=1}^{t-1} \\alpha_{i,j}$$\n",
        "$$\\text{coverage_penalty} = \\lambda \\sum_j \\min(\\alpha_{t,j}, \\text{coverage}_{t,j})$$\n",
        "\n",
        "#### Diverse Beam Search\n",
        "$$\\text{diverse_score} = \\text{score} - \\lambda \\sum_{i=1}^{k-1} \\text{similarity}(\\text{beam}_k, \\text{beam}_i)$$\n",
        "\n",
        "#### Temperature Scaling\n",
        "$$P_{\\text{temp}}(y_t) = \\frac{\\exp(\\text{logit}_t / \\tau)}{\\sum_j \\exp(\\text{logit}_j / \\tau)}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise_4_implementation"
      },
      "source": [
        "# Exercise 4: Comprehensive Beam Search Implementation and Analysis\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "import heapq\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "print(\"Exercise 4: Beam Search Implementation and Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "@dataclass\n",
        "class BeamState:\n",
        "    \"\"\"Represents a single beam state during search.\"\"\"\n",
        "    sequence: List[int]\n",
        "    score: float\n",
        "    hidden_state: Optional[tf.Tensor] = None\n",
        "    attention_weights: Optional[tf.Tensor] = None\n",
        "    \n",
        "    def __lt__(self, other):\n",
        "        return self.score < other.score\n",
        "\n",
        "class BeamSearchDecoder:\n",
        "    \"\"\"\n",
        "    Educational implementation of beam search decoder.\n",
        "    Demonstrates the core algorithm with full customization.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, beam_width=5, max_length=50, \n",
        "                 length_penalty=0.6, coverage_penalty=0.0):\n",
        "        self.model = model\n",
        "        self.beam_width = beam_width\n",
        "        self.max_length = max_length\n",
        "        self.length_penalty = length_penalty\n",
        "        self.coverage_penalty = coverage_penalty\n",
        "        \n",
        "    def search(self, encoder_input, start_token=1, end_token=0, return_attention=False):\n",
        "        \"\"\"\n",
        "        Perform beam search decoding.\n",
        "        \n",
        "        Args:\n",
        "            encoder_input: Input sequence to encode\n",
        "            start_token: Token to start decoding\n",
        "            end_token: Token that ends decoding\n",
        "            return_attention: Whether to return attention weights\n",
        "        \n",
        "        Returns:\n",
        "            List of (sequence, score) tuples\n",
        "        \"\"\"\n",
        "        # Initialize beams with start token\n",
        "        initial_state = BeamState(\n",
        "            sequence=[start_token],\n",
        "            score=0.0\n",
        "        )\n",
        "        \n",
        "        beams = [initial_state]\n",
        "        finished_beams = []\n",
        "        \n",
        "        for step in range(self.max_length):\n",
        "            if not beams:  # All beams finished\n",
        "                break\n",
        "                \n",
        "            # Generate candidates for all current beams\n",
        "            candidates = []\n",
        "            \n",
        "            for beam in beams:\n",
        "                if beam.sequence[-1] == end_token:\n",
        "                    # Beam already finished\n",
        "                    finished_beams.append(beam)\n",
        "                    continue\n",
        "                \n",
        "                # Get next token probabilities\n",
        "                next_probs = self._get_next_probabilities(\n",
        "                    encoder_input, beam.sequence\n",
        "                )\n",
        "                \n",
        "                # Create candidates for each possible next token\n",
        "                for token_id, log_prob in enumerate(next_probs):\n",
        "                    if log_prob > -1e9:  # Skip very low probability tokens\n",
        "                        new_sequence = beam.sequence + [token_id]\n",
        "                        new_score = beam.score + log_prob\n",
        "                        \n",
        "                        # Apply length normalization\n",
        "                        if self.length_penalty > 0:\n",
        "                            normalized_score = new_score / (len(new_sequence) ** self.length_penalty)\n",
        "                        else:\n",
        "                            normalized_score = new_score\n",
        "                        \n",
        "                        candidates.append(BeamState(\n",
        "                            sequence=new_sequence,\n",
        "                            score=normalized_score\n",
        "                        ))\n",
        "            \n",
        "            # Select top-k candidates for next iteration\n",
        "            candidates.sort(key=lambda x: x.score, reverse=True)\n",
        "            beams = candidates[:self.beam_width]\n",
        "            \n",
        "            # Check if all beams are finished\n",
        "            if all(beam.sequence[-1] == end_token for beam in beams):\n",
        "                finished_beams.extend(beams)\n",
        "                break\n",
        "        \n",
        "        # Combine finished and unfinished beams\n",
        "        all_beams = finished_beams + beams\n",
        "        \n",
        "        # Sort by score and return top results\n",
        "        all_beams.sort(key=lambda x: x.score, reverse=True)\n",
        "        \n",
        "        return [(beam.sequence, beam.score) for beam in all_beams[:self.beam_width]]\n",
        "    \n",
        "    def _get_next_probabilities(self, encoder_input, current_sequence):\n",
        "        \"\"\"\n",
        "        Get log probabilities for next tokens.\n",
        "        This is a simplified version - real implementation would use the actual model.\n",
        "        \"\"\"\n",
        "        # Simulate next token probabilities\n",
        "        vocab_size = 20\n",
        "        \n",
        "        # Create some realistic probability distribution\n",
        "        # Higher probability for lower token IDs (simulating common words)\n",
        "        probs = np.random.dirichlet(np.linspace(2, 0.5, vocab_size))\n",
        "        \n",
        "        # Add some sequence-dependent bias\n",
        "        if len(current_sequence) > 1:\n",
        "            last_token = current_sequence[-1]\n",
        "            # Increase probability of token similar to last one\n",
        "            if last_token < vocab_size:\n",
        "                probs[last_token] *= 2\n",
        "                if last_token + 1 < vocab_size:\n",
        "                    probs[last_token + 1] *= 1.5\n",
        "        \n",
        "        # Normalize and convert to log probabilities\n",
        "        probs = probs / np.sum(probs)\n",
        "        log_probs = np.log(probs + 1e-10)  # Add small epsilon to avoid log(0)\n",
        "        \n",
        "        return log_probs\n",
        "\n",
        "# Comparison between different decoding strategies\n",
        "class DecodingComparison:\n",
        "    \"\"\"\n",
        "    Compare different decoding strategies on the same inputs.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size=20):\n",
        "        self.vocab_size = vocab_size\n",
        "    \n",
        "    def greedy_decode(self, encoder_input, max_length=10, start_token=1, end_token=0):\n",
        "        \"\"\"Greedy decoding baseline.\"\"\"\n",
        "        sequence = [start_token]\n",
        "        total_score = 0.0\n",
        "        \n",
        "        for _ in range(max_length):\n",
        "            probs = self._get_probabilities(encoder_input, sequence)\n",
        "            next_token = np.argmax(probs)\n",
        "            total_score += np.log(probs[next_token])\n",
        "            \n",
        "            sequence.append(next_token)\n",
        "            \n",
        "            if next_token == end_token:\n",
        "                break\n",
        "        \n",
        "        return sequence, total_score\n",
        "    \n",
        "    def random_sample(self, encoder_input, max_length=10, start_token=1, end_token=0, temperature=1.0):\n",
        "        \"\"\"Random sampling with temperature.\"\"\"\n",
        "        sequence = [start_token]\n",
        "        total_score = 0.0\n",
        "        \n",
        "        for _ in range(max_length):\n",
        "            probs = self._get_probabilities(encoder_input, sequence)\n",
        "            \n",
        "            # Apply temperature\n",
        "            if temperature != 1.0:\n",
        "                log_probs = np.log(probs + 1e-10) / temperature\n",
        "                probs = np.exp(log_probs)\n",
        "                probs = probs / np.sum(probs)\n",
        "            \n",
        "            next_token = np.random.choice(len(probs), p=probs)\n",
        "            total_score += np.log(probs[next_token])\n",
        "            \n",
        "            sequence.append(next_token)\n",
        "            \n",
        "            if next_token == end_token:\n",
        "                break\n",
        "        \n",
        "        return sequence, total_score\n",
        "    \n",
        "    def top_k_sample(self, encoder_input, k=5, max_length=10, start_token=1, end_token=0):\n",
        "        \"\"\"Top-k sampling.\"\"\"\n",
        "        sequence = [start_token]\n",
        "        total_score = 0.0\n",
        "        \n",
        "        for _ in range(max_length):\n",
        "            probs = self._get_probabilities(encoder_input, sequence)\n",
        "            \n",
        "            # Keep only top-k probabilities\n",
        "            top_k_indices = np.argpartition(probs, -k)[-k:]\n",
        "            top_k_probs = probs[top_k_indices]\n",
        "            top_k_probs = top_k_probs / np.sum(top_k_probs)  # Renormalize\n",
        "            \n",
        "            # Sample from top-k\n",
        "            local_choice = np.random.choice(len(top_k_probs), p=top_k_probs)\n",
        "            next_token = top_k_indices[local_choice]\n",
        "            total_score += np.log(probs[next_token])\n",
        "            \n",
        "            sequence.append(next_token)\n",
        "            \n",
        "            if next_token == end_token:\n",
        "                break\n",
        "        \n",
        "        return sequence, total_score\n",
        "    \n",
        "    def _get_probabilities(self, encoder_input, sequence):\n",
        "        \"\"\"Get probability distribution for next token.\"\"\"\n",
        "        # Simulate realistic probability distribution\n",
        "        base_probs = np.random.dirichlet(np.ones(self.vocab_size) * 0.5)\n",
        "        \n",
        "        # Add some sequence-dependent patterns\n",
        "        if len(sequence) > 1:\n",
        "            last_token = sequence[-1]\n",
        "            if last_token < self.vocab_size:\n",
        "                # Create some dependencies\n",
        "                base_probs[last_token] *= 0.5  # Reduce repetition\n",
        "                if last_token + 1 < self.vocab_size:\n",
        "                    base_probs[last_token + 1] *= 2  # Increase next token\n",
        "        \n",
        "        # Normalize\n",
        "        return base_probs / np.sum(base_probs)\n",
        "\n",
        "# Demonstrate beam search\n",
        "print(\"Demonstrating Beam Search Algorithm...\")\n",
        "\n",
        "# Create decoder and comparison object\n",
        "beam_decoder = BeamSearchDecoder(model=None, beam_width=5, max_length=8, length_penalty=0.6)\n",
        "comparison = DecodingComparison(vocab_size=20)\n",
        "\n",
        "# Test input\n",
        "test_input = np.array([1, 2, 3, 4])  # Dummy encoder input\n",
        "\n",
        "print(f\"\\nTest input: {test_input}\")\n",
        "print(f\"Beam width: {beam_decoder.beam_width}\")\n",
        "print(f\"Max length: {beam_decoder.max_length}\")\n",
        "print(f\"Length penalty: {beam_decoder.length_penalty}\")\n",
        "\n",
        "# Run different decoding strategies\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DECODING STRATEGY COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Greedy decoding\n",
        "greedy_seq, greedy_score = comparison.greedy_decode(test_input)\n",
        "print(f\"\\n1. Greedy Decoding:\")\n",
        "print(f\"   Sequence: {greedy_seq}\")\n",
        "print(f\"   Score: {greedy_score:.4f}\")\n",
        "print(f\"   Length: {len(greedy_seq)}\")\n",
        "\n",
        "# 2. Beam search\n",
        "beam_results = beam_decoder.search(test_input)\n",
        "print(f\"\\n2. Beam Search (width={beam_decoder.beam_width}):\")\n",
        "for i, (seq, score) in enumerate(beam_results[:3]):\n",
        "    print(f\"   Beam {i+1}: {seq} (score: {score:.4f}, length: {len(seq)})\")\n",
        "\n",
        "# 3. Random sampling\n",
        "print(f\"\\n3. Random Sampling (5 samples):\")\n",
        "for i in range(5):\n",
        "    random_seq, random_score = comparison.random_sample(test_input, temperature=1.0)\n",
        "    print(f\"   Sample {i+1}: {random_seq} (score: {random_score:.4f})\")\n",
        "\n",
        "# 4. Top-k sampling\n",
        "print(f\"\\n4. Top-k Sampling (k=5, 5 samples):\")\n",
        "for i in range(5):\n",
        "    topk_seq, topk_score = comparison.top_k_sample(test_input, k=5)\n",
        "    print(f\"   Sample {i+1}: {topk_seq} (score: {topk_score:.4f})\")\n",
        "\n",
        "# Beam width analysis\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BEAM WIDTH ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "beam_widths = [1, 3, 5, 10, 15]\n",
        "results_by_width = {}\n",
        "times_by_width = {}\n",
        "\n",
        "for width in beam_widths:\n",
        "    decoder = BeamSearchDecoder(model=None, beam_width=width, max_length=8)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    results = decoder.search(test_input)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    results_by_width[width] = results\n",
        "    times_by_width[width] = end_time - start_time\n",
        "    \n",
        "    best_sequence, best_score = results[0]\n",
        "    print(f\"\\nBeam width {width:2d}: {best_sequence} (score: {best_score:.4f}, time: {times_by_width[width]:.4f}s)\")\n",
        "\n",
        "# Length penalty analysis\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LENGTH PENALTY ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "length_penalties = [0.0, 0.2, 0.6, 1.0, 1.4]\n",
        "results_by_penalty = {}\n",
        "\n",
        "for penalty in length_penalties:\n",
        "    decoder = BeamSearchDecoder(model=None, beam_width=5, max_length=8, length_penalty=penalty)\n",
        "    results = decoder.search(test_input)\n",
        "    \n",
        "    results_by_penalty[penalty] = results\n",
        "    best_sequence, best_score = results[0]\n",
        "    print(f\"Penalty {penalty:.1f}: {best_sequence} (score: {best_score:.4f}, length: {len(best_sequence)})\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "# Beam width vs performance\n",
        "plt.subplot(3, 4, 1)\n",
        "best_scores = [results_by_width[w][0][1] for w in beam_widths]\n",
        "plt.plot(beam_widths, best_scores, 'bo-', linewidth=2, markersize=8)\n",
        "plt.title('Best Score vs Beam Width')\n",
        "plt.xlabel('Beam Width')\n",
        "plt.ylabel('Best Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Beam width vs computation time\n",
        "plt.subplot(3, 4, 2)\n",
        "times = [times_by_width[w] * 1000 for w in beam_widths]  # Convert to milliseconds\n",
        "plt.plot(beam_widths, times, 'ro-', linewidth=2, markersize=8)\n",
        "plt.title('Computation Time vs Beam Width')\n",
        "plt.xlabel('Beam Width')\n",
        "plt.ylabel('Time (ms)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Length penalty vs sequence length\n",
        "plt.subplot(3, 4, 3)\n",
        "avg_lengths = [np.mean([len(seq) for seq, _ in results_by_penalty[p][:3]]) for p in length_penalties]\n",
        "plt.plot(length_penalties, avg_lengths, 'go-', linewidth=2, markersize=8)\n",
        "plt.title('Average Length vs Length Penalty')\n",
        "plt.xlabel('Length Penalty')\n",
        "plt.ylabel('Average Sequence Length')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Score distribution for different beam widths\n",
        "plt.subplot(3, 4, 4)\n",
        "width_to_plot = 5\n",
        "scores = [score for _, score in results_by_width[width_to_plot]]\n",
        "plt.bar(range(len(scores)), scores, alpha=0.7, color='skyblue')\n",
        "plt.title(f'Score Distribution (Beam Width {width_to_plot})')\n",
        "plt.xlabel('Beam Rank')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Comparison of decoding strategies\n",
        "plt.subplot(3, 4, 5)\n",
        "strategies = ['Greedy', 'Beam\\n(w=5)', 'Random', 'Top-k']\n",
        "# Simulate scores for comparison\n",
        "strategy_scores = [greedy_score, beam_results[0][1], -8.5, -7.8]  # Example scores\n",
        "colors = ['lightcoral', 'lightblue', 'lightgreen', 'lightyellow']\n",
        "\n",
        "bars = plt.bar(strategies, strategy_scores, color=colors, alpha=0.8)\n",
        "plt.title('Decoding Strategy Comparison')\n",
        "plt.ylabel('Best Score')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "for bar, score in zip(bars, strategy_scores):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "             f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Beam search tree visualization (simplified)\n",
        "plt.subplot(3, 4, 6)\n",
        "# Create a simple tree structure visualization\n",
        "plt.text(0.5, 0.9, 'Beam Search Tree', ha='center', fontsize=12, fontweight='bold')\n",
        "plt.text(0.5, 0.8, 'START', ha='center', bbox=dict(boxstyle=\"round\", facecolor='lightblue'))\n",
        "\n",
        "# Level 1\n",
        "level1_positions = [0.2, 0.4, 0.6, 0.8]\n",
        "for i, pos in enumerate(level1_positions):\n",
        "    plt.text(pos, 0.6, f'T{i+1}', ha='center', bbox=dict(boxstyle=\"round\", facecolor='lightgreen'))\n",
        "    plt.plot([0.5, pos], [0.75, 0.65], 'k-', alpha=0.5)\n",
        "\n",
        "# Level 2 (only top beams)\n",
        "level2_positions = [0.3, 0.5, 0.7]\n",
        "for i, pos in enumerate(level2_positions):\n",
        "    plt.text(pos, 0.4, f'T{i+1}', ha='center', bbox=dict(boxstyle=\"round\", facecolor='lightyellow'))\n",
        "    plt.plot([level1_positions[i], pos], [0.55, 0.45], 'k-', alpha=0.5)\n",
        "\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "plt.axis('off')\n",
        "\n",
        "# Computational complexity\n",
        "plt.subplot(3, 4, 7)\n",
        "sequence_lengths = [5, 10, 15, 20, 25]\n",
        "vocab_size = 20\n",
        "beam_width = 5\n",
        "\n",
        "greedy_ops = [T * vocab_size for T in sequence_lengths]\n",
        "beam_ops = [T * beam_width * vocab_size for T in sequence_lengths]\n",
        "\n",
        "plt.plot(sequence_lengths, greedy_ops, 'r-', label='Greedy', linewidth=2)\n",
        "plt.plot(sequence_lengths, beam_ops, 'b-', label=f'Beam (w={beam_width})', linewidth=2)\n",
        "plt.title('Computational Complexity')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Operations')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Quality vs diversity trade-off\n",
        "plt.subplot(3, 4, 8)\n",
        "methods = ['Greedy', 'Beam\\n(w=3)', 'Beam\\n(w=10)', 'Random', 'Top-k']\n",
        "quality = [3, 4, 4.5, 2, 3.5]  # Simulated quality scores (1-5)\n",
        "diversity = [1, 2, 1.5, 5, 4]  # Simulated diversity scores (1-5)\n",
        "\n",
        "plt.scatter(diversity, quality, s=100, alpha=0.7, c=['red', 'blue', 'darkblue', 'green', 'orange'])\n",
        "for i, method in enumerate(methods):\n",
        "    plt.annotate(method, (diversity[i], quality[i]), xytext=(5, 5), \n",
        "                textcoords='offset points', fontsize=8)\n",
        "\n",
        "plt.xlabel('Diversity')\n",
        "plt.ylabel('Quality')\n",
        "plt.title('Quality vs Diversity Trade-off')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Memory usage by beam width\n",
        "plt.subplot(3, 4, 9)\n",
        "max_length = 20\n",
        "memory_usage = [w * max_length for w in beam_widths]  # Simplified memory model\n",
        "\n",
        "plt.bar(range(len(beam_widths)), memory_usage, alpha=0.7, color='purple')\n",
        "plt.title('Memory Usage vs Beam Width')\n",
        "plt.xlabel('Beam Width')\n",
        "plt.ylabel('Memory Units')\n",
        "plt.xticks(range(len(beam_widths)), beam_widths)\n",
        "\n",
        "# Best practices summary\n",
        "plt.subplot(3, 4, 10)\n",
        "plt.text(0.1, 0.9, 'BEAM SEARCH BEST PRACTICES', fontsize=12, fontweight='bold')\n",
        "practices = [\n",
        "    '• Start with beam width 3-5',\n",
        "    '• Use length penalty 0.6-1.0',\n",
        "    '• Set reasonable max length',\n",
        "    '• Consider computational cost',\n",
        "    '• Monitor diversity vs quality',\n",
        "    '• Use early stopping'\n",
        "]\n",
        "\n",
        "for i, practice in enumerate(practices):\n",
        "    plt.text(0.1, 0.75 - i*0.1, practice, fontsize=10)\n",
        "\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "plt.axis('off')\n",
        "\n",
        "# Implementation tools comparison\n",
        "plt.subplot(3, 4, 11)\n",
        "tools = ['Custom', 'TF Addons', 'HuggingFace', 'OpenNMT']\n",
        "ease_of_use = [2, 4, 5, 4]  # 1-5 scale\n",
        "flexibility = [5, 3, 3, 4]  # 1-5 scale\n",
        "\n",
        "x = np.arange(len(tools))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, ease_of_use, width, label='Ease of Use', alpha=0.8, color='lightblue')\n",
        "plt.bar(x + width/2, flexibility, width, label='Flexibility', alpha=0.8, color='lightcoral')\n",
        "plt.xlabel('Implementation Tool')\n",
        "plt.ylabel('Score (1-5)')\n",
        "plt.title('Implementation Tools Comparison')\n",
        "plt.xticks(x, tools, rotation=45)\n",
        "plt.legend()\n",
        "\n",
        "# Performance vs beam width curve\n",
        "plt.subplot(3, 4, 12)\n",
        "extended_widths = range(1, 21)\n",
        "# Simulate performance curve (diminishing returns)\n",
        "performance = [1 - np.exp(-w/5) for w in extended_widths]\n",
        "compute_cost = [w for w in extended_widths]\n",
        "\n",
        "fig_ax = plt.gca()\n",
        "ax2 = fig_ax.twinx()\n",
        "\n",
        "fig_ax.plot(extended_widths, performance, 'b-', linewidth=2, label='Performance')\n",
        "ax2.plot(extended_widths, compute_cost, 'r-', linewidth=2, label='Compute Cost')\n",
        "\n",
        "fig_ax.set_xlabel('Beam Width')\n",
        "fig_ax.set_ylabel('Performance', color='blue')\n",
        "ax2.set_ylabel('Compute Cost', color='red')\n",
        "plt.title('Performance vs Cost Trade-off')\n",
        "fig_ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final comprehensive analysis\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXERCISE 4 COMPREHENSIVE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. BEAM SEARCH FUNDAMENTALS:\")\n",
        "print(\"   • Maintains multiple candidate sequences (beams)\")\n",
        "print(\"   • Explores more search space than greedy decoding\")\n",
        "print(\"   • Uses length normalization to avoid short sequence bias\")\n",
        "print(\"   • Provides balance between quality and computational cost\")\n",
        "\n",
        "print(\"\\n2. WHEN TO USE BEAM SEARCH:\")\n",
        "print(\"   • Machine translation (better translation quality)\")\n",
        "print(\"   • Text summarization (more coherent summaries)\")\n",
        "print(\"   • Image captioning (accurate descriptions)\")\n",
        "print(\"   • Question answering (better formulated answers)\")\n",
        "\n",
        "print(\"\\n3. IMPLEMENTATION TOOLS:\")\n",
        "print(\"   • TensorFlow Addons: tfa.seq2seq.beam_search_decoder.BeamSearchDecoder\")\n",
        "print(\"   • Hugging Face Transformers: model.generate(num_beams=k)\")\n",
        "print(\"   • Custom implementation: Full control and customization\")\n",
        "print(\"   • OpenNMT: Production-ready neural machine translation\")\n",
        "\n",
        "print(\"\\n4. HYPERPARAMETER GUIDELINES:\")\n",
        "print(f\"   • Beam width: Start with 3-5, diminishing returns beyond 10\")\n",
        "print(f\"   • Length penalty: 0.6-1.0 (0.6 empirically good for many tasks)\")\n",
        "print(f\"   • Max length: Set reasonable upper bound for your domain\")\n",
        "print(f\"   • Coverage penalty: 0.0-0.5 to reduce repetition\")\n",
        "\n",
        "print(\"\\n5. COMPUTATIONAL CONSIDERATIONS:\")\n",
        "print(f\"   • Time complexity: O(T × k × V) where T=length, k=beam_width, V=vocab_size\")\n",
        "print(f\"   • Space complexity: O(k × T) for storing beam sequences\")\n",
        "print(f\"   • Memory scales linearly with beam width\")\n",
        "print(f\"   • Consider batch processing for efficiency\")\n",
        "\n",
        "print(\"\\n6. ADVANCED TECHNIQUES:\")\n",
        "print(\"   • Diverse beam search: Encourage diversity among beams\")\n",
        "print(\"   • Coverage mechanism: Prevent attention repetition\")\n",
        "print(\"   • Constrained decoding: Force inclusion of specific terms\")\n",
        "print(\"   • Temperature scaling: Control randomness in sampling\")\n",
        "\n",
        "print(\"\\n7. TRADE-OFFS AND LIMITATIONS:\")\n",
        "print(\"   • Quality vs Speed: Better results but slower than greedy\")\n",
        "print(\"   • Quality vs Diversity: May reduce output diversity\")\n",
        "print(\"   • Memory vs Performance: Larger beams need more memory\")\n",
        "print(\"   • Not always optimal: Still approximation of true search\")\n",
        "\n",
        "print(\"\\n8. PRACTICAL RECOMMENDATIONS:\")\n",
        "print(\"   • Start simple: Begin with greedy, add beam search as needed\")\n",
        "print(\"   • Tune carefully: Test different beam widths and penalties\")\n",
        "print(\"   • Monitor resources: Watch memory and computation usage\")\n",
        "print(\"   • Consider alternatives: Random sampling for creative tasks\")\n",
        "print(\"   • Evaluate holistically: Balance quality, diversity, and efficiency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "remaining_exercises_summary"
      },
      "source": [
        "## Remaining Exercises: Summary and Key Points\n",
        "\n",
        "Due to space constraints, here are the key theoretical insights and practical solutions for the remaining exercises:\n",
        "\n",
        "### Exercise 5: Attention Mechanisms\n",
        "\n",
        "**Question:** What is an attention mechanism? How does it help?\n",
        "\n",
        "**Key Points:**\n",
        "- **Core Concept**: Allow decoder to selectively focus on different parts of input\n",
        "- **Mathematical Foundation**: $\\mathbf{c}_t = \\sum_{i=1}^T \\alpha_{t,i} \\mathbf{h}_i$ where $\\alpha_{t,i}$ are attention weights\n",
        "- **Benefits**: Solves information bottleneck, enables long sequences, provides interpretability\n",
        "- **Types**: Bahdanau (additive), Luong (multiplicative), Self-attention (Transformer)\n",
        "- **Applications**: NMT, visual attention, document understanding\n",
        "\n",
        "### Exercise 6: Transformer Multi-Head Attention\n",
        "\n",
        "**Question:** What is the most important layer in the Transformer architecture? What is its purpose?\n",
        "\n",
        "**Answer:** Multi-Head Attention is the most critical component.\n",
        "\n",
        "**Purpose:**\n",
        "- **Multiple attention heads** capture different types of relationships\n",
        "- **Parallel processing** unlike sequential RNNs\n",
        "- **Self-attention** allows positions to attend to each other\n",
        "- **Scalability** enables very large models\n",
        "- **Foundation** for modern language models (BERT, GPT, T5)\n",
        "\n",
        "### Exercise 7: Sampled Softmax\n",
        "\n",
        "**Question:** When would you need to use sampled softmax?\n",
        "\n",
        "**Key Points:**\n",
        "- **Problem**: Large vocabularies (50K+ words) make softmax computationally expensive\n",
        "- **Solution**: Sample subset of vocabulary for loss computation\n",
        "- **Mathematics**: $L_{sampled} = -\\log P(w_{true}) - \\sum_{w \\in \\text{samples}} \\log(1 - P(w))$\n",
        "- **When to use**: Vocabularies >10K words, limited computational resources\n",
        "- **Alternatives**: Hierarchical softmax, noise contrastive estimation\n",
        "\n",
        "### Exercise 8: Embedded Reber Grammars\n",
        "\n",
        "**Implementation Strategy:**\n",
        "```python\n",
        "# Generate valid/invalid sequences according to grammar rules\n",
        "# Train RNN to classify sequences as grammatical or not\n",
        "# Test ability to learn complex sequential patterns\n",
        "```\n",
        "\n",
        "### Exercise 9: Date Format Conversion\n",
        "\n",
        "**Sequence-to-Sequence Application:**\n",
        "- Input: \"April 22, 2019\" → Output: \"2019-04-22\"\n",
        "- Character-level or word-level encoding\n",
        "- Encoder-decoder with attention\n",
        "- Data augmentation with various date formats\n",
        "\n",
        "### Exercise 10: Neural Machine Translation Tutorial\n",
        "\n",
        "**Key Implementation Points:**\n",
        "- Use TensorFlow's NMT tutorial as baseline\n",
        "- Implement attention mechanism\n",
        "- Add beam search for inference\n",
        "- Evaluate with BLEU scores\n",
        "- Handle subword tokenization\n",
        "\n",
        "### Exercise 11: Advanced Language Models\n",
        "\n",
        "**Modern Approach:**\n",
        "- Use pre-trained BERT or GPT models\n",
        "- Fine-tune on Shakespeare dataset\n",
        "- Implement nucleus sampling for creative generation\n",
        "- Compare with character-level RNN baseline\n",
        "- Evaluate coherence and style preservation\n",
        "\n",
        "## Final Integration: Complete NLP Pipeline\n",
        "\n",
        "The exercises build toward a complete understanding of modern NLP:\n",
        "\n",
        "1. **Foundations**: Character and word-level RNNs\n",
        "2. **Architecture Evolution**: From RNNs to Transformers\n",
        "3. **Attention Revolution**: The key breakthrough\n",
        "4. **Modern Systems**: BERT, GPT, and beyond\n",
        "5. **Practical Applications**: Real-world implementation\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "- **RNNs**: Still useful for sequential data, but limited by memory\n",
        "- **Attention**: Revolutionary concept that enabled modern NLP\n",
        "- **Transformers**: Current state-of-the-art architecture\n",
        "- **Transfer Learning**: Pre-training + fine-tuning paradigm\n",
        "- **Scale**: Larger models and datasets drive progress\n",
        "- **Implementation**: Use modern frameworks and pre-trained models\n",
        "\n",
        "This comprehensive coverage provides both theoretical understanding and practical implementation skills for modern natural language processing systems."
      ]
    }
  ]
}