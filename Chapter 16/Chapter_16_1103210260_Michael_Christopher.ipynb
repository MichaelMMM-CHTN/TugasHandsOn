{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6e9f2ca74c5f4224aeecba007625f625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05ba3d932c9440b389700061f0b7da00",
              "IPY_MODEL_cd4bd956a5164d36b91558256668c8cd",
              "IPY_MODEL_52427f6257d74e30af014e1a1c086878"
            ],
            "layout": "IPY_MODEL_1d00ff05e8ac4b83874907ea29db58bb"
          }
        },
        "05ba3d932c9440b389700061f0b7da00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2c553fba00046baa84f127fd655a283",
            "placeholder": "​",
            "style": "IPY_MODEL_e38352ee89264b4ea46cd80ddd3a6db8",
            "value": "Dl Completed...: 100%"
          }
        },
        "cd4bd956a5164d36b91558256668c8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8f90453df5e41e8806d603eef44e3d3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80d3ed0882a6423aa653328c07452ea4",
            "value": 1
          }
        },
        "52427f6257d74e30af014e1a1c086878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dea491e8cb9431bb60bef30bea75d30",
            "placeholder": "​",
            "style": "IPY_MODEL_a69af730e347402388f03d5c2b94c12f",
            "value": " 1/1 [00:14&lt;00:00, 14.05s/ url]"
          }
        },
        "1d00ff05e8ac4b83874907ea29db58bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2c553fba00046baa84f127fd655a283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e38352ee89264b4ea46cd80ddd3a6db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8f90453df5e41e8806d603eef44e3d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "80d3ed0882a6423aa653328c07452ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9dea491e8cb9431bb60bef30bea75d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a69af730e347402388f03d5c2b94c12f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "725e5a53528b4efc90a26f79e72cb1bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88b0e3f2a6874f09be85b1c34683fc2e",
              "IPY_MODEL_07d6c3bc9ded4aef9174620796e9aded",
              "IPY_MODEL_d5bf94943f34453baaed77b2ddfb29ea"
            ],
            "layout": "IPY_MODEL_e07da3c5ce0d488186d08381db699f7f"
          }
        },
        "88b0e3f2a6874f09be85b1c34683fc2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f302e1045a3b47e0ac2dd5b0a1a80281",
            "placeholder": "​",
            "style": "IPY_MODEL_566b40bb90004f0cb909a00fb5cf2a2e",
            "value": "Dl Size...: 100%"
          }
        },
        "07d6c3bc9ded4aef9174620796e9aded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c4563235814403fa4abf16e3ed76f5b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbb94a27283047afa07f42f33085f490",
            "value": 1
          }
        },
        "d5bf94943f34453baaed77b2ddfb29ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a58342f14ef40d9a6dfbb723d86d032",
            "placeholder": "​",
            "style": "IPY_MODEL_3e8b9e23ad20464691f90a7fe335009c",
            "value": " 80/80 [00:14&lt;00:00, 10.06 MiB/s]"
          }
        },
        "e07da3c5ce0d488186d08381db699f7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f302e1045a3b47e0ac2dd5b0a1a80281": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "566b40bb90004f0cb909a00fb5cf2a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c4563235814403fa4abf16e3ed76f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "bbb94a27283047afa07f42f33085f490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a58342f14ef40d9a6dfbb723d86d032": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e8b9e23ad20464691f90a7fe335009c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efa94596947f4be8ab9f79841898eeba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8660431e7e84bdead7c1bf89050ab26",
              "IPY_MODEL_04db5f5e0e4e4e11bbbeed2172ebda37",
              "IPY_MODEL_b6bbccf52cef44eab1879cd0fbb06f15"
            ],
            "layout": "IPY_MODEL_67ae78dc46b741de86b86f95c070e50f"
          }
        },
        "c8660431e7e84bdead7c1bf89050ab26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_406b38a7061b44828b5a446fee9d31e1",
            "placeholder": "​",
            "style": "IPY_MODEL_27273e5bbc164b439e9fc8a1853730a3",
            "value": "Generating splits...: 100%"
          }
        },
        "04db5f5e0e4e4e11bbbeed2172ebda37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_474dc75c1f3f415abdec747f8dd2e533",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ee9f50934c64f3294c99b0de677bc32",
            "value": 3
          }
        },
        "b6bbccf52cef44eab1879cd0fbb06f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91b5e942564949758a2352fec6add204",
            "placeholder": "​",
            "style": "IPY_MODEL_96d3094ab42d4ce7980ffc6e128eea28",
            "value": " 3/3 [00:26&lt;00:00,  9.23s/ splits]"
          }
        },
        "67ae78dc46b741de86b86f95c070e50f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "406b38a7061b44828b5a446fee9d31e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27273e5bbc164b439e9fc8a1853730a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "474dc75c1f3f415abdec747f8dd2e533": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ee9f50934c64f3294c99b0de677bc32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91b5e942564949758a2352fec6add204": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96d3094ab42d4ce7980ffc6e128eea28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e96b0fc089e408e86c5266ba30ccbda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1bb610ae549e4f58ab6923f63fda4bbf",
              "IPY_MODEL_ca84f8ecc9714b9a82a12e08872e31a7",
              "IPY_MODEL_ba6fd51bf85b407099735a1270cbe684"
            ],
            "layout": "IPY_MODEL_25e859cfd510405581ea9248bebc581b"
          }
        },
        "1bb610ae549e4f58ab6923f63fda4bbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d701deef93694d1d94adc6e7f162a113",
            "placeholder": "​",
            "style": "IPY_MODEL_539abb1ce25b49a58cb281b8bbf0869b",
            "value": "Generating train examples...: "
          }
        },
        "ca84f8ecc9714b9a82a12e08872e31a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97334d5cfccc4dbfbf8e4962e86923ae",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0000f5a7e7694395a2d0f16b01caab38",
            "value": 1
          }
        },
        "ba6fd51bf85b407099735a1270cbe684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8fd303e209441848553a288a930b10c",
            "placeholder": "​",
            "style": "IPY_MODEL_5e44b0dff8d34ab6a68d84f7e56ad16e",
            "value": " 23279/? [00:04&lt;00:00, 5940.20 examples/s]"
          }
        },
        "25e859cfd510405581ea9248bebc581b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "d701deef93694d1d94adc6e7f162a113": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "539abb1ce25b49a58cb281b8bbf0869b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97334d5cfccc4dbfbf8e4962e86923ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0000f5a7e7694395a2d0f16b01caab38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8fd303e209441848553a288a930b10c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e44b0dff8d34ab6a68d84f7e56ad16e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d9a9660c0bb458185f96557af302079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a6c7cd3b38a40318d883debb84d13f7",
              "IPY_MODEL_552ee3cd46944df793046ce94432f579",
              "IPY_MODEL_fc64b0ae81634e3295680881765c54c6"
            ],
            "layout": "IPY_MODEL_1fbb3dc808af476188f22a08e2c2fa0b"
          }
        },
        "7a6c7cd3b38a40318d883debb84d13f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82a3675e242c49e3b9ab322549409029",
            "placeholder": "​",
            "style": "IPY_MODEL_5667654101f24955aa7fd8fd7c5a4fb1",
            "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.22Z47M_1.0.0/imdb_reviews-train.tfrecord*...:   0%"
          }
        },
        "552ee3cd46944df793046ce94432f579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_928c69f766dc4bd393d9e4badef5587e",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2223d20e62e2438281b1ca4c28050356",
            "value": 25000
          }
        },
        "fc64b0ae81634e3295680881765c54c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc2e0adef18f43419a5fa1e90ffe26d3",
            "placeholder": "​",
            "style": "IPY_MODEL_70686d5c86f040d4bc7c034c0a349d01",
            "value": " 0/25000 [00:00&lt;?, ? examples/s]"
          }
        },
        "1fbb3dc808af476188f22a08e2c2fa0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "82a3675e242c49e3b9ab322549409029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5667654101f24955aa7fd8fd7c5a4fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "928c69f766dc4bd393d9e4badef5587e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2223d20e62e2438281b1ca4c28050356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc2e0adef18f43419a5fa1e90ffe26d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70686d5c86f040d4bc7c034c0a349d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "030983d88da6453db2a50b2fc80e4fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f1069da70c64685b8d3461f7a3d3e77",
              "IPY_MODEL_804de0fefd3e41f1a260006838566ec4",
              "IPY_MODEL_0310e7ee77024552b66056a1d0d7b8d4"
            ],
            "layout": "IPY_MODEL_1572033545334b66a8732da0d46747fd"
          }
        },
        "8f1069da70c64685b8d3461f7a3d3e77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a719295e21d941ccb518c7378b8765c7",
            "placeholder": "​",
            "style": "IPY_MODEL_935b7e4fad144e778bbbc3c3beb629f3",
            "value": "Generating test examples...: "
          }
        },
        "804de0fefd3e41f1a260006838566ec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb69afa559a84017a6b751be83fe988d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8baf6b23c3d4422b2a76b2857489c3f",
            "value": 1
          }
        },
        "0310e7ee77024552b66056a1d0d7b8d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_075d95ccc9204469ba6bde74f8ae7a38",
            "placeholder": "​",
            "style": "IPY_MODEL_b567f1fe498f4e789f5290d1aba95e61",
            "value": " 23855/? [00:04&lt;00:00, 6018.94 examples/s]"
          }
        },
        "1572033545334b66a8732da0d46747fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "a719295e21d941ccb518c7378b8765c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "935b7e4fad144e778bbbc3c3beb629f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb69afa559a84017a6b751be83fe988d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b8baf6b23c3d4422b2a76b2857489c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "075d95ccc9204469ba6bde74f8ae7a38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b567f1fe498f4e789f5290d1aba95e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4f41922fa574a0e8519848fec3e01c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c308d6a4eded470fa4612ff470f92bdc",
              "IPY_MODEL_9fdf50dee3d84b14bfead787a072d756",
              "IPY_MODEL_95af438272224dc5a93910ff00084b54"
            ],
            "layout": "IPY_MODEL_9119210e328a41e8bcc6279a8c289c4a"
          }
        },
        "c308d6a4eded470fa4612ff470f92bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ff50c7326d3483594ffbd4daaa62feb",
            "placeholder": "​",
            "style": "IPY_MODEL_70c6bd54b4d14fecbb9eb884a1156184",
            "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.22Z47M_1.0.0/imdb_reviews-test.tfrecord*...:   0%"
          }
        },
        "9fdf50dee3d84b14bfead787a072d756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0f315ae2b594de3a419f589f034a8fc",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b91ad7e573034ad79d163b6f8d67a210",
            "value": 25000
          }
        },
        "95af438272224dc5a93910ff00084b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_407543d141754e3494c187bddaf291c1",
            "placeholder": "​",
            "style": "IPY_MODEL_5c0c251692dc4456bd43fa7d3ce2b20e",
            "value": " 0/25000 [00:00&lt;?, ? examples/s]"
          }
        },
        "9119210e328a41e8bcc6279a8c289c4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "1ff50c7326d3483594ffbd4daaa62feb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70c6bd54b4d14fecbb9eb884a1156184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0f315ae2b594de3a419f589f034a8fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b91ad7e573034ad79d163b6f8d67a210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "407543d141754e3494c187bddaf291c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c0c251692dc4456bd43fa7d3ce2b20e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9856f099bf5a43bf86c1c8688521d9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9bd5b462a9744c9acb98ea10e462bb1",
              "IPY_MODEL_ffbac5872e9d4d22b14722702fbee62c",
              "IPY_MODEL_7878fdacf4154eb18d8c245af8feb89d"
            ],
            "layout": "IPY_MODEL_ee3ec672de9949ddb2954c1dba8633bd"
          }
        },
        "a9bd5b462a9744c9acb98ea10e462bb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8be1d7fd32f64efc8f2b0ab298c79ee0",
            "placeholder": "​",
            "style": "IPY_MODEL_23a5cec3aaa84471858074a196bca5a3",
            "value": "Generating unsupervised examples...: "
          }
        },
        "ffbac5872e9d4d22b14722702fbee62c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ed8ae2240354ce79066fdf6f7a54875",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ecac5ce38cd409b863d954e07fed3d9",
            "value": 1
          }
        },
        "7878fdacf4154eb18d8c245af8feb89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b7f9b217bef41dc8e9cd7b8890e396b",
            "placeholder": "​",
            "style": "IPY_MODEL_f1f7a426bbb74d89a7b7f19848fa273f",
            "value": " 46522/? [00:09&lt;00:00, 6868.34 examples/s]"
          }
        },
        "ee3ec672de9949ddb2954c1dba8633bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "8be1d7fd32f64efc8f2b0ab298c79ee0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23a5cec3aaa84471858074a196bca5a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ed8ae2240354ce79066fdf6f7a54875": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0ecac5ce38cd409b863d954e07fed3d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b7f9b217bef41dc8e9cd7b8890e396b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f7a426bbb74d89a7b7f19848fa273f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6069e3e1d2fa44fd97d2c2c28dc538a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80d54edbecbe41efa6b3688b3e0c53e4",
              "IPY_MODEL_a6246d8236314ee38685d0ccbd761339",
              "IPY_MODEL_1d2fd5ed56a848cf920be7cfd76b2eb8"
            ],
            "layout": "IPY_MODEL_e0957c2278454296bdeb9f42df1d9537"
          }
        },
        "80d54edbecbe41efa6b3688b3e0c53e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1dc58e797cf4ad1bb06455f6d3a337a",
            "placeholder": "​",
            "style": "IPY_MODEL_d5b8778d4b9548dfab3bde5985d8c73c",
            "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.22Z47M_1.0.0/imdb_reviews-unsupervised.tfrecord*...:   0%"
          }
        },
        "a6246d8236314ee38685d0ccbd761339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68ae78f1b04f4142a54612af33556f2b",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb61da1ba08e406182e734dcd8bb1e01",
            "value": 50000
          }
        },
        "1d2fd5ed56a848cf920be7cfd76b2eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf8d4865a33b4fd78679b8f640633e1c",
            "placeholder": "​",
            "style": "IPY_MODEL_da7bd6465d9c428da85d810d1658a99d",
            "value": " 0/50000 [00:00&lt;?, ? examples/s]"
          }
        },
        "e0957c2278454296bdeb9f42df1d9537": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "e1dc58e797cf4ad1bb06455f6d3a337a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5b8778d4b9548dfab3bde5985d8c73c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68ae78f1b04f4142a54612af33556f2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb61da1ba08e406182e734dcd8bb1e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf8d4865a33b4fd78679b8f640633e1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da7bd6465d9c428da85d810d1658a99d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Chapter 16: Natural Language Processing with RNNs and Attention\n",
        "\n",
        "## Comprehensive Implementation and Theory Guide\n",
        "\n",
        "This notebook provides a complete implementation and theoretical explanation of Chapter 16 from \"Hands-On Machine Learning\" by Aurélien Géron, covering Natural Language Processing with RNNs and Attention mechanisms.\n",
        "\n",
        "### Learning Objectives:\n",
        "1. **Character-level RNNs** for text generation\n",
        "2. **Sentiment Analysis** using word-level RNNs\n",
        "3. **Encoder-Decoder Networks** for Neural Machine Translation\n",
        "4. **Attention Mechanisms** (Bahdanau, Luong, Self-Attention)\n",
        "5. **Transformer Architecture** - the revolutionary \"Attention is All You Need\" model\n",
        "6. **Recent Language Models** (ELMo, ULMFiT, GPT, BERT)\n",
        "\n",
        "### Mathematical Foundations:\n",
        "We'll cover the mathematical theory behind each concept, including:\n",
        "- RNN forward propagation equations\n",
        "- Attention mechanism formulations\n",
        "- Scaled Dot-Product Attention\n",
        "- Multi-Head Attention mathematics\n",
        "- Positional encoding formulas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## Setup and Imports\n",
        "\n",
        "First, let's set up our environment with all necessary dependencies for this comprehensive NLP tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05ee27db-456f-413b-9b5d-f8b4310c9bcf"
      },
      "source": [
        "# Install required packages for advanced NLP\n",
        "!pip install \"tensorflow\" tensorflow-datasets tensorflow-hub\n",
        "!pip install transformers datasets tokenizers\n",
        "!pip install matplotlib seaborn plotly\n",
        "!pip install numpy pandas scipy scikit-learn\n",
        "\n",
        "# Clone the hands-on-ml2 repository for additional resources\n",
        "!git clone https://github.com/ageron/handson-ml2.git\n",
        "%cd handson-ml2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.11/dist-packages (4.9.9)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.11/dist-packages (0.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.7.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.1.9)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (1.12.2)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (18.1.0)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (1.17.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (4.67.1)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub) (2.18.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.23.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-datasets) (25.3.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow-datasets) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.70.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Cloning into 'handson-ml2'...\n",
            "remote: Enumerating objects: 3100, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 3100 (delta 0), reused 0 (delta 0), pack-reused 3096 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3100/3100), 150.92 MiB | 41.52 MiB/s, done.\n",
            "Resolving deltas: 100% (1905/1905), done.\n",
            "/content/handson-ml2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53db25e0-cefc-4ff8-cc5a-b39c3176030a"
      },
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, losses, metrics\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Configure TensorFlow\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {tf.config.experimental.list_physical_devices('GPU')}\")\n",
        "print(f\"Eager execution: {tf.executing_eagerly()}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Eager execution: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_theory"
      },
      "source": [
        "## Introduction: The Turing Test and Language Understanding\n",
        "\n",
        "### Historical Context\n",
        "\n",
        "Alan Turing's famous 1950 paper \"Computing Machinery and Intelligence\" proposed what became known as the **Turing Test**. Interestingly, Turing chose a **linguistic task** to evaluate machine intelligence, highlighting that mastering language is arguably Homo sapiens's greatest cognitive ability.\n",
        "\n",
        "### The Turing Test (Imitation Game)\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "Let $M$ be a machine, $H$ be a human, and $I$ be an interrogator. The test can be formalized as:\n",
        "\n",
        "$$P(\\text{Machine passes}) = P(I \\text{ mistakes } M \\text{ for } H | \\text{text-only conversation})$$\n",
        "\n",
        "The machine passes if it can fool the interrogator into thinking it's human.\n",
        "\n",
        "### Why RNNs for NLP?\n",
        "\n",
        "Natural language has **sequential structure** where:\n",
        "1. **Order matters**: \"The cat sat on the mat\" ≠ \"Mat the on sat cat the\"\n",
        "2. **Context dependency**: Meaning depends on previous words\n",
        "3. **Variable length**: Sentences have different lengths\n",
        "4. **Long-term dependencies**: Words can reference concepts mentioned much earlier\n",
        "\n",
        "**RNN Mathematical Foundation:**\n",
        "For a sequence $x_1, x_2, ..., x_T$, an RNN computes:\n",
        "\n",
        "$$h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)$$\n",
        "$$y_t = W_{hy} \\cdot h_t + b_y$$\n",
        "\n",
        "Where:\n",
        "- $h_t$: hidden state at time $t$\n",
        "- $x_t$: input at time $t$\n",
        "- $y_t$: output at time $t$\n",
        "- $W_{xh}, W_{hh}, W_{hy}$: weight matrices\n",
        "- $b_h, b_y$: bias vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "char_rnn_theory"
      },
      "source": [
        "## Part 1: Character-Level RNNs for Text Generation\n",
        "\n",
        "### Theory: Character-Level Language Modeling\n",
        "\n",
        "A **Character RNN** predicts the next character in a sequence, learning to model the probability distribution:\n",
        "\n",
        "$$P(c_{t+1} | c_1, c_2, ..., c_t)$$\n",
        "\n",
        "Where $c_i$ represents the $i$-th character in the sequence.\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "For character-level modeling:\n",
        "1. **Input encoding**: Each character $c$ is mapped to a one-hot vector $x \\in \\mathbb{R}^V$ where $V$ is vocabulary size\n",
        "2. **Hidden state evolution**: $h_t = \\text{GRU}(x_t, h_{t-1})$ or $h_t = \\text{LSTM}(x_t, h_{t-1})$\n",
        "3. **Output prediction**: $y_t = \\text{softmax}(W_o h_t + b_o)$ gives probability distribution over characters\n",
        "4. **Loss function**: Cross-entropy loss $L = -\\sum_{t=1}^T \\log P(c_{t+1} | c_1, ..., c_t)$\n",
        "\n",
        "### Why Character-Level?\n",
        "\n",
        "**Advantages:**\n",
        "- **Smaller vocabulary**: Only need ~100 characters vs 50K+ words\n",
        "- **No out-of-vocabulary issues**: Can handle any text\n",
        "- **Learns morphology**: Understands word structure\n",
        "- **Language agnostic**: Works across languages\n",
        "\n",
        "**Disadvantages:**\n",
        "- **Longer sequences**: More time steps to process\n",
        "- **Harder to capture long-term dependencies**: Character-level patterns vs word-level semantics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_prep_theory"
      },
      "source": [
        "### Dataset Preparation Theory\n",
        "\n",
        "#### Sequential Dataset Splitting\n",
        "\n",
        "Unlike image classification, we **cannot randomly shuffle** sequential data. Proper splitting for time series/sequential data:\n",
        "\n",
        "**Temporal Split:**\n",
        "- Training: First 90% chronologically\n",
        "- Validation: Next 5%\n",
        "- Test: Final 5%\n",
        "\n",
        "**Mathematical Consideration - Stationarity:**\n",
        "We assume the time series is **stationary**, meaning statistical properties don't change over time:\n",
        "$$E[X_t] = \\mu \\text{ (constant mean)}$$\n",
        "$$\\text{Var}(X_t) = \\sigma^2 \\text{ (constant variance)}$$\n",
        "$$\\text{Cov}(X_t, X_{t+k}) = \\gamma(k) \\text{ (covariance depends only on lag)}$$\n",
        "\n",
        "#### Windowing Strategy\n",
        "\n",
        "**Problem:** Cannot train on entire sequence (would be like a network with millions of layers)\n",
        "\n",
        "**Solution:** **Truncated Backpropagation Through Time (TBPTT)**\n",
        "\n",
        "Split long sequence into overlapping windows:\n",
        "- Window 1: characters 0-100\n",
        "- Window 2: characters 1-101  \n",
        "- Window 3: characters 2-102\n",
        "- ...\n",
        "\n",
        "**Mathematical Impact:**\n",
        "- **Gradient flow**: Limited to window size\n",
        "- **Memory requirements**: $O(\\text{window_size})$ instead of $O(\\text{sequence_length})$\n",
        "- **Trade-off**: Shorter windows → faster training but limited long-term learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shakespeare_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29bacb56-fe8f-4f12-b7b9-4b5205604bb1"
      },
      "source": [
        "# Download Shakespeare's complete works\n",
        "# This demonstrates the data acquisition step from the book\n",
        "shakespeare_url = \"https://homl.info/shakespeare\"\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "\n",
        "# Read the text data\n",
        "with open(filepath, 'r', encoding='utf-8') as f:\n",
        "    shakespeare_text = f.read()\n",
        "\n",
        "print(f\"Shakespeare corpus statistics:\")\n",
        "print(f\"Total characters: {len(shakespeare_text):,}\")\n",
        "print(f\"First 200 characters:\")\n",
        "print(repr(shakespeare_text[:200]))\n",
        "\n",
        "# Character frequency analysis\n",
        "char_freq = Counter(shakespeare_text)\n",
        "print(f\"\\nUnique characters: {len(char_freq)}\")\n",
        "print(f\"Most common characters: {char_freq.most_common(10)}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Shakespeare corpus statistics:\n",
            "Total characters: 1,115,394\n",
            "First 200 characters:\n",
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you'\n",
            "\n",
            "Unique characters: 65\n",
            "Most common characters: [(' ', 169892), ('e', 94611), ('t', 67009), ('o', 65798), ('a', 55507), ('h', 51310), ('s', 49696), ('r', 48889), ('n', 48529), ('i', 45537)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tokenization_theory"
      },
      "source": [
        "### Tokenization Theory\n",
        "\n",
        "**Tokenization** converts text into numerical sequences for neural network processing.\n",
        "\n",
        "#### Character-Level Tokenization\n",
        "\n",
        "**Process:**\n",
        "1. **Vocabulary creation**: $V = \\{c_1, c_2, ..., c_{|V|}\\}$ where each $c_i$ is a unique character\n",
        "2. **Character mapping**: $f: c \\rightarrow \\mathbb{N}$ where $f(c_i) = i$\n",
        "3. **Sequence encoding**: Text \"hello\" → [8, 5, 12, 12, 15] (example indices)\n",
        "\n",
        "**One-Hot Encoding:**\n",
        "Each character index $i$ becomes a vector $e_i \\in \\{0,1\\}^{|V|}$ where:\n",
        "$$e_i[j] = \\begin{cases} 1 & \\text{if } j = i \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
        "\n",
        "**Memory Complexity:**\n",
        "- **Dense representation**: $O(T)$ for sequence length $T$\n",
        "- **One-hot representation**: $O(T \\times |V|)$\n",
        "- **Embedding representation**: $O(T \\times d)$ where $d$ is embedding dimension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tokenizer_implementation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "149c855d-a96a-452a-971c-57705b249ea9"
      },
      "source": [
        "# Character-level tokenization implementation\n",
        "# This follows the exact approach from the book\n",
        "\n",
        "# Create character-level tokenizer\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts([shakespeare_text])\n",
        "\n",
        "# Analyze tokenizer properties\n",
        "max_id = len(tokenizer.word_index)  # Number of distinct characters\n",
        "dataset_size = len(shakespeare_text)  # Total number of characters\n",
        "\n",
        "print(f\"Tokenizer Analysis:\")\n",
        "print(f\"Vocabulary size (unique characters): {max_id}\")\n",
        "print(f\"Total dataset size: {dataset_size:,} characters\")\n",
        "\n",
        "# Demonstrate encoding and decoding\n",
        "test_text = \"First\"\n",
        "encoded = tokenizer.texts_to_sequences([test_text])\n",
        "decoded = tokenizer.sequences_to_texts(encoded)\n",
        "\n",
        "print(f\"\\nTokenization demonstration:\")\n",
        "print(f\"Original: '{test_text}'\")\n",
        "print(f\"Encoded: {encoded[0]}\")\n",
        "print(f\"Decoded: '{decoded[0]}'\")\n",
        "\n",
        "# Show character to ID mapping for first 20 characters\n",
        "print(f\"\\nCharacter to ID mapping (first 20):\")\n",
        "for char, idx in list(tokenizer.word_index.items())[:20]:\n",
        "    print(f\"'{char}' -> {idx}\")\n",
        "\n",
        "# Encode the entire text (subtract 1 to get IDs from 0 to max_id-1)\n",
        "[encoded_text] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
        "print(f\"\\nEncoded text shape: {encoded_text.shape}\")\n",
        "print(f\"First 50 encoded characters: {encoded_text[:50]}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer Analysis:\n",
            "Vocabulary size (unique characters): 39\n",
            "Total dataset size: 1,115,394 characters\n",
            "\n",
            "Tokenization demonstration:\n",
            "Original: 'First'\n",
            "Encoded: [20, 6, 9, 8, 3]\n",
            "Decoded: 'f i r s t'\n",
            "\n",
            "Character to ID mapping (first 20):\n",
            "' ' -> 1\n",
            "'e' -> 2\n",
            "'t' -> 3\n",
            "'o' -> 4\n",
            "'a' -> 5\n",
            "'i' -> 6\n",
            "'h' -> 7\n",
            "'s' -> 8\n",
            "'r' -> 9\n",
            "'n' -> 10\n",
            "'\n",
            "' -> 11\n",
            "'l' -> 12\n",
            "'d' -> 13\n",
            "'u' -> 14\n",
            "'m' -> 15\n",
            "'y' -> 16\n",
            "'w' -> 17\n",
            "',' -> 18\n",
            "'c' -> 19\n",
            "'f' -> 20\n",
            "\n",
            "Encoded text shape: (1115394,)\n",
            "First 50 encoded characters: [19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1\n",
            "  0 22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1\n",
            "  4  8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "windowing_theory"
      },
      "source": [
        "### Windowing and Dataset Creation Theory\n",
        "\n",
        "#### The Windowing Problem\n",
        "\n",
        "**Challenge:** Transform a single long sequence into multiple training examples\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "Given sequence $S = [s_1, s_2, ..., s_N]$ of length $N$, create windows of size $w$:\n",
        "\n",
        "$$W_i = [s_i, s_{i+1}, ..., s_{i+w-1}] \\text{ for } i = 1, 2, ..., N-w+1$$\n",
        "\n",
        "**Input-Target Pairs:**\n",
        "For next-character prediction:\n",
        "- **Input**: $X_i = [s_i, s_{i+1}, ..., s_{i+w-2}]$ (first $w-1$ characters)\n",
        "- **Target**: $Y_i = [s_{i+1}, s_{i+2}, ..., s_{i+w-1}]$ (shifted by 1)\n",
        "\n",
        "#### TensorFlow Dataset Operations\n",
        "\n",
        "**Key Operations:**\n",
        "1. **`tf.data.Dataset.from_tensor_slices()`**: Creates dataset from tensor\n",
        "2. **`dataset.window()`**: Creates sliding windows\n",
        "3. **`dataset.flat_map()`**: Flattens nested datasets\n",
        "4. **`dataset.batch()`**: Groups examples into batches\n",
        "5. **`dataset.map()`**: Applies transformations\n",
        "6. **`dataset.shuffle()`**: Randomizes order\n",
        "7. **`dataset.prefetch()`**: Optimizes pipeline performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dataset_creation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1566cd-c685-4d3c-a68a-a3d01d61c436"
      },
      "source": [
        "# Dataset creation following the book's methodology\n",
        "# This demonstrates the complete pipeline from raw text to training data\n",
        "\n",
        "# Split data: 90% train, 10% validation/test\n",
        "train_size = dataset_size * 90 // 100\n",
        "train_encoded = encoded_text[:train_size]\n",
        "val_encoded = encoded_text[train_size:]\n",
        "\n",
        "print(f\"Data splitting:\")\n",
        "print(f\"Training size: {len(train_encoded):,} characters\")\n",
        "print(f\"Validation size: {len(val_encoded):,} characters\")\n",
        "\n",
        "# Create TensorFlow dataset from training data\n",
        "dataset = tf.data.Dataset.from_tensor_slices(train_encoded)\n",
        "\n",
        "# Define window parameters\n",
        "n_steps = 100  # Sequence length for training\n",
        "window_length = n_steps + 1  # +1 because target = input shifted by 1\n",
        "\n",
        "print(f\"\\nWindow configuration:\")\n",
        "print(f\"Input sequence length: {n_steps}\")\n",
        "print(f\"Window length (input + target): {window_length}\")\n",
        "\n",
        "# Create sliding windows with overlap\n",
        "# shift=1 means each window starts 1 character after the previous\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
        "\n",
        "# Convert nested dataset to flat dataset of tensors\n",
        "# Each window becomes a single tensor of shape [window_length]\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "# Batch windows together for efficient training\n",
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "\n",
        "# Split each window into input (first n_steps) and target (last n_steps)\n",
        "# Input: characters 0 to n_steps-1\n",
        "# Target: characters 1 to n_steps (shifted by 1)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "\n",
        "# Convert character indices to one-hot vectors\n",
        "# This is necessary for the embedding layer or direct one-hot input\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "\n",
        "# Optimize dataset performance\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Examine a sample batch\n",
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(f\"\\nBatch shapes:\")\n",
        "    print(f\"Input shape: {X_batch.shape}\")\n",
        "    print(f\"Target shape: {Y_batch.shape}\")\n",
        "    print(f\"\\nFirst example in batch:\")\n",
        "    print(f\"Input characters (first 10): {tf.argmax(X_batch[0][:10], axis=-1).numpy()}\")\n",
        "    print(f\"Target characters (first 10): {Y_batch[0][:10].numpy()}\")\n",
        "\n",
        "    # Verify the shift relationship\n",
        "    input_chars = tf.argmax(X_batch[0], axis=-1).numpy()\n",
        "    target_chars = Y_batch[0].numpy()\n",
        "    print(f\"\\nVerifying shift relationship (first 5 pairs):\")\n",
        "    for i in range(5):\n",
        "        print(f\"Input[{i}]: {input_chars[i]} -> Target[{i}]: {target_chars[i]} (should be Input[{i+1}]: {input_chars[i+1] if i+1 < len(input_chars) else 'N/A'})\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data splitting:\n",
            "Training size: 1,003,854 characters\n",
            "Validation size: 111,540 characters\n",
            "\n",
            "Window configuration:\n",
            "Input sequence length: 100\n",
            "Window length (input + target): 101\n",
            "\n",
            "Batch shapes:\n",
            "Input shape: (32, 100, 39)\n",
            "Target shape: (32, 100)\n",
            "\n",
            "First example in batch:\n",
            "Input characters (first 10): [ 6  5  7  0  7  4 15  7  0  2]\n",
            "Target characters (first 10): [ 5  7  0  7  4 15  7  0  2  6]\n",
            "\n",
            "Verifying shift relationship (first 5 pairs):\n",
            "Input[0]: 6 -> Target[0]: 5 (should be Input[1]: 5)\n",
            "Input[1]: 5 -> Target[1]: 7 (should be Input[2]: 7)\n",
            "Input[2]: 7 -> Target[2]: 0 (should be Input[3]: 0)\n",
            "Input[3]: 0 -> Target[3]: 7 (should be Input[4]: 7)\n",
            "Input[4]: 7 -> Target[4]: 4 (should be Input[5]: 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "char_rnn_model_theory"
      },
      "source": [
        "### Character RNN Model Architecture Theory\n",
        "\n",
        "#### GRU vs LSTM for Text Generation\n",
        "\n",
        "**GRU (Gated Recurrent Unit) Mathematical Formulation:**\n",
        "\n",
        "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$$\n",
        "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$$\n",
        "$$\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t])$$\n",
        "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
        "\n",
        "Where:\n",
        "- $r_t$: reset gate (controls how much past information to forget)\n",
        "- $z_t$: update gate (controls how much new information to accept)\n",
        "- $\\tilde{h}_t$: candidate hidden state\n",
        "- $\\odot$: element-wise multiplication\n",
        "- $\\sigma$: sigmoid function\n",
        "\n",
        "**Why GRU for Text Generation?**\n",
        "1. **Simpler than LSTM**: Fewer parameters (2 gates vs 3)\n",
        "2. **Good performance**: Often matches LSTM performance\n",
        "3. **Faster training**: Fewer computations per time step\n",
        "4. **Better gradient flow**: Less vanishing gradient issues than vanilla RNN\n",
        "\n",
        "#### Dropout in RNNs\n",
        "\n",
        "**Two types of dropout:**\n",
        "1. **Input dropout**: Applied to inputs at each time step\n",
        "2. **Recurrent dropout**: Applied to hidden state connections\n",
        "\n",
        "**Mathematical formulation:**\n",
        "$$h_t = \\text{GRU}(\\text{dropout}(x_t), \\text{recurrent_dropout}(h_{t-1}))$$\n",
        "\n",
        "**Benefits:**\n",
        "- **Regularization**: Prevents overfitting\n",
        "- **Improved generalization**: Forces model to not rely on specific neurons\n",
        "- **Better text diversity**: Reduces repetitive patterns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "char_rnn_model",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "e93e6404-9f3a-4bac-999a-51daf8ff1969"
      },
      "source": [
        "# Character RNN Model Implementation\n",
        "# This follows the exact architecture described in the book\n",
        "\n",
        "def create_char_rnn_model(vocab_size, embedding_dim=None, rnn_units=128,\n",
        "                         dropout_rate=0.2, num_layers=2):\n",
        "    \"\"\"\n",
        "    Create a character-level RNN model for text generation.\n",
        "\n",
        "    Args:\n",
        "        vocab_size: Size of character vocabulary\n",
        "        embedding_dim: Dimension of character embeddings (None for one-hot)\n",
        "        rnn_units: Number of units in each GRU layer\n",
        "        dropout_rate: Dropout rate for regularization\n",
        "        num_layers: Number of GRU layers\n",
        "\n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    model = keras.models.Sequential([\n",
        "        # First GRU layer\n",
        "        # return_sequences=True: return full sequence, not just last output\n",
        "        # input_shape=[None, vocab_size]: variable length sequences, one-hot encoded\n",
        "        keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            input_shape=[None, vocab_size],\n",
        "            dropout=dropout_rate,           # Input dropout\n",
        "            recurrent_dropout=dropout_rate  # Recurrent dropout\n",
        "        ),\n",
        "\n",
        "        # Second GRU layer\n",
        "        keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            dropout=dropout_rate,\n",
        "            recurrent_dropout=dropout_rate\n",
        "        ),\n",
        "\n",
        "        # Output layer: Dense layer applied at each time step\n",
        "        # TimeDistributed applies the Dense layer to each time step independently\n",
        "        keras.layers.TimeDistributed(\n",
        "            keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "char_rnn_model = create_char_rnn_model(\n",
        "    vocab_size=max_id,\n",
        "    rnn_units=128,\n",
        "    dropout_rate=0.2,\n",
        "    num_layers=2\n",
        ")\n",
        "\n",
        "# Display model architecture\n",
        "print(\"Character RNN Model Architecture:\")\n",
        "char_rnn_model.summary()\n",
        "\n",
        "# Compile the model\n",
        "# sparse_categorical_crossentropy: targets are integers, not one-hot\n",
        "char_rnn_model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Calculate total parameters\n",
        "total_params = char_rnn_model.count_params()\n",
        "print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
        "\n",
        "# Analyze parameter distribution\n",
        "print(\"\\nParameter distribution by layer:\")\n",
        "for i, layer in enumerate(char_rnn_model.layers):\n",
        "    layer_params = layer.count_params()\n",
        "    print(f\"Layer {i+1} ({layer.__class__.__name__}): {layer_params:,} parameters\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character RNN Model Architecture:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m64,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m99,072\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m)       │         \u001b[38;5;34m5,031\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">64,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,031</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m168,999\u001b[0m (660.15 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">168,999</span> (660.15 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m168,999\u001b[0m (660.15 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">168,999</span> (660.15 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total trainable parameters: 168,999\n",
            "\n",
            "Parameter distribution by layer:\n",
            "Layer 1 (GRU): 64,896 parameters\n",
            "Layer 2 (GRU): 99,072 parameters\n",
            "Layer 3 (TimeDistributed): 5,031 parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_theory"
      },
      "source": [
        "### Training Theory and Optimization\n",
        "\n",
        "#### Loss Function: Sparse Categorical Crossentropy\n",
        "\n",
        "For character prediction, we use **sparse categorical crossentropy**:\n",
        "\n",
        "$$L = -\\frac{1}{T} \\sum_{t=1}^{T} \\log P(c_{t+1}^{\\text{true}} | c_1, ..., c_t)$$\n",
        "\n",
        "Where:\n",
        "- $T$: sequence length\n",
        "- $c_{t+1}^{\\text{true}}$: true next character (integer index)\n",
        "- $P(c_{t+1}^{\\text{true}} | c_1, ..., c_t)$: predicted probability of true character\n",
        "\n",
        "#### Gradient Computation in RNNs\n",
        "\n",
        "**Backpropagation Through Time (BPTT):**\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial W} = \\sum_{t=1}^{T} \\frac{\\partial L_t}{\\partial W}$$\n",
        "\n",
        "$$\\frac{\\partial L_t}{\\partial W} = \\frac{\\partial L_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial W} + \\sum_{k=1}^{t-1} \\frac{\\partial L_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial h_k} \\frac{\\partial h_k}{\\partial W}$$\n",
        "\n",
        "**Vanishing Gradient Problem:**\n",
        "$$\\frac{\\partial h_t}{\\partial h_k} = \\prod_{i=k+1}^{t} \\frac{\\partial h_i}{\\partial h_{i-1}}$$\n",
        "\n",
        "If $\\left|\\frac{\\partial h_i}{\\partial h_{i-1}}\\right| < 1$, then gradients vanish exponentially with sequence length.\n",
        "\n",
        "#### Adam Optimizer\n",
        "\n",
        "**Adaptive Moment Estimation:**\n",
        "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n",
        "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n",
        "$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n",
        "$$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
        "$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$\n",
        "\n",
        "Where:\n",
        "- $g_t$: gradient at step $t$\n",
        "- $m_t$: first moment estimate (momentum)\n",
        "- $v_t$: second moment estimate (variance)\n",
        "- $\\beta_1, \\beta_2$: decay rates (typically 0.9, 0.999)\n",
        "- $\\alpha$: learning rate\n",
        "- $\\epsilon$: numerical stability term"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "training_char_rnn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "08b704df-200f-44c1-a343-fdda56cc4804"
      },
      "source": [
        "# Training the Character RNN\n",
        "# This demonstrates the training process with monitoring and callbacks\n",
        "\n",
        "# Re-import necessary libraries to ensure they are defined\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create callbacks for monitoring training\n",
        "callbacks = [\n",
        "    # Reduce learning rate when validation loss plateaus\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=0.001,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Early stopping to prevent overfitting\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Save model checkpoints\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        'char_rnn_best.weights.h5', # Corrected file extension\n",
        "        monitor='loss',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting Character RNN training...\")\n",
        "# Get dataset size for progress tracking\n",
        "# Note: This takes time for large datasets, consider taking a subset for faster testing\n",
        "try:\n",
        "    steps_per_epoch = len(list(dataset.take(-1)))\n",
        "except:\n",
        "    # Fallback for very large datasets where len(list(dataset)) is too slow\n",
        "    steps_per_epoch = 1000 # Estimate or set a reasonable number\n",
        "\n",
        "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "\n",
        "\n",
        "# Note: Training on full dataset takes hours. For demonstration, we'll train on subset\n",
        "# For full training, increase epochs to 20-50\n",
        "epochs = 3  # Reduced for demonstration\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = char_rnn_model.fit(\n",
        "    dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.title('Model Loss During Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.title('Model Accuracy During Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final metrics\n",
        "final_loss = history.history['loss'][-1]\n",
        "final_accuracy = history.history['accuracy'][-1]\n",
        "print(f\"\\nFinal Training Metrics:\")\n",
        "print(f\"Loss: {final_loss:.4f}\")\n",
        "print(f\"Accuracy: {final_accuracy:.4f}\")\n",
        "print(f\"Perplexity: {np.exp(final_loss):.2f}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Character RNN training...\n",
            "Steps per epoch: 31368\n",
            "Epoch 1/3\n",
            "  11979/Unknown \u001b[1m4806s\u001b[0m 400ms/step - accuracy: 0.4726 - loss: 1.7524"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-676402553.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m history = char_rnn_model.fit(\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "text_generation_theory"
      },
      "source": [
        "### Text Generation Theory\n",
        "\n",
        "#### Sampling Strategies\n",
        "\n",
        "**1. Greedy Sampling:**\n",
        "Always pick the most likely next character:\n",
        "$$c_{t+1} = \\arg\\max_c P(c | c_1, ..., c_t)$$\n",
        "\n",
        "**Problem:** Often leads to repetitive, boring text\n",
        "\n",
        "**2. Random Sampling with Temperature:**\n",
        "Sample from probability distribution with temperature scaling:\n",
        "\n",
        "$$P_\\tau(c_i | \\text{context}) = \\frac{\\exp(z_i / \\tau)}{\\sum_j \\exp(z_j / \\tau)}$$\n",
        "\n",
        "Where:\n",
        "- $z_i$: logit for character $i$\n",
        "- $\\tau$: temperature parameter\n",
        "\n",
        "**Temperature Effects:**\n",
        "- $\\tau \\to 0$: **Deterministic** (greedy sampling)\n",
        "- $\\tau = 1$: **Original distribution**\n",
        "- $\\tau > 1$: **More random** (flatter distribution)\n",
        "- $\\tau \\to \\infty$: **Uniform random**\n",
        "\n",
        "#### TensorFlow's tf.random.categorical()\n",
        "\n",
        "Samples from categorical distribution given log probabilities:\n",
        "$$\\text{categorical}(\\text{logits}) \\sim \\text{Multinomial}(1, \\text{softmax}(\\text{logits}))$$\n",
        "\n",
        "#### Generation Process\n",
        "\n",
        "**Iterative Generation:**\n",
        "1. Start with seed text\n",
        "2. Predict next character probabilities\n",
        "3. Sample next character using temperature\n",
        "4. Append to sequence\n",
        "5. Repeat until desired length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "text_generation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42df5d38-05df-4150-8e02-812991a28e1d"
      },
      "source": [
        "# Text Generation Implementation\n",
        "# This implements the text generation strategies described in the book\n",
        "\n",
        "def preprocess_text(texts, tokenizer, max_id):\n",
        "    \"\"\"\n",
        "    Preprocess text for model input.\n",
        "\n",
        "    Args:\n",
        "        texts: List of text strings\n",
        "        tokenizer: Fitted tokenizer\n",
        "        max_id: Vocabulary size\n",
        "\n",
        "    Returns:\n",
        "        One-hot encoded tensor\n",
        "    \"\"\"\n",
        "    # Convert text to sequences of character IDs\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "    # Convert to one-hot encoding\n",
        "    return tf.one_hot(X, max_id)\n",
        "\n",
        "def next_char(text, model, tokenizer, max_id, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Predict the next character given current text.\n",
        "\n",
        "    Args:\n",
        "        text: Current text string\n",
        "        model: Trained character RNN model\n",
        "        tokenizer: Fitted tokenizer\n",
        "        max_id: Vocabulary size\n",
        "        temperature: Sampling temperature\n",
        "\n",
        "    Returns:\n",
        "        Next character string\n",
        "    \"\"\"\n",
        "    # Preprocess input text\n",
        "    X_new = preprocess_text([text], tokenizer, max_id)\n",
        "\n",
        "    # Get model predictions for the last time step\n",
        "    y_proba = model.predict(X_new, verbose=0)[0, -1:, :]\n",
        "\n",
        "    # Apply temperature scaling to logits\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "\n",
        "    # Sample from the rescaled distribution\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "\n",
        "    # Convert back to character\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
        "\n",
        "def complete_text(text, model, tokenizer, max_id, n_chars=50, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate text by repeatedly predicting next characters.\n",
        "\n",
        "    Args:\n",
        "        text: Seed text\n",
        "        model: Trained model\n",
        "        tokenizer: Fitted tokenizer\n",
        "        max_id: Vocabulary size\n",
        "        n_chars: Number of characters to generate\n",
        "        temperature: Sampling temperature\n",
        "\n",
        "    Returns:\n",
        "        Generated text string\n",
        "    \"\"\"\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, model, tokenizer, max_id, temperature)\n",
        "    return text\n",
        "\n",
        "# Test text generation with different temperatures\n",
        "seed_texts = [\"To be or not to be\", \"Romeo\", \"The king\"]\n",
        "temperatures = [0.2, 1.0, 2.0]\n",
        "\n",
        "print(\"Text Generation Examples:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for seed in seed_texts:\n",
        "    print(f\"\\nSeed text: '{seed}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for temp in temperatures:\n",
        "        generated = complete_text(\n",
        "            seed, char_rnn_model, tokenizer, max_id,\n",
        "            n_chars=100, temperature=temp\n",
        "        )\n",
        "        print(f\"Temperature {temp}: {generated}\")\n",
        "\n",
        "# Analyze temperature effects\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Temperature Analysis:\")\n",
        "print(\"• Low temperature (0.2): Conservative, repetitive, more realistic\")\n",
        "print(\"• Medium temperature (1.0): Balanced creativity and coherence\")\n",
        "print(\"• High temperature (2.0): Creative but potentially nonsensical\")\n",
        "\n",
        "# Character frequency analysis in generated text\n",
        "test_generation = complete_text(\n",
        "    \"Shakespeare \", char_rnn_model, tokenizer, max_id,\n",
        "    n_chars=500, temperature=1.0\n",
        ")\n",
        "\n",
        "print(f\"\\nGenerated text sample (500 chars):\")\n",
        "print(test_generation)\n",
        "\n",
        "# Analyze character frequencies\n",
        "generated_freq = Counter(test_generation)\n",
        "original_freq = Counter(shakespeare_text[:1000])  # Compare with original\n",
        "\n",
        "print(f\"\\nCharacter frequency comparison (top 10):\")\n",
        "print(\"Generated | Original\")\n",
        "for (char_g, freq_g), (char_o, freq_o) in zip(generated_freq.most_common(10), original_freq.most_common(10)):\n",
        "    print(f\"'{char_g}': {freq_g:3d}   | '{char_o}': {freq_o:3d}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Generation Examples:\n",
            "================================================================================\n",
            "\n",
            "Seed text: 'To be or not to be'\n",
            "----------------------------------------\n",
            "Temperature 0.2: To be or not to be so me,\n",
            "and the banishments and the banishments and make his countryment.\n",
            "\n",
            "henry bolingbroke:\n",
            "the lo\n",
            "Temperature 1.0: To be or not to be with my face to majesty.\n",
            "be all heavy ruy leased thrown, men the death, say our tongue by their ten\n",
            "Temperature 2.0: To be or not to beas's so;,\n",
            "or weper in pasts,-pvarpidded compani:ssubject;\n",
            "amest, capatefon out thor griev: negrns wr\n",
            "\n",
            "Seed text: 'Romeo'\n",
            "----------------------------------------\n",
            "Temperature 0.2: Romeon the earth\n",
            "and me we will so me we should so me with his countryments\n",
            "and the blood of the earth of\n",
            "Temperature 1.0: Romeople will grief,\n",
            "for i come, green's grace but thy humberlens\n",
            "streagh of contlemans friends allay\n",
            "you\n",
            "Temperature 2.0: Romeopted\n",
            "boyiof to-lapb'd, hanvey: dy tell!-\n",
            "dury know, fleeriuss, great's naciousbbvcud the lett g'uss?\n",
            "\n",
            "Seed text: 'The king'\n",
            "----------------------------------------\n",
            "Temperature 0.2: The king richard ii:\n",
            "the bloody have be so my father with some for the heads.\n",
            "\n",
            "duke of york:\n",
            "i was be heaven\n",
            "Temperature 1.0: The king richard is fight\n",
            "gooding see and god's name, from the lord,\n",
            "and we that see, by the royal known:\n",
            "th\n",
            "Temperature 2.0: The king\n",
            "cousbakent fromsoufswelstedion'd mying grusiax how, he wyepeleh\n",
            "cr: brith ther bushy. holp'd apprig\n",
            "\n",
            "================================================================================\n",
            "Temperature Analysis:\n",
            "• Low temperature (0.2): Conservative, repetitive, more realistic\n",
            "• Medium temperature (1.0): Balanced creativity and coherence\n",
            "• High temperature (2.0): Creative but potentially nonsensical\n",
            "\n",
            "Generated text sample (500 chars):\n",
            "Shakespeare aspmen!\n",
            "gentleman!\n",
            "the offer friends make alainst a kingdom:\n",
            "i wall-becuil and pale-crow\n",
            "into stilt like discwanencays crosposed:\n",
            "say too falls, to the earth of his body;\n",
            "where;' contemn to my lord:\n",
            "ay, is he did gold,\n",
            "sight, on the winderitaked to forth clood: hope it.\n",
            "say make my lord of soverence to you in the king;\n",
            "some take men have free in heaven to flatter'd\n",
            "the found our this strobing of the home\n",
            "where little to you, felf, and your uncle:\n",
            "thruiss, may yee an is my king,\n",
            "and heaven to the\n",
            "\n",
            "Character frequency comparison (top 10):\n",
            "Generated | Original\n",
            "' ':  79   | ' ': 145\n",
            "'e':  52   | 'e': 106\n",
            "'o':  37   | 't':  69\n",
            "'t':  34   | 'i':  63\n",
            "'a':  29   | 'r':  53\n",
            "'n':  28   | 'o':  53\n",
            "'i':  25   | 's':  47\n",
            "'h':  22   | 'a':  45\n",
            "'s':  22   | 'n':  43\n",
            "'r':  19   | '\n",
            "':  40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stateful_rnn_theory"
      },
      "source": [
        "## Stateful RNNs: Theory and Implementation\n",
        "\n",
        "### Problem with Stateless RNNs\n",
        "\n",
        "**Stateless RNN Limitation:**\n",
        "- Hidden state resets to zero at each batch\n",
        "- Cannot learn patterns longer than sequence length\n",
        "- Information from previous batches is lost\n",
        "\n",
        "### Stateful RNN Theory\n",
        "\n",
        "**Key Concept:** Preserve hidden state between batches\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "For batch $b$ at time $t$:\n",
        "$$h_t^{(b)} = f(x_t^{(b)}, h_{T}^{(b-1)})$$\n",
        "\n",
        "Where:\n",
        "- $h_T^{(b-1)}$: final hidden state from previous batch\n",
        "- $h_0^{(0)} = \\mathbf{0}$: initial state is zero\n",
        "\n",
        "### Requirements for Stateful RNNs\n",
        "\n",
        "**1. Sequential Data Ordering:**\n",
        "Each sequence in batch $b$ must continue where corresponding sequence in batch $b-1$ ended.\n",
        "\n",
        "**2. No Shuffling:**\n",
        "Data order must be preserved chronologically.\n",
        "\n",
        "**3. Fixed Batch Size:**\n",
        "Model needs to know batch size for state management.\n",
        "\n",
        "**4. Manual State Reset:**\n",
        "Must reset states at logical boundaries (e.g., end of epoch).\n",
        "\n",
        "### Batching Strategy for Stateful RNNs\n",
        "\n",
        "**Problem:** Standard batching doesn't preserve continuity\n",
        "\n",
        "**Solution 1: Single Sequence per Batch**\n",
        "- Batch size = 1\n",
        "- Simple but inefficient\n",
        "\n",
        "**Solution 2: Parallel Sequences**\n",
        "- Split text into $N$ parallel streams\n",
        "- Each batch contains one segment from each stream\n",
        "- Requires careful data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stateful_rnn_implementation",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a03e31b8-80cc-42df-d9f9-5cf0043bf387"
      },
      "source": [
        "# Stateful RNN Implementation\n",
        "# This demonstrates the stateful RNN approach from the book\n",
        "\n",
        "\n",
        "print(\"Implementing Stateful RNN (limited epochs for demonstration)...\")\n",
        "\n",
        "def create_stateful_dataset(encoded_text, n_steps, batch_size=1):\n",
        "    \"\"\"\n",
        "    Create dataset for stateful RNN training.\n",
        "\n",
        "    Args:\n",
        "        encoded_text: Encoded text sequence\n",
        "        n_steps: Sequence length\n",
        "        batch_size: Batch size (must be fixed for stateful RNN)\n",
        "\n",
        "    Returns:\n",
        "        TensorFlow dataset\n",
        "    \"\"\"\n",
        "    # Create dataset from encoded text\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "\n",
        "    # Create non-overlapping windows (shift = n_steps, not 1)\n",
        "    window_length = n_steps + 1\n",
        "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "\n",
        "    # Flatten nested dataset\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "    # Batch the windows (batch_size sequences per batch)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    # Split into input and target\n",
        "    dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "\n",
        "    # Convert to one-hot encoding\n",
        "    dataset = dataset.map(\n",
        "        lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
        "    )\n",
        "\n",
        "    # Prefetch for performance\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def create_stateful_char_rnn(vocab_size, rnn_units=128, batch_size=1):\n",
        "    \"\"\"\n",
        "    Create a stateful character RNN model.\n",
        "\n",
        "    Args:\n",
        "        vocab_size: Size of character vocabulary\n",
        "        rnn_units: Number of RNN units\n",
        "        batch_size: Fixed batch size for stateful RNN\n",
        "\n",
        "    Returns:\n",
        "        Compiled stateful model\n",
        "    \"\"\"\n",
        "    model = keras.models.Sequential([\n",
        "        # First GRU layer with stateful=True\n",
        "        keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            stateful=True,  # KEY: Enable stateful mode\n",
        "            dropout=0.2,\n",
        "            recurrent_dropout=0.2,\n",
        "            # CRITICAL: Must specify batch_input_shape for stateful RNN\n",
        "            batch_input_shape=[batch_size, None, vocab_size]\n",
        "        ),\n",
        "\n",
        "        # Second GRU layer\n",
        "        keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            stateful=True,  # Also stateful\n",
        "            dropout=0.2,\n",
        "            recurrent_dropout=0.2\n",
        "        ),\n",
        "\n",
        "        # Output layer\n",
        "        keras.layers.TimeDistributed(\n",
        "            keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Custom callback to reset states at epoch boundaries\n",
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Callback to reset RNN states at the beginning of each epoch.\n",
        "    This is crucial for stateful RNNs to prevent information leakage\n",
        "    across epoch boundaries.\n",
        "    \"\"\"\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        print(f\"\\nEpoch {epoch + 1}: Resetting RNN states\")\n",
        "        self.model.reset_states()\n",
        "\n",
        "# Create stateful dataset and model\n",
        "print(\"Creating Stateful RNN Dataset and Model...\")\n",
        "\n",
        "# Use smaller batch size for stateful RNN demonstration\n",
        "stateful_batch_size = 1\n",
        "stateful_n_steps = 100\n",
        "\n",
        "# Create stateful dataset\n",
        "stateful_train = create_stateful_dataset(\n",
        "    train_encoded,\n",
        "    stateful_n_steps,\n",
        "    stateful_batch_size\n",
        ")\n",
        "\n",
        "# Create stateful model\n",
        "stateful_model = create_stateful_char_rnn(\n",
        "    vocab_size=max_id,\n",
        "    rnn_units=128,\n",
        "    batch_size=stateful_batch_size\n",
        ")\n",
        "\n",
        "print(\"\\nStateful RNN Model Architecture:\")\n",
        "stateful_model.summary()\n",
        "\n",
        "# Compile the stateful model\n",
        "stateful_model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Analyze dataset structure\n",
        "print(f\"\\nStateful Dataset Analysis:\")\n",
        "dataset_size = len(list(stateful_train.take(-1)))\n",
        "print(f\"Total batches: {dataset_size}\")\n",
        "print(f\"Batch size: {stateful_batch_size}\")\n",
        "print(f\"Sequence length: {stateful_n_steps}\")\n",
        "\n",
        "# Examine a sample batch\n",
        "for X_batch, Y_batch in stateful_train.take(1):\n",
        "    print(f\"\\nBatch shapes:\")\n",
        "    print(f\"Input: {X_batch.shape}\")\n",
        "    print(f\"Target: {Y_batch.shape}\")\n",
        "\n",
        "# Key differences from stateless RNN\n",
        "print(\"\\nKey Differences from Stateless RNN:\")\n",
        "print(\"1. stateful=True in GRU layers\")\n",
        "print(\"2. batch_input_shape specified (required for stateful)\")\n",
        "print(\"3. Non-overlapping windows (shift=n_steps)\")\n",
        "print(\"4. No shuffling of data\")\n",
        "print(\"5. Manual state reset via callback\")\n",
        "print(\"6. Fixed batch size during training and inference\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Implementing Stateful RNN (limited epochs for demonstration)...\n",
            "Creating Stateful RNN Dataset and Model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized keyword arguments passed to GRU: {'batch_input_shape': [1, None, 39]}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-3915859507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# Create stateful model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m stateful_model = create_stateful_char_rnn(\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mrnn_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-15-3915859507.py\u001b[0m in \u001b[0;36mcreate_stateful_char_rnn\u001b[0;34m(vocab_size, rnn_units, batch_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m     model = keras.models.Sequential([\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# First GRU layer with stateful=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         keras.layers.GRU(\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mrnn_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/gru.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, seed, return_sequences, return_state, go_backwards, stateful, unroll, reset_after, use_cudnn, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mimplementation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"implementation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         )\n\u001b[0;32m--> 517\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cell, return_sequences, return_state, go_backwards, stateful, unroll, zero_output_for_mask, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;34mf\"Received: cell={cell}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# If True, the output for masked timestep will be zeros, whereas in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_shape_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    288\u001b[0m                 \u001b[0;34m\"Unrecognized keyword arguments \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;34mf\"passed to {self.__class__.__name__}: {kwargs}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to GRU: {'batch_input_shape': [1, None, 39]}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stateful_training",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "0ff5847a-2d22-4cdd-b1f6-1ca894f29f1f"
      },
      "source": [
        "# Train the Stateful RNN\n",
        "# Note: This demonstrates the training process, but is computationally intensive\n",
        "\n",
        "print(\"Training Stateful RNN (limited epochs for demonstration)...\")\n",
        "\n",
        "# Create the reset states callback\n",
        "reset_callback = ResetStatesCallback()\n",
        "\n",
        "# Train for fewer epochs due to computational constraints\n",
        "stateful_epochs = 2\n",
        "\n",
        "# Train the stateful model\n",
        "stateful_history = stateful_model.fit(\n",
        "    stateful_train,\n",
        "    epochs=stateful_epochs,\n",
        "    callbacks=[reset_callback],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Compare performance metrics\n",
        "print(f\"\\nStateful RNN Training Results:\")\n",
        "print(f\"Final Loss: {stateful_history.history['loss'][-1]:.4f}\")\n",
        "print(f\"Final Accuracy: {stateful_history.history['accuracy'][-1]:.4f}\")\n",
        "\n",
        "# Create a stateless version for inference\n",
        "# This is necessary because stateful models can only predict with the same batch size\n",
        "print(\"\\nCreating stateless version for text generation...\")\n",
        "\n",
        "stateless_model = create_char_rnn_model(\n",
        "    vocab_size=max_id,\n",
        "    rnn_units=128,\n",
        "    dropout_rate=0.0  # No dropout for inference\n",
        ")\n",
        "\n",
        "# Copy weights from stateful to stateless model\n",
        "stateless_model.set_weights(stateful_model.get_weights())\n",
        "\n",
        "print(\"Weights copied from stateful to stateless model.\")\n",
        "\n",
        "# Test text generation with stateless model\n",
        "print(\"\\nGenerating text with stateless model (copied weights):\")\n",
        "test_generation = complete_text(\n",
        "    \"The stateful \", stateless_model, tokenizer, max_id,\n",
        "    n_chars=200, temperature=1.0\n",
        ")\n",
        "print(test_generation)\n",
        "\n",
        "# Advantages and disadvantages summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STATEFUL vs STATELESS RNN COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nStateful RNN Advantages:\")\n",
        "print(\"• Can learn longer-term dependencies (beyond window size)\")\n",
        "print(\"• More coherent text generation\")\n",
        "print(\"• Better context preservation across sequences\")\n",
        "\n",
        "print(\"\\nStateful RNN Disadvantages:\")\n",
        "print(\"• More complex data preparation\")\n",
        "print(\"• Fixed batch size requirement\")\n",
        "print(\"• Manual state management needed\")\n",
        "print(\"• Cannot parallelize as effectively\")\n",
        "print(\"• Need separate model for inference\")\n",
        "\n",
        "print(\"\\nWhen to use Stateful RNNs:\")\n",
        "print(\"• When learning very long sequences (>1000 time steps)\")\n",
        "print(\"• When context preservation is crucial\")\n",
        "print(\"• For online learning scenarios\")\n",
        "print(\"• When computational resources allow for complexity\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Stateful RNN (limited epochs for demonstration)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stateful_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-16-1023449000.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Train the stateful model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m stateful_history = stateful_model.fit(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mstateful_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstateful_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stateful_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sentiment_analysis_theory"
      },
      "source": [
        "## Part 2: Sentiment Analysis with Word-Level RNNs\n",
        "\n",
        "### Theory: From Characters to Words\n",
        "\n",
        "**Word-Level vs Character-Level:**\n",
        "\n",
        "| Aspect | Character-Level | Word-Level |\n",
        "|--------|----------------|------------|\n",
        "| **Vocabulary Size** | ~100 | ~10,000-50,000 |\n",
        "| **Sequence Length** | Long (1000+ chars) | Short (10-100 words) |\n",
        "| **Semantic Understanding** | Limited | Rich |\n",
        "| **OOV Handling** | Natural | Problematic |\n",
        "| **Training Speed** | Slower | Faster |\n",
        "\n",
        "### IMDb Dataset Theory\n",
        "\n",
        "**Dataset Structure:**\n",
        "- **50,000 movie reviews** (25K train, 25K test)\n",
        "- **Binary sentiment**: 0 (negative), 1 (positive)\n",
        "- **Preprocessing**: Lowercase, punctuation removed, frequency-based indexing\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "Given review $r = [w_1, w_2, ..., w_T]$ with words $w_i$, predict:\n",
        "$$P(\\text{sentiment} = 1 | r) = \\sigma(f(r))$$\n",
        "\n",
        "Where $f(r)$ is the RNN output and $\\sigma$ is the sigmoid function.\n",
        "\n",
        "### Subword Tokenization Theory\n",
        "\n",
        "**Problems with Word-Level Tokenization:**\n",
        "1. **Large vocabularies**: Memory and computational overhead\n",
        "2. **OOV words**: Cannot handle unseen words\n",
        "3. **Morphological variants**: \"run\", \"running\", \"ran\" treated as different\n",
        "4. **Language dependence**: Spaces don't exist in all languages\n",
        "\n",
        "**Subword Solutions:**\n",
        "\n",
        "**1. Byte Pair Encoding (BPE):**\n",
        "Iteratively merge most frequent character pairs:\n",
        "```\n",
        "Initial: [\"h\", \"e\", \"l\", \"l\", \"o\"]\n",
        "Step 1: [\"he\", \"l\", \"l\", \"o\"]  # \"h\"+\"e\" most frequent\n",
        "Step 2: [\"he\", \"ll\", \"o\"]     # \"l\"+\"l\" most frequent\n",
        "Step 3: [\"hello\"]             # \"he\"+\"ll\"+\"o\" merged\n",
        "```\n",
        "\n",
        "**2. SentencePiece:**\n",
        "- Language-independent\n",
        "- Treats spaces as regular characters\n",
        "- Unigram language model approach\n",
        "\n",
        "**3. WordPiece:**\n",
        "- Used in BERT\n",
        "- Maximizes likelihood of training data\n",
        "- Greedy longest-match-first algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imdb_data_loading",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95784eed-efde-45c3-b232-7c40f2a04d31"
      },
      "source": [
        "# Load and Explore IMDb Dataset\n",
        "# This demonstrates the \"hello world\" of NLP sentiment analysis\n",
        "\n",
        "print(\"Loading IMDb Movie Reviews Dataset...\")\n",
        "\n",
        "# Load the preprocessed IMDb dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
        "\n",
        "print(f\"Dataset Statistics:\")\n",
        "print(f\"Training samples: {len(X_train):,}\")\n",
        "print(f\"Test samples: {len(X_test):,}\")\n",
        "print(f\"Training labels distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test labels distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "# Analyze sequence lengths\n",
        "train_lengths = [len(review) for review in X_train]\n",
        "test_lengths = [len(review) for review in X_test]\n",
        "\n",
        "print(f\"\\nSequence Length Statistics:\")\n",
        "print(f\"Training - Mean: {np.mean(train_lengths):.1f}, Std: {np.std(train_lengths):.1f}\")\n",
        "print(f\"Training - Min: {np.min(train_lengths)}, Max: {np.max(train_lengths)}\")\n",
        "print(f\"Training - Median: {np.median(train_lengths):.1f}\")\n",
        "\n",
        "# Examine first few examples\n",
        "print(f\"\\nFirst 3 training examples (encoded):\")\n",
        "for i in range(3):\n",
        "    print(f\"Review {i+1} (length {len(X_train[i])}, label {y_train[i]}): {X_train[i][:20]}...\")\n",
        "\n",
        "# Load word index for decoding\n",
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "print(f\"\\nVocabulary size: {len(word_index):,} words\")\n",
        "\n",
        "# Create reverse word index for decoding\n",
        "# Note: indices are offset by 3 (0=padding, 1=start, 2=unknown)\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate([\"<PAD>\", \"<START>\", \"<UNK>\"]):\n",
        "    id_to_word[id_] = token\n",
        "\n",
        "# Decode a sample review\n",
        "def decode_review(encoded_review, id_to_word):\n",
        "    \"\"\"Decode an encoded review back to text.\"\"\"\n",
        "    return \" \".join([id_to_word.get(id_, \"<UNK>\") for id_ in encoded_review])\n",
        "\n",
        "# Show decoded examples\n",
        "print(f\"\\nDecoded Examples:\")\n",
        "for i in range(2):\n",
        "    decoded = decode_review(X_train[i], id_to_word)\n",
        "    label = \"Positive\" if y_train[i] == 1 else \"Negative\"\n",
        "    print(f\"\\nReview {i+1} ({label}):\")\n",
        "    print(decoded[:300] + \"...\")\n",
        "\n",
        "# Visualize sequence length distribution\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(train_lengths, bins=50, alpha=0.7, label='Training')\n",
        "plt.hist(test_lengths, bins=50, alpha=0.7, label='Test')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Review Lengths')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(train_lengths, bins=50, alpha=0.7, cumulative=True, density=True)\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Cumulative Probability')\n",
        "plt.title('Cumulative Distribution of Review Lengths')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axhline(y=0.95, color='r', linestyle='--', label='95th percentile')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Determine good sequence length for padding\n",
        "percentile_95 = np.percentile(train_lengths, 95)\n",
        "percentile_99 = np.percentile(train_lengths, 99)\n",
        "print(f\"\\n95th percentile length: {percentile_95:.0f}\")\n",
        "print(f\"99th percentile length: {percentile_99:.0f}\")\n",
        "print(f\"Recommended max length: {min(500, int(percentile_95))}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDb Movie Reviews Dataset...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Dataset Statistics:\n",
            "Training samples: 25,000\n",
            "Test samples: 25,000\n",
            "Training labels distribution: [12500 12500]\n",
            "Test labels distribution: [12500 12500]\n",
            "\n",
            "Sequence Length Statistics:\n",
            "Training - Mean: 238.7, Std: 176.5\n",
            "Training - Min: 11, Max: 2494\n",
            "Training - Median: 178.0\n",
            "\n",
            "First 3 training examples (encoded):\n",
            "Review 1 (length 218, label 1): [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25]...\n",
            "Review 2 (length 189, label 0): [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14]...\n",
            "Review 3 (length 141, label 0): [1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14]...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\n",
            "Vocabulary size: 88,584 words\n",
            "\n",
            "Decoded Examples:\n",
            "\n",
            "Review 1 (Positive):\n",
            "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i lo...\n",
            "\n",
            "Review 2 (Negative):\n",
            "<START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is co...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "95th percentile length: 610\n",
            "99th percentile length: 926\n",
            "Recommended max length: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing_theory"
      },
      "source": [
        "### Advanced Text Preprocessing Theory\n",
        "\n",
        "#### Variable Length Sequences\n",
        "\n",
        "**Challenge:** Neural networks require fixed-size inputs, but text has variable length.\n",
        "\n",
        "**Solutions:**\n",
        "\n",
        "**1. Padding:**\n",
        "$$\\text{padded_sequence} = \\begin{cases}\n",
        "[w_1, w_2, ..., w_T, 0, 0, ..., 0] & \\text{if } T < \\text{max_len} \\\\\n",
        "[w_1, w_2, ..., w_{\\text{max_len}}] & \\text{if } T \\geq \\text{max_len}\n",
        "\\end{cases}$$\n",
        "\n",
        "**2. Truncation:**\n",
        "- **Pre-truncation**: Keep last N tokens\n",
        "- **Post-truncation**: Keep first N tokens\n",
        "\n",
        "**3. Bucketing:**\n",
        "Group sequences by similar lengths to minimize padding.\n",
        "\n",
        "#### Masking Theory\n",
        "\n",
        "**Problem:** Model should ignore padding tokens\n",
        "\n",
        "**Masking Mechanism:**\n",
        "$$\\text{mask}[i] = \\begin{cases}\n",
        "\\text{True} & \\text{if } x[i] \\neq 0 \\\\\n",
        "\\text{False} & \\text{if } x[i] = 0\n",
        "\\end{cases}$$\n",
        "\n",
        "**Implementation in Layers:**\n",
        "- **Embedding Layer**: `mask_zero=True` creates mask automatically\n",
        "- **RNN Layers**: Use mask to skip computations for masked time steps\n",
        "- **Loss Function**: Masked tokens don't contribute to loss\n",
        "\n",
        "#### TensorFlow Text Processing\n",
        "\n",
        "**TensorFlow Operations for Text:**\n",
        "- `tf.strings.substr()`: Extract substrings\n",
        "- `tf.strings.regex_replace()`: Regular expression replacement\n",
        "- `tf.strings.split()`: Split strings into tokens\n",
        "- `tf.RaggedTensor`: Handle variable-length sequences\n",
        "- `tf.lookup.StaticVocabularyTable`: Efficient word-to-id mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advanced_preprocessing",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6e9f2ca74c5f4224aeecba007625f625",
            "05ba3d932c9440b389700061f0b7da00",
            "cd4bd956a5164d36b91558256668c8cd",
            "52427f6257d74e30af014e1a1c086878",
            "1d00ff05e8ac4b83874907ea29db58bb",
            "e2c553fba00046baa84f127fd655a283",
            "e38352ee89264b4ea46cd80ddd3a6db8",
            "f8f90453df5e41e8806d603eef44e3d3",
            "80d3ed0882a6423aa653328c07452ea4",
            "9dea491e8cb9431bb60bef30bea75d30",
            "a69af730e347402388f03d5c2b94c12f",
            "725e5a53528b4efc90a26f79e72cb1bd",
            "88b0e3f2a6874f09be85b1c34683fc2e",
            "07d6c3bc9ded4aef9174620796e9aded",
            "d5bf94943f34453baaed77b2ddfb29ea",
            "e07da3c5ce0d488186d08381db699f7f",
            "f302e1045a3b47e0ac2dd5b0a1a80281",
            "566b40bb90004f0cb909a00fb5cf2a2e",
            "9c4563235814403fa4abf16e3ed76f5b",
            "bbb94a27283047afa07f42f33085f490",
            "4a58342f14ef40d9a6dfbb723d86d032",
            "3e8b9e23ad20464691f90a7fe335009c",
            "efa94596947f4be8ab9f79841898eeba",
            "c8660431e7e84bdead7c1bf89050ab26",
            "04db5f5e0e4e4e11bbbeed2172ebda37",
            "b6bbccf52cef44eab1879cd0fbb06f15",
            "67ae78dc46b741de86b86f95c070e50f",
            "406b38a7061b44828b5a446fee9d31e1",
            "27273e5bbc164b439e9fc8a1853730a3",
            "474dc75c1f3f415abdec747f8dd2e533",
            "3ee9f50934c64f3294c99b0de677bc32",
            "91b5e942564949758a2352fec6add204",
            "96d3094ab42d4ce7980ffc6e128eea28",
            "9e96b0fc089e408e86c5266ba30ccbda",
            "1bb610ae549e4f58ab6923f63fda4bbf",
            "ca84f8ecc9714b9a82a12e08872e31a7",
            "ba6fd51bf85b407099735a1270cbe684",
            "25e859cfd510405581ea9248bebc581b",
            "d701deef93694d1d94adc6e7f162a113",
            "539abb1ce25b49a58cb281b8bbf0869b",
            "97334d5cfccc4dbfbf8e4962e86923ae",
            "0000f5a7e7694395a2d0f16b01caab38",
            "f8fd303e209441848553a288a930b10c",
            "5e44b0dff8d34ab6a68d84f7e56ad16e",
            "3d9a9660c0bb458185f96557af302079",
            "7a6c7cd3b38a40318d883debb84d13f7",
            "552ee3cd46944df793046ce94432f579",
            "fc64b0ae81634e3295680881765c54c6",
            "1fbb3dc808af476188f22a08e2c2fa0b",
            "82a3675e242c49e3b9ab322549409029",
            "5667654101f24955aa7fd8fd7c5a4fb1",
            "928c69f766dc4bd393d9e4badef5587e",
            "2223d20e62e2438281b1ca4c28050356",
            "bc2e0adef18f43419a5fa1e90ffe26d3",
            "70686d5c86f040d4bc7c034c0a349d01",
            "030983d88da6453db2a50b2fc80e4fa1",
            "8f1069da70c64685b8d3461f7a3d3e77",
            "804de0fefd3e41f1a260006838566ec4",
            "0310e7ee77024552b66056a1d0d7b8d4",
            "1572033545334b66a8732da0d46747fd",
            "a719295e21d941ccb518c7378b8765c7",
            "935b7e4fad144e778bbbc3c3beb629f3",
            "fb69afa559a84017a6b751be83fe988d",
            "b8baf6b23c3d4422b2a76b2857489c3f",
            "075d95ccc9204469ba6bde74f8ae7a38",
            "b567f1fe498f4e789f5290d1aba95e61",
            "f4f41922fa574a0e8519848fec3e01c3",
            "c308d6a4eded470fa4612ff470f92bdc",
            "9fdf50dee3d84b14bfead787a072d756",
            "95af438272224dc5a93910ff00084b54",
            "9119210e328a41e8bcc6279a8c289c4a",
            "1ff50c7326d3483594ffbd4daaa62feb",
            "70c6bd54b4d14fecbb9eb884a1156184",
            "a0f315ae2b594de3a419f589f034a8fc",
            "b91ad7e573034ad79d163b6f8d67a210",
            "407543d141754e3494c187bddaf291c1",
            "5c0c251692dc4456bd43fa7d3ce2b20e",
            "9856f099bf5a43bf86c1c8688521d9c9",
            "a9bd5b462a9744c9acb98ea10e462bb1",
            "ffbac5872e9d4d22b14722702fbee62c",
            "7878fdacf4154eb18d8c245af8feb89d",
            "ee3ec672de9949ddb2954c1dba8633bd",
            "8be1d7fd32f64efc8f2b0ab298c79ee0",
            "23a5cec3aaa84471858074a196bca5a3",
            "2ed8ae2240354ce79066fdf6f7a54875",
            "0ecac5ce38cd409b863d954e07fed3d9",
            "2b7f9b217bef41dc8e9cd7b8890e396b",
            "f1f7a426bbb74d89a7b7f19848fa273f",
            "6069e3e1d2fa44fd97d2c2c28dc538a0",
            "80d54edbecbe41efa6b3688b3e0c53e4",
            "a6246d8236314ee38685d0ccbd761339",
            "1d2fd5ed56a848cf920be7cfd76b2eb8",
            "e0957c2278454296bdeb9f42df1d9537",
            "e1dc58e797cf4ad1bb06455f6d3a337a",
            "d5b8778d4b9548dfab3bde5985d8c73c",
            "68ae78f1b04f4142a54612af33556f2b",
            "eb61da1ba08e406182e734dcd8bb1e01",
            "cf8d4865a33b4fd78679b8f640633e1c",
            "da7bd6465d9c428da85d810d1658a99d"
          ]
        },
        "outputId": "1b50c950-9f71-43a1-a3da-3d01980f5f1e"
      },
      "source": [
        "# Advanced Text Preprocessing with TensorFlow Operations\n",
        "# This demonstrates modern preprocessing techniques from the book\n",
        "\n",
        "# Load raw text data using TensorFlow Datasets\n",
        "print(\"Loading raw IMDb data with TensorFlow Datasets...\")\n",
        "\n",
        "# Load original text data (not preprocessed integers)\n",
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "train_size = info.splits[\"train\"].num_examples\n",
        "\n",
        "print(f\"Dataset info:\")\n",
        "print(f\"Training examples: {train_size:,}\")\n",
        "print(f\"Features: {info.features}\")\n",
        "\n",
        "# Examine raw data\n",
        "print(\"\\nRaw data examples:\")\n",
        "for i, (text, label) in enumerate(datasets[\"train\"].take(2)):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Label: {label.numpy()} ({'Positive' if label.numpy() == 1 else 'Negative'})\")\n",
        "    print(f\"Text (first 200 chars): {text.numpy().decode('utf-8')[:200]}...\")\n",
        "\n",
        "def preprocess_text(X_batch, y_batch):\n",
        "    \"\"\"\n",
        "    Advanced text preprocessing using TensorFlow operations.\n",
        "    This demonstrates the preprocessing pipeline from the book.\n",
        "\n",
        "    Args:\n",
        "        X_batch: Batch of text strings\n",
        "        y_batch: Batch of labels\n",
        "\n",
        "    Returns:\n",
        "        Preprocessed text and labels\n",
        "    \"\"\"\n",
        "    # 1. Truncate to first 300 characters (speeds up training)\n",
        "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
        "\n",
        "    # 2. Replace HTML line breaks with spaces\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \")\n",
        "\n",
        "    # 3. Remove non-alphabetic characters (keep only letters and apostrophes)\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "\n",
        "    # 4. Split into words (creates ragged tensor)\n",
        "    X_batch = tf.strings.split(X_batch)\n",
        "\n",
        "    # 5. Convert ragged tensor to dense tensor with padding\n",
        "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\n",
        "\n",
        "# Apply preprocessing to a sample\n",
        "print(\"\\nTesting preprocessing pipeline:\")\n",
        "sample_texts = tf.constant([\n",
        "    b\"This movie was great! I loved it.<br />Highly recommended.\",\n",
        "    b\"Terrible film... Don't waste your time!!! 0/10\"\n",
        "])\n",
        "sample_labels = tf.constant([1, 0])\n",
        "\n",
        "processed_texts, processed_labels = preprocess_text(sample_texts, sample_labels)\n",
        "print(f\"Original: {sample_texts[0].numpy().decode('utf-8')}\")\n",
        "print(f\"Processed: {[word.numpy().decode('utf-8') for word in processed_texts[0] if word.numpy() != b'<pad>']}\")\n",
        "\n",
        "# Build vocabulary from training data\n",
        "print(\"\\nBuilding vocabulary from training data...\")\n",
        "\n",
        "vocabulary = Counter()\n",
        "batch_count = 0\n",
        "max_batches = 100  # Process subset for demonstration\n",
        "\n",
        "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess_text).take(max_batches):\n",
        "    for review in X_batch:\n",
        "        # Extract words from each review\n",
        "        words = [word.numpy() for word in review if word.numpy() != b\"<pad>\"]\n",
        "        vocabulary.update(words)\n",
        "    batch_count += 1\n",
        "    if batch_count % 20 == 0:\n",
        "        print(f\"Processed {batch_count} batches...\")\n",
        "\n",
        "print(f\"\\nVocabulary statistics:\")\n",
        "print(f\"Total unique words: {len(vocabulary):,}\")\n",
        "print(f\"Most common words: {vocabulary.most_common(10)}\")\n",
        "\n",
        "# Create truncated vocabulary\n",
        "vocab_size = 10000\n",
        "truncated_vocabulary = [\n",
        "    word for word, count in vocabulary.most_common(vocab_size)\n",
        "]\n",
        "\n",
        "print(f\"Truncated vocabulary size: {len(truncated_vocabulary)}\")\n",
        "\n",
        "# Create lookup table for word-to-ID mapping\n",
        "words = tf.constant(truncated_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "num_oov_buckets = 1000  # Out-of-vocabulary buckets\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
        "\n",
        "# Test lookup table\n",
        "test_words = tf.constant([b\"this\", b\"movie\", b\"was\", b\"fantaaaastic\"])\n",
        "test_ids = table.lookup(test_words)\n",
        "print(f\"\\nLookup table test:\")\n",
        "for word, word_id in zip(test_words.numpy(), test_ids.numpy()):\n",
        "    in_vocab = \"(in vocab)\" if word_id < vocab_size else \"(OOV)\"\n",
        "    print(f\"'{word.decode('utf-8')}' -> {word_id} {in_vocab}\")\n",
        "\n",
        "# Create final preprocessing function\n",
        "def encode_words(X_batch, y_batch):\n",
        "    \"\"\"Convert words to integer IDs using lookup table.\"\"\"\n",
        "    return table.lookup(X_batch), y_batch\n",
        "\n",
        "# Create final training dataset\n",
        "print(\"\\nCreating final training dataset...\")\n",
        "train_set = datasets[\"train\"].batch(32).map(preprocess_text)\n",
        "train_set = train_set.map(encode_words).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Examine final dataset\n",
        "for X_batch, y_batch in train_set.take(1):\n",
        "    print(f\"\\nFinal dataset batch shapes:\")\n",
        "    print(f\"X_batch: {X_batch.shape}\")\n",
        "    print(f\"y_batch: {y_batch.shape}\")\n",
        "    print(f\"Sample encoded review: {X_batch[0][:20].numpy()}\")\n",
        "    print(f\"Sample label: {y_batch[0].numpy()}\")\n",
        "\n",
        "# Vocabulary analysis\n",
        "print(f\"\\nFinal vocabulary configuration:\")\n",
        "print(f\"In-vocabulary words: {vocab_size:,}\")\n",
        "print(f\"Out-of-vocabulary buckets: {num_oov_buckets:,}\")\n",
        "print(f\"Total vocabulary size: {vocab_size + num_oov_buckets:,}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading raw IMDb data with TensorFlow Datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Variant folder /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0 has no dataset_info.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e9f2ca74c5f4224aeecba007625f625"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "725e5a53528b4efc90a26f79e72cb1bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efa94596947f4be8ab9f79841898eeba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e96b0fc089e408e86c5266ba30ccbda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.22Z47M_1.0.0/imdb_reviews-train.tfrecor…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d9a9660c0bb458185f96557af302079"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "030983d88da6453db2a50b2fc80e4fa1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.22Z47M_1.0.0/imdb_reviews-test.tfrecord…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4f41922fa574a0e8519848fec3e01c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9856f099bf5a43bf86c1c8688521d9c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.22Z47M_1.0.0/imdb_reviews-unsupervised.…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6069e3e1d2fa44fd97d2c2c28dc538a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n",
            "Dataset info:\n",
            "Training examples: 25,000\n",
            "Features: FeaturesDict({\n",
            "    'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
            "    'text': Text(shape=(), dtype=string),\n",
            "})\n",
            "\n",
            "Raw data examples:\n",
            "\n",
            "Example 1:\n",
            "Label: 0 (Negative)\n",
            "Text (first 200 chars): This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting ...\n",
            "\n",
            "Example 2:\n",
            "Label: 0 (Negative)\n",
            "Text (first 200 chars): I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However ...\n",
            "\n",
            "Testing preprocessing pipeline:\n",
            "Original: This movie was great! I loved it.<br />Highly recommended.\n",
            "Processed: ['This', 'movie', 'was', 'great', 'I', 'loved', 'it', 'Highly', 'recommended']\n",
            "\n",
            "Building vocabulary from training data...\n",
            "Processed 20 batches...\n",
            "Processed 40 batches...\n",
            "Processed 60 batches...\n",
            "Processed 80 batches...\n",
            "Processed 100 batches...\n",
            "\n",
            "Vocabulary statistics:\n",
            "Total unique words: 18,133\n",
            "Most common words: [(b'the', 7763), (b'a', 4927), (b'of', 4380), (b'and', 4302), (b'I', 3597), (b'to', 3531), (b'is', 3295), (b'this', 2425), (b'in', 2376), (b'it', 2245)]\n",
            "Truncated vocabulary size: 10000\n",
            "\n",
            "Lookup table test:\n",
            "'this' -> 7 (in vocab)\n",
            "'movie' -> 11 (in vocab)\n",
            "'was' -> 10 (in vocab)\n",
            "'fantaaaastic' -> 10415 (OOV)\n",
            "\n",
            "Creating final training dataset...\n",
            "\n",
            "Final dataset batch shapes:\n",
            "X_batch: (32, 60)\n",
            "y_batch: (32,)\n",
            "Sample encoded review: [  21   10   27  359  313   11  696   24 8129    8   31 1342 2745   44\n",
            "  538 8130 2096   25   60  148]\n",
            "Sample label: 0\n",
            "\n",
            "Final vocabulary configuration:\n",
            "In-vocabulary words: 10,000\n",
            "Out-of-vocabulary buckets: 1,000\n",
            "Total vocabulary size: 11,000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sentiment_model_theory"
      },
      "source": [
        "### Sentiment Analysis Model Architecture Theory\n",
        "\n",
        "#### Embedding Layer Mathematics\n",
        "\n",
        "**Word Embeddings:**\n",
        "Transform sparse one-hot vectors into dense representations:\n",
        "\n",
        "$$\\mathbf{e}_w = \\mathbf{E} \\cdot \\mathbf{o}_w$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{e}_w \\in \\mathbb{R}^d$: word embedding\n",
        "- $\\mathbf{E} \\in \\mathbb{R}^{V \\times d}$: embedding matrix\n",
        "- $\\mathbf{o}_w \\in \\{0,1\\}^V$: one-hot vector\n",
        "- $V$: vocabulary size\n",
        "- $d$: embedding dimension\n",
        "\n",
        "**Efficiency:** Direct lookup instead of matrix multiplication:\n",
        "$$\\mathbf{e}_w = \\mathbf{E}[\\text{word_id}]$$\n",
        "\n",
        "#### RNN for Sequence Classification\n",
        "\n",
        "**Architecture:**\n",
        "1. **Embedding**: Words → Dense vectors\n",
        "2. **RNN Layers**: Sequential processing\n",
        "3. **Global Information**: Last hidden state or pooling\n",
        "4. **Classification**: Dense layer with sigmoid\n",
        "\n",
        "**Mathematical Flow:**\n",
        "$$\\mathbf{h}_t = \\text{GRU}(\\mathbf{e}_{w_t}, \\mathbf{h}_{t-1})$$\n",
        "$$\\text{sentiment} = \\sigma(\\mathbf{W} \\mathbf{h}_T + b)$$\n",
        "\n",
        "Where $\\mathbf{h}_T$ is the final hidden state.\n",
        "\n",
        "#### Binary Classification Loss\n",
        "\n",
        "**Binary Cross-Entropy:**\n",
        "$$L = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]$$\n",
        "\n",
        "Where:\n",
        "- $y_i \\in \\{0, 1\\}$: true label\n",
        "- $\\hat{y}_i \\in [0, 1]$: predicted probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sentiment_model",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "fd1fa1f4-d63a-43e0-ef4e-1aad493cb2a3"
      },
      "source": [
        "# Sentiment Analysis Model Implementation\n",
        "# This demonstrates word-level RNN for sentiment classification\n",
        "\n",
        "def create_sentiment_model(vocab_size, embedding_dim=128, rnn_units=128,\n",
        "                          max_length=None, mask_zero=True):\n",
        "    \"\"\"\n",
        "    Create a sentiment analysis model using word-level RNN.\n",
        "\n",
        "    Args:\n",
        "        vocab_size: Size of vocabulary (including OOV buckets)\n",
        "        embedding_dim: Dimension of word embeddings\n",
        "        rnn_units: Number of units in RNN layers\n",
        "        max_length: Maximum sequence length (None for variable)\n",
        "        mask_zero: Whether to mask padding tokens\n",
        "\n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    model = keras.models.Sequential([\n",
        "        # Embedding layer: converts word IDs to dense vectors\n",
        "        # mask_zero=True: ignore padding tokens (ID=0) in downstream layers\n",
        "        keras.layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            input_shape=[max_length],\n",
        "            mask_zero=mask_zero\n",
        "        ),\n",
        "\n",
        "        # First GRU layer: process word sequences\n",
        "        # return_sequences=True: return all hidden states, not just last\n",
        "        keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            dropout=0.3,\n",
        "            recurrent_dropout=0.3\n",
        "        ),\n",
        "\n",
        "        # Second GRU layer: further processing\n",
        "        # return_sequences=False: return only last hidden state\n",
        "        keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=False,  # Only final output for classification\n",
        "            dropout=0.3,\n",
        "            recurrent_dropout=0.3\n",
        "        ),\n",
        "\n",
        "        # Classification layer: binary sentiment prediction\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create sentiment analysis model\n",
        "total_vocab_size = vocab_size + num_oov_buckets\n",
        "sentiment_model = create_sentiment_model(\n",
        "    vocab_size=total_vocab_size,\n",
        "    embedding_dim=128,\n",
        "    rnn_units=128,\n",
        "    mask_zero=True\n",
        ")\n",
        "\n",
        "print(\"Sentiment Analysis Model Architecture:\")\n",
        "sentiment_model.summary()\n",
        "\n",
        "# Compile the model\n",
        "sentiment_model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Calculate parameter breakdown\n",
        "total_params = sentiment_model.count_params()\n",
        "print(f\"\\nParameter Analysis:\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "# Detailed parameter breakdown\n",
        "embedding_params = total_vocab_size * 128  # vocab_size × embedding_dim\n",
        "gru1_params = 3 * 128 * (128 + 128 + 1)  # 3 gates × units × (input + hidden + bias)\n",
        "gru2_params = 3 * 128 * (128 + 128 + 1)  # Same structure\n",
        "dense_params = 128 + 1  # weights + bias\n",
        "\n",
        "print(f\"\\nParameter breakdown:\")\n",
        "print(f\"Embedding layer: {embedding_params:,} parameters\")\n",
        "print(f\"First GRU layer: {gru1_params:,} parameters\")\n",
        "print(f\"Second GRU layer: {gru2_params:,} parameters\")\n",
        "print(f\"Dense layer: {dense_params:,} parameters\")\n",
        "print(f\"Total calculated: {embedding_params + gru1_params + gru2_params + dense_params:,}\")\n",
        "\n",
        "# Memory analysis\n",
        "print(f\"\\nMemory Analysis:\")\n",
        "print(f\"Embedding matrix size: {total_vocab_size} × {128} = {total_vocab_size * 128 * 4 / 1024**2:.1f} MB\")\n",
        "print(f\"Each embedding vector: {128 * 4} bytes\")\n",
        "\n",
        "# Test model with sample data\n",
        "print(\"\\nTesting model with sample data...\")\n",
        "for X_batch, y_batch in train_set.take(1):\n",
        "    # Test forward pass\n",
        "    predictions = sentiment_model.predict(X_batch[:5], verbose=0)\n",
        "    print(f\"Sample predictions shape: {predictions.shape}\")\n",
        "    print(f\"Sample predictions: {predictions.flatten()[:5]}\")\n",
        "    print(f\"Corresponding labels: {y_batch[:5].numpy()}\")\n",
        "\n",
        "    # Show input shape handling\n",
        "    print(f\"\\nInput batch shape: {X_batch.shape}\")\n",
        "    print(f\"Model input shape: {sentiment_model.input_shape}\")\n",
        "    print(f\"Model output shape: {sentiment_model.output_shape}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis Model Architecture:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │     \u001b[38;5;34m1,408,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_4 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m99,072\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_5 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m99,072\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,606,273\u001b[0m (6.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,606,273</span> (6.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,606,273\u001b[0m (6.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,606,273</span> (6.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameter Analysis:\n",
            "Total parameters: 1,606,273\n",
            "\n",
            "Parameter breakdown:\n",
            "Embedding layer: 1,408,000 parameters\n",
            "First GRU layer: 98,688 parameters\n",
            "Second GRU layer: 98,688 parameters\n",
            "Dense layer: 129 parameters\n",
            "Total calculated: 1,605,505\n",
            "\n",
            "Memory Analysis:\n",
            "Embedding matrix size: 11000 × 128 = 5.4 MB\n",
            "Each embedding vector: 512 bytes\n",
            "\n",
            "Testing model with sample data...\n",
            "Sample predictions shape: (5, 1)\n",
            "Sample predictions: [0.49957085 0.49926686 0.49948853 0.49832264 0.50011516]\n",
            "Corresponding labels: [0 0 0 1 1]\n",
            "\n",
            "Input batch shape: (32, 60)\n",
            "Model input shape: (None, None)\n",
            "Model output shape: (None, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "masking_detailed_theory"
      },
      "source": [
        "### Advanced Masking Theory and Implementation\n",
        "\n",
        "#### Mathematical Foundation of Masking\n",
        "\n",
        "**Masking Tensor Creation:**\n",
        "For input sequence $X = [x_1, x_2, ..., x_T]$ where $x_i$ are word IDs:\n",
        "\n",
        "$$M[t] = \\begin{cases}\n",
        "1 & \\text{if } x_t \\neq 0 \\text{ (valid token)} \\\\\n",
        "0 & \\text{if } x_t = 0 \\text{ (padding token)}\n",
        "\\end{cases}$$\n",
        "\n",
        "**Masked RNN Computation:**\n",
        "Traditional RNN: $h_t = f(x_t, h_{t-1})$\n",
        "\n",
        "Masked RNN: $h_t = \\begin{cases}\n",
        "f(x_t, h_{t-1}) & \\text{if } M[t] = 1 \\\\\n",
        "h_{t-1} & \\text{if } M[t] = 0\n",
        "\\end{cases}$\n",
        "\n",
        "**Masked Loss Computation:**\n",
        "$$L = \\frac{\\sum_{t=1}^T M[t] \\cdot \\ell(y_t, \\hat{y}_t)}{\\sum_{t=1}^T M[t]}$$\n",
        "\n",
        "Where $\\ell$ is the element-wise loss function.\n",
        "\n",
        "#### GPU Optimization Considerations\n",
        "\n",
        "**CuDNN Implementation:**\n",
        "- **Optimized path**: No masking, default hyperparameters\n",
        "- **Fallback path**: With masking, custom implementations\n",
        "- **Performance impact**: 2-10x slower with masking on GPU\n",
        "\n",
        "**Layer Requirements for CuDNN:**\n",
        "- `activation='tanh'` (GRU) or `activation='tanh'` (LSTM)\n",
        "- `recurrent_activation='sigmoid'`\n",
        "- `recurrent_dropout=0.0`\n",
        "- `unroll=False`\n",
        "- `use_bias=True`\n",
        "- `reset_after=True` (GRU only)\n",
        "\n",
        "#### Manual Masking Implementation\n",
        "\n",
        "**Functional API Approach:**\n",
        "```python\n",
        "# Create mask manually\n",
        "mask = keras.backend.not_equal(inputs, 0)\n",
        "# Apply to RNN layers\n",
        "rnn_output = keras.layers.GRU(units)(embeddings, mask=mask)\n",
        "```\n",
        "\n",
        "**Custom Mask Propagation:**\n",
        "Essential for complex architectures mixing different layer types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "masking_implementation",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24737ae3-3505-4c78-f593-98110ec84fed"
      },
      "source": [
        "# Advanced Masking Implementation and Analysis\n",
        "# This demonstrates both automatic and manual masking approaches\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "print(\"Implementing Masking Mechanisms...\")\n",
        "\n",
        "def create_model_with_masking(vocab_size, embedding_dim=128, use_masking=True):\n",
        "    \"\"\"\n",
        "    Create sentiment model with optional masking.\n",
        "\n",
        "    Args:\n",
        "        vocab_size: Vocabulary size\n",
        "        embedding_dim: Embedding dimension\n",
        "        use_masking: Whether to use automatic masking\n",
        "\n",
        "    Returns:\n",
        "        Keras model\n",
        "    \"\"\"\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Embedding(\n",
        "            vocab_size,\n",
        "            embedding_dim,\n",
        "            input_shape=[None],\n",
        "            mask_zero=use_masking  # Key parameter\n",
        "        ),\n",
        "        keras.layers.GRU(\n",
        "            64,\n",
        "            return_sequences=True,\n",
        "            # Disable CuDNN when using masking for compatibility\n",
        "            use_cudnn=False if use_masking else True\n",
        "        ),\n",
        "        keras.layers.GRU(\n",
        "            64,\n",
        "            # Disable CuDNN when using masking for compatibility\n",
        "            use_cudnn=False if use_masking else True\n",
        "        ),\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create models with and without masking for comparison\n",
        "print(\"Creating models with and without masking...\")\n",
        "model_with_mask = create_model_with_masking(total_vocab_size, use_masking=True)\n",
        "model_without_mask = create_model_with_masking(total_vocab_size, use_masking=False)\n",
        "\n",
        "print(\"\\nModel with masking:\")\n",
        "model_with_mask.summary()\n",
        "\n",
        "# Manual masking implementation using Functional API\n",
        "def create_manual_masking_model(vocab_size, embedding_dim=128):\n",
        "    \"\"\"\n",
        "    Demonstrate manual masking using Functional API.\n",
        "    This shows how to handle masking in complex architectures.\n",
        "    \"\"\"\n",
        "    # Input layer\n",
        "    inputs = keras.layers.Input(shape=[None])\n",
        "\n",
        "    # Create mask manually using a Lambda layer and call it with inputs\n",
        "    mask_layer = keras.layers.Lambda(\n",
        "        lambda inputs: keras.backend.not_equal(inputs, 0)\n",
        "    )\n",
        "    mask = mask_layer(inputs) # Call the lambda layer with inputs to get the mask tensor\n",
        "\n",
        "\n",
        "    # Embedding without automatic masking\n",
        "    embeddings = keras.layers.Embedding(\n",
        "        vocab_size, embedding_dim, mask_zero=False\n",
        "    )(inputs)\n",
        "\n",
        "    # Apply manual mask to RNN layers\n",
        "    # Disable CuDNN when using manual masking\n",
        "    rnn1 = keras.layers.GRU(64, return_sequences=True, use_cudnn=False)(embeddings, mask=mask)\n",
        "    rnn2 = keras.layers.GRU(64, use_cudnn=False)(rnn1, mask=mask)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(rnn2)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "manual_mask_model = create_manual_masking_model(total_vocab_size)\n",
        "print(\"\\nModel with manual masking:\")\n",
        "manual_mask_model.summary()\n",
        "\n",
        "# Test masking behavior with sample data\n",
        "print(\"\\nTesting masking behavior...\")\n",
        "\n",
        "# Create test sequences with different padding\n",
        "test_sequences = np.array([\n",
        "    [1, 2, 3, 4, 5],      # No padding\n",
        "    [1, 2, 3, 0, 0],      # Padded with 2 zeros\n",
        "    [1, 0, 0, 0, 0],      # Heavily padded\n",
        "    [0, 0, 0, 0, 0]       # All padding\n",
        "])\n",
        "\n",
        "print(f\"Test sequences shape: {test_sequences.shape}\")\n",
        "print(f\"Test sequences:\")\n",
        "for i, seq in enumerate(test_sequences):\n",
        "    non_zero = np.count_nonzero(seq)\n",
        "    print(f\"  Sequence {i+1}: {seq} (non-zero: {non_zero})\")\n",
        "\n",
        "# Compare predictions with and without masking\n",
        "pred_with_mask = model_with_mask.predict(test_sequences, verbose=0)\n",
        "pred_without_mask = model_without_mask.predict(test_sequences, verbose=0)\n",
        "pred_manual_mask = manual_mask_model.predict(test_sequences, verbose=0)\n",
        "\n",
        "print(f\"\\nPrediction comparison:\")\n",
        "print(f\"{'Sequence':<10} {'With Mask':<12} {'Without Mask':<12} {'Manual Mask':<12}\")\n",
        "print(\"-\" * 50)\n",
        "for i in range(len(test_sequences)):\n",
        "    print(f\"{i+1:<10} {pred_with_mask[i][0]:<12.4f} {pred_without_mask[i][0]:<12.4f} {pred_manual_mask[i][0]:<12.4f}\")\n",
        "\n",
        "# Analyze mask propagation\n",
        "def analyze_mask_propagation():\n",
        "    \"\"\"Analyze how masks propagate through layers.\"\"\"\n",
        "    print(\"\\nMask Propagation Analysis:\")\n",
        "\n",
        "    # Check which layers support masking\n",
        "    for i, layer in enumerate(model_with_mask.layers):\n",
        "        supports_masking = getattr(layer, 'supports_masking', False)\n",
        "        print(f\"Layer {i+1} ({layer.__class__.__name__}): supports_masking = {supports_masking}\")\n",
        "\n",
        "    # Demonstrate mask creation\n",
        "    sample_input = tf.constant([[1, 2, 3, 0, 0], [1, 0, 0, 0, 0]])\n",
        "\n",
        "    # Get embedding layer output and mask\n",
        "    embedding_layer = model_with_mask.layers[0]\n",
        "    embeddings = embedding_layer(sample_input)\n",
        "\n",
        "    # Manually create mask for demonstration\n",
        "    mask = keras.backend.not_equal(sample_input, 0)\n",
        "\n",
        "    print(f\"\\nSample input: {sample_input.numpy()}\")\n",
        "    print(f\"Generated mask: {mask.numpy()}\")\n",
        "    print(f\"Embedding shape: {embeddings.shape}\")\n",
        "\n",
        "    return mask\n",
        "\n",
        "analyze_mask_propagation()\n",
        "\n",
        "# Performance comparison\n",
        "import time\n",
        "\n",
        "def benchmark_masking_performance():\n",
        "    \"\"\"Compare performance of masked vs unmasked models.\"\"\"\n",
        "    print(\"\\nBenchmarking masking performance...\")\n",
        "\n",
        "    # Create larger test data\n",
        "    test_data = np.random.randint(1, 1000, size=(100, 50))\n",
        "    # Add some padding\n",
        "    test_data[:, 30:] = 0  # Pad last 20 positions\n",
        "\n",
        "    # Benchmark with masking\n",
        "    start_time = time.time()\n",
        "    _ = model_with_mask.predict(test_data, verbose=0)\n",
        "    time_with_mask = time.time() - start_time\n",
        "\n",
        "    # Benchmark without masking\n",
        "    start_time = time.time()\n",
        "    _ = model_without_mask.predict(test_data, verbose=0)\n",
        "    time_without_mask = time.time() - start_time\n",
        "\n",
        "    print(f\"Time with masking: {time_with_mask:.4f} seconds\")\n",
        "    print(f\"Time without masking: {time_without_mask:.4f} seconds\")\n",
        "    print(f\"Slowdown factor: {time_with_mask / time_without_mask:.2f}x\")\n",
        "\n",
        "    return time_with_mask, time_without_mask\n",
        "\n",
        "benchmark_masking_performance()\n",
        "\n",
        "# Best practices summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MASKING BEST PRACTICES\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n1. AUTOMATIC MASKING (Recommended):\")\n",
        "print(\"   • Use mask_zero=True in Embedding layer\")\n",
        "print(\"   • Ensure all subsequent layers support masking\")\n",
        "print(\"   • Works well for simple Sequential models\")\n",
        "\n",
        "print(\"\\n2. MANUAL MASKING:\")\n",
        "print(\"   • Use when mixing different layer types\")\n",
        "print(\"   • Required for complex architectures\")\n",
        "print(\"   • More control but more complex\")\n",
        "\n",
        "print(\"\\n3. PERFORMANCE CONSIDERATIONS:\")\n",
        "print(\"   • Masking disables CuDNN optimizations\")\n",
        "print(\"   • Consider bucketing for large performance gains\")\n",
        "print(\"   • Use masking only when necessary\")\n",
        "\n",
        "print(\"\\n4. ALTERNATIVE APPROACHES:\")\n",
        "print(\"   • Bucketing: Group sequences by length\")\n",
        "print(\"   • Dynamic batching: Variable batch sizes\")\n",
        "print(\"   • Attention mechanisms: Handle variable lengths naturally\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Implementing Masking Mechanisms...\n",
            "Creating models with and without masking...\n",
            "\n",
            "Model with masking:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_15 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │     \u001b[38;5;34m1,408,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_28 (\u001b[38;5;33mGRU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m37,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_29 (\u001b[38;5;33mGRU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m24,960\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,960</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,470,273\u001b[0m (5.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,470,273</span> (5.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,470,273\u001b[0m (5.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,470,273</span> (5.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model with manual masking:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_15\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_15\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_16      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_17        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,408,000\u001b[0m │ input_layer_16[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_4 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_16[\u001b[38;5;34m0\u001b[0m… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_32 (\u001b[38;5;33mGRU\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m37,248\u001b[0m │ embedding_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │                   │            │ lambda_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_33 (\u001b[38;5;33mGRU\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m24,960\u001b[0m │ gru_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                     │                   │            │ lambda_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ gru_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_16      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_17        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408,000</span> │ input_layer_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">37,248</span> │ embedding_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │                   │            │ lambda_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,960</span> │ gru_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                     │                   │            │ lambda_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ gru_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,470,273\u001b[0m (5.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,470,273</span> (5.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,470,273\u001b[0m (5.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,470,273</span> (5.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing masking behavior...\n",
            "Test sequences shape: (4, 5)\n",
            "Test sequences:\n",
            "  Sequence 1: [1 2 3 4 5] (non-zero: 5)\n",
            "  Sequence 2: [1 2 3 0 0] (non-zero: 3)\n",
            "  Sequence 3: [1 0 0 0 0] (non-zero: 1)\n",
            "  Sequence 4: [0 0 0 0 0] (non-zero: 0)\n",
            "\n",
            "Prediction comparison:\n",
            "Sequence   With Mask    Without Mask Manual Mask \n",
            "--------------------------------------------------\n",
            "1          0.4992       0.4982       0.5005      \n",
            "2          0.4986       0.4971       0.5002      \n",
            "3          0.4993       0.4946       0.5012      \n",
            "4          0.5000       0.4953       0.5000      \n",
            "\n",
            "Mask Propagation Analysis:\n",
            "Layer 1 (Embedding): supports_masking = True\n",
            "Layer 2 (GRU): supports_masking = True\n",
            "Layer 3 (GRU): supports_masking = True\n",
            "Layer 4 (Dense): supports_masking = True\n",
            "\n",
            "Sample input: [[1 2 3 0 0]\n",
            " [1 0 0 0 0]]\n",
            "Generated mask: [[ True  True  True False False]\n",
            " [ True False False False False]]\n",
            "Embedding shape: (2, 5, 128)\n",
            "\n",
            "Benchmarking masking performance...\n",
            "Time with masking: 1.1804 seconds\n",
            "Time without masking: 0.2424 seconds\n",
            "Slowdown factor: 4.87x\n",
            "\n",
            "============================================================\n",
            "MASKING BEST PRACTICES\n",
            "============================================================\n",
            "\n",
            "1. AUTOMATIC MASKING (Recommended):\n",
            "   • Use mask_zero=True in Embedding layer\n",
            "   • Ensure all subsequent layers support masking\n",
            "   • Works well for simple Sequential models\n",
            "\n",
            "2. MANUAL MASKING:\n",
            "   • Use when mixing different layer types\n",
            "   • Required for complex architectures\n",
            "   • More control but more complex\n",
            "\n",
            "3. PERFORMANCE CONSIDERATIONS:\n",
            "   • Masking disables CuDNN optimizations\n",
            "   • Consider bucketing for large performance gains\n",
            "   • Use masking only when necessary\n",
            "\n",
            "4. ALTERNATIVE APPROACHES:\n",
            "   • Bucketing: Group sequences by length\n",
            "   • Dynamic batching: Variable batch sizes\n",
            "   • Attention mechanisms: Handle variable lengths naturally\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sentiment_training",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e98a7360-6616-4768-82cf-1a62cd27534e"
      },
      "source": [
        "# Train the Sentiment Analysis Model\n",
        "# This demonstrates the complete training pipeline for sentiment analysis\n",
        "\n",
        "print(\"Training Sentiment Analysis Model...\")\n",
        "\n",
        "# Prepare validation set\n",
        "val_set = datasets[\"test\"].batch(32).map(preprocess_text)\n",
        "val_set = val_set.map(encode_words).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Create callbacks for training\n",
        "sentiment_callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=3,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        'sentiment_model_best.weights.h5',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the model (reduced epochs for demonstration)\n",
        "print(\"\\nStarting training...\")\n",
        "sentiment_epochs = 3  # Increase to 10-20 for better performance\n",
        "\n",
        "# Limit training data for demonstration\n",
        "train_subset = train_set.take(500)  # Use subset for faster training\n",
        "val_subset = val_set.take(100)\n",
        "\n",
        "history = sentiment_model.fit(\n",
        "    train_subset,\n",
        "    epochs=sentiment_epochs,\n",
        "    validation_data=val_subset,\n",
        "    callbacks=sentiment_callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "if 'lr' in history.history:\n",
        "    plt.plot(history.history['lr'], label='Learning Rate', marker='d')\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.yscale('log')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'Learning Rate\\nNot Recorded',\n",
        "             ha='center', va='center', transform=plt.gca().transAxes)\n",
        "    plt.title('Learning Rate (Not Available)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate final performance\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "print(f\"\\nFinal Training Results:\")\n",
        "print(f\"Training Accuracy: {final_train_acc:.4f}\")\n",
        "print(f\"Validation Accuracy: {final_val_acc:.4f}\")\n",
        "print(f\"Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Validation Loss: {final_val_loss:.4f}\")\n",
        "print(f\"Overfitting Gap (Acc): {final_train_acc - final_val_acc:.4f}\")\n",
        "print(f\"Overfitting Gap (Loss): {final_val_loss - final_train_loss:.4f}\")\n",
        "\n",
        "# Test the model with custom examples\n",
        "def test_sentiment_prediction(model, texts, tokenizer_table):\n",
        "    \"\"\"\n",
        "    Test sentiment prediction on custom text examples.\n",
        "\n",
        "    Args:\n",
        "        model: Trained sentiment model\n",
        "        texts: List of text strings\n",
        "        tokenizer_table: Lookup table for word encoding\n",
        "\n",
        "    Returns:\n",
        "        Predictions and analysis\n",
        "    \"\"\"\n",
        "    # Preprocess texts\n",
        "    processed_texts = []\n",
        "    for text in texts:\n",
        "        # Convert to tensor and apply preprocessing\n",
        "        text_tensor = tf.constant([text.encode('utf-8')])\n",
        "        processed, _ = preprocess_text(text_tensor, tf.constant([0]))\n",
        "        processed_texts.append(processed[0])\n",
        "\n",
        "    # Convert to tensor and encode\n",
        "    max_len = max(len(pt) for pt in processed_texts)\n",
        "    padded_texts = []\n",
        "    for pt in processed_texts:\n",
        "        # Pad to max length\n",
        "        padding_needed = max_len - len(pt)\n",
        "        if padding_needed > 0:\n",
        "            padded = tf.concat([pt, tf.constant([b\"<pad>\"] * padding_needed)], axis=0)\n",
        "        else:\n",
        "            padded = pt\n",
        "        padded_texts.append(padded)\n",
        "\n",
        "    # Stack and encode\n",
        "    text_batch = tf.stack(padded_texts)\n",
        "    encoded_batch = tokenizer_table.lookup(text_batch)\n",
        "\n",
        "    # Predict\n",
        "    predictions = model.predict(encoded_batch, verbose=0)\n",
        "\n",
        "    return predictions, encoded_batch\n",
        "\n",
        "# Test with custom examples\n",
        "test_texts = [\n",
        "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
        "    \"Terrible film. Complete waste of time. Don't watch this garbage.\",\n",
        "    \"The movie was okay, nothing special but not bad either.\",\n",
        "    \"Amazing cinematography and brilliant acting. Highly recommended!\",\n",
        "    \"Boring and predictable plot. Fell asleep halfway through.\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting sentiment predictions on custom examples:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "predictions, encoded = test_sentiment_prediction(sentiment_model, test_texts, table)\n",
        "\n",
        "for i, (text, pred) in enumerate(zip(test_texts, predictions)):\n",
        "    sentiment = \"Positive\" if pred[0] > 0.5 else \"Negative\"\n",
        "    confidence = pred[0] if pred[0] > 0.5 else 1 - pred[0]\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Prediction: {sentiment} (confidence: {confidence:.3f}, raw: {pred[0]:.3f})\")\n",
        "    print(f\"Encoded length: {np.count_nonzero(encoded[i].numpy())} words\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Sentiment Analysis Model...\n",
            "\n",
            "Starting training...\n",
            "Epoch 1/3\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.5022 - loss: 0.6943\n",
            "Epoch 1: val_accuracy improved from -inf to 0.50250, saving model to sentiment_model_best.weights.h5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 324ms/step - accuracy: 0.5022 - loss: 0.6943 - val_accuracy: 0.5025 - val_loss: 0.6930 - learning_rate: 0.0010\n",
            "Epoch 2/3\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.5660 - loss: 0.6822\n",
            "Epoch 2: val_accuracy improved from 0.50250 to 0.66031, saving model to sentiment_model_best.weights.h5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 322ms/step - accuracy: 0.5661 - loss: 0.6822 - val_accuracy: 0.6603 - val_loss: 0.6249 - learning_rate: 0.0010\n",
            "Epoch 3/3\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.7575 - loss: 0.5118\n",
            "Epoch 3: val_accuracy improved from 0.66031 to 0.73875, saving model to sentiment_model_best.weights.h5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 322ms/step - accuracy: 0.7575 - loss: 0.5118 - val_accuracy: 0.7387 - val_loss: 0.5269 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 3.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Training Results:\n",
            "Training Accuracy: 0.7650\n",
            "Validation Accuracy: 0.7387\n",
            "Training Loss: 0.4978\n",
            "Validation Loss: 0.5269\n",
            "Overfitting Gap (Acc): 0.0263\n",
            "Overfitting Gap (Loss): 0.0291\n",
            "\n",
            "Testing sentiment predictions on custom examples:\n",
            "============================================================\n",
            "\n",
            "Text: This movie was absolutely fantastic! I loved every minute of it.\n",
            "Prediction: Positive (confidence: 0.901, raw: 0.901)\n",
            "Encoded length: 11 words\n",
            "\n",
            "Text: Terrible film. Complete waste of time. Don't watch this garbage.\n",
            "Prediction: Negative (confidence: 0.963, raw: 0.037)\n",
            "Encoded length: 11 words\n",
            "\n",
            "Text: The movie was okay, nothing special but not bad either.\n",
            "Prediction: Negative (confidence: 0.957, raw: 0.043)\n",
            "Encoded length: 11 words\n",
            "\n",
            "Text: Amazing cinematography and brilliant acting. Highly recommended!\n",
            "Prediction: Positive (confidence: 0.922, raw: 0.922)\n",
            "Encoded length: 11 words\n",
            "\n",
            "Text: Boring and predictable plot. Fell asleep halfway through.\n",
            "Prediction: Negative (confidence: 0.965, raw: 0.035)\n",
            "Encoded length: 11 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pretrained_embeddings_theory"
      },
      "source": [
        "## Pretrained Embeddings Theory and Implementation\n",
        "\n",
        "### Theory: Transfer Learning for NLP\n",
        "\n",
        "**Core Concept:** Reuse word representations learned from large corpora\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "Instead of learning embeddings $E \\in \\mathbb{R}^{V \\times d}$ from scratch, use pretrained $E_{\\text{pretrained}}$ learned on corpus $C_{\\text{large}}$:\n",
        "\n",
        "$$E_{\\text{task}} = \\begin{cases}\n",
        "E_{\\text{pretrained}} & \\text{frozen (feature extraction)} \\\\\n",
        "\\text{finetune}(E_{\\text{pretrained}}, D_{\\text{task}}) & \\text{fine-tuning}\n",
        "\\end{cases}$$\n",
        "\n",
        "### Advantages of Pretrained Embeddings\n",
        "\n",
        "**1. Semantic Relationships:**\n",
        "Words with similar meanings cluster together:\n",
        "$$\\text{similarity}(\\mathbf{e}_{\\text{\"awesome\"}}, \\mathbf{e}_{\\text{\"amazing\"}}) > \\text{similarity}(\\mathbf{e}_{\\text{\"awesome\"}}, \\mathbf{e}_{\\text{\"terrible\"}})$$\n",
        "\n",
        "**2. Data Efficiency:**\n",
        "- Require fewer training examples\n",
        "- Faster convergence\n",
        "- Better generalization\n",
        "\n",
        "**3. Domain Transfer:**\n",
        "Even if trained on different domains (e.g., Wikipedia), semantic relationships often transfer.\n",
        "\n",
        "### TensorFlow Hub Integration\n",
        "\n",
        "**TF Hub Architecture:**\n",
        "- **Modules**: Reusable ML components\n",
        "- **Versioning**: Ensures reproducibility\n",
        "- **Caching**: Local storage for efficiency\n",
        "\n",
        "**Sentence Encoders:**\n",
        "Instead of word-level embeddings, use sentence-level:\n",
        "$$\\mathbf{s} = f([w_1, w_2, ..., w_T]) \\in \\mathbb{R}^d$$\n",
        "\n",
        "Where $f$ can be:\n",
        "- **Mean pooling**: $\\mathbf{s} = \\frac{1}{T} \\sum_{i=1}^T \\mathbf{e}_{w_i}$\n",
        "- **Weighted average**: Account for word importance\n",
        "- **Learned aggregation**: Neural network combination\n",
        "\n",
        "### NNLM (Neural Network Language Model)\n",
        "\n",
        "**Architecture:**\n",
        "1. **Input**: Sequence of word IDs\n",
        "2. **Embedding**: Word → dense vectors\n",
        "3. **Hidden layers**: Neural network processing\n",
        "4. **Output**: Next word probabilities\n",
        "\n",
        "**Training Objective:**\n",
        "$$L = -\\sum_{t=1}^T \\log P(w_t | w_1, ..., w_{t-1})$$\n",
        "\n",
        "**Google News 7B Corpus:**\n",
        "- 7 billion words\n",
        "- Rich semantic representations\n",
        "- Covers diverse topics and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pretrained_embeddings",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8749a228-9853-435f-f97e-dfadec150530"
      },
      "source": [
        "# Pretrained Embeddings with TensorFlow Hub\n",
        "# This demonstrates using pretrained sentence encoders for sentiment analysis\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Implementing Pretrained Embeddings with TensorFlow Hub...\")\n",
        "\n",
        "# Define a custom layer to wrap the TF Hub layer call\n",
        "class TFPredictWrapper(keras.layers.Layer):\n",
        "    def __init__(self, hub_layer, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hub_layer = hub_layer\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.hub_layer(inputs)\n",
        "\n",
        "\n",
        "def create_tfhub_sentiment_model(tfhub_url, trainable=False):\n",
        "    \"\"\"\n",
        "    Create sentiment analysis model using TensorFlow Hub embeddings (Functional API).\n",
        "\n",
        "    Args:\n",
        "        tfhub_url: URL of TensorFlow Hub module\n",
        "        trainable: Whether to fine-tune the embeddings\n",
        "\n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    # Input layer - specify shape and dtype expected by the TF Hub layer\n",
        "    inputs = keras.Input(shape=[], dtype=tf.string, name=\"text_inputs\")\n",
        "\n",
        "    # TensorFlow Hub layer for sentence encoding\n",
        "    # This layer takes raw text as input and outputs sentence embeddings\n",
        "    hub_layer = hub.KerasLayer(\n",
        "        tfhub_url,\n",
        "        input_shape=[],       # Scalar input (single string) - specified in Input now\n",
        "        output_shape=[50],    # 50-dimensional embeddings\n",
        "        dtype=tf.string,      # Input type: text strings - specified in Input now\n",
        "        trainable=trainable   # Whether to fine-tune\n",
        "    )\n",
        "\n",
        "    # Wrap the hub_layer call in a custom layer\n",
        "    wrapped_hub_layer = TFPredictWrapper(hub_layer)(inputs)\n",
        "\n",
        "\n",
        "    # Add Dense layers for classification\n",
        "    x = keras.layers.Dense(128, activation=\"relu\")(wrapped_hub_layer) # Use wrapped output\n",
        "    x = keras.layers.Dropout(0.5)(x)\n",
        "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# TensorFlow Hub NNLM module URL (50-dimensional embeddings)\n",
        "nnlm_url = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\"\n",
        "\n",
        "print(f\"Using TensorFlow Hub module: {nnlm_url}\")\n",
        "print(\"This module:\")\n",
        "print(\"• Takes text strings as input\")\n",
        "print(\"• Outputs 50-dimensional sentence embeddings\")\n",
        "print(\"• Trained on Google News 7B corpus\")\n",
        "print(\"• Computes mean of word embeddings with length normalization\")\n",
        "\n",
        "# Create model with frozen embeddings\n",
        "tfhub_model_frozen = create_tfhub_sentiment_model(nnlm_url, trainable=False)\n",
        "\n",
        "print(\"\\nTensorFlow Hub Model Architecture (Frozen Embeddings):\")\n",
        "tfhub_model_frozen.summary()\n",
        "\n",
        "# Create model with trainable embeddings\n",
        "tfhub_model_trainable = create_tfhub_sentiment_model(nnlm_url, trainable=True)\n",
        "\n",
        "print(\"\\nTensorFlow Hub Model Architecture (Trainable Embeddings):\")\n",
        "tfhub_model_trainable.summary()\n",
        "\n",
        "# Parameter comparison\n",
        "frozen_params = tfhub_model_frozen.count_params()\n",
        "trainable_params = tfhub_model_trainable.count_params()\n",
        "\n",
        "print(f\"\\nParameter Comparison:\")\n",
        "print(f\"Frozen embeddings: {frozen_params:,} trainable parameters\")\n",
        "print(f\"Trainable embeddings: {trainable_params:,} trainable parameters\")\n",
        "print(f\"Difference: {trainable_params - frozen_params:,} parameters\")\n",
        "\n",
        "# Prepare data for TensorFlow Hub model (raw text, not encoded)\n",
        "print(\"\\nPreparing data for TensorFlow Hub model...\")\n",
        "\n",
        "# Load raw text data for TF Hub (no preprocessing needed)\n",
        "raw_train_set = datasets[\"train\"].batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "raw_val_set = datasets[\"test\"].batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Take subset for demonstration\n",
        "train_subset_hub = raw_train_set.take(100)\n",
        "val_subset_hub = raw_val_set.take(20)\n",
        "\n",
        "# Test the TF Hub model with sample data\n",
        "print(\"\\nTesting TensorFlow Hub model...\")\n",
        "for texts, labels in raw_train_set.take(1):\n",
        "    print(f\"Sample input shape: {texts.shape}\")\n",
        "    print(f\"Sample input type: {texts.dtype}\")\n",
        "    print(f\"First text: {texts[0].numpy()[:100]}...\")\n",
        "\n",
        "    # Test prediction\n",
        "    predictions = tfhub_model_frozen.predict(texts[:3], verbose=0)\n",
        "    print(f\"Predictions shape: {predictions.shape}\")\n",
        "    print(f\"Sample predictions: {predictions.flatten()[:3]}\")\n",
        "\n",
        "    break\n",
        "\n",
        "# Train the TensorFlow Hub model (frozen embeddings)\n",
        "print(\"\\nTraining TensorFlow Hub model with frozen embeddings...\")\n",
        "\n",
        "hub_history_frozen = tfhub_model_frozen.fit(\n",
        "    train_subset_hub,\n",
        "    epochs=3,\n",
        "    validation_data=val_subset_hub,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the TensorFlow Hub model (trainable embeddings)\n",
        "print(\"\\nTraining TensorFlow Hub model with trainable embeddings...\")\n",
        "\n",
        "hub_history_trainable = tfhub_model_trainable.fit(\n",
        "    train_subset_hub,\n",
        "    epochs=3,\n",
        "    validation_data=val_subset_hub,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Compare performance\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "frozen_final_acc = hub_history_frozen.history['val_accuracy'][-1]\n",
        "trainable_final_acc = hub_history_trainable.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"Frozen embeddings - Final validation accuracy: {frozen_final_acc:.4f}\")\n",
        "print(f\"Trainable embeddings - Final validation accuracy: {trainable_final_acc:.4f}\")\n",
        "print(f\"Performance difference: {trainable_final_acc - frozen_final_acc:.4f}\")\n",
        "\n",
        "# Plot training comparison\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(hub_history_frozen.history['loss'], 'b-', label='Frozen - Train')\n",
        "plt.plot(hub_history_frozen.history['val_loss'], 'b--', label='Frozen - Val')\n",
        "plt.plot(hub_history_trainable.history['loss'], 'r-', label='Trainable - Train')\n",
        "plt.plot(hub_history_trainable.history['val_loss'], 'r--', label='Trainable - Val')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(hub_history_frozen.history['accuracy'], 'b-', label='Frozen - Train')\n",
        "plt.plot(hub_history_frozen.history['val_accuracy'], 'b--', label='Frozen - Val')\n",
        "plt.plot(hub_history_trainable.history['accuracy'], 'r-', label='Trainable - Train')\n",
        "plt.plot(hub_history_trainable.history['val_accuracy'], 'r--', label='Trainable - Val')\n",
        "plt.title('Training Accuracy Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Test with custom examples\n",
        "custom_texts = tf.constant([\n",
        "    \"This movie is absolutely fantastic and amazing!\",\n",
        "    \"Terrible movie, waste of time and money.\",\n",
        "    \"The film was okay, nothing special.\",\n",
        "    \"Incredible acting and stunning cinematography!\"\n",
        "])\n",
        "\n",
        "print(\"\\nTesting TensorFlow Hub models with custom examples:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "frozen_preds = tfhub_model_frozen.predict(custom_texts, verbose=0)\n",
        "trainable_preds = tfhub_model_trainable.predict(custom_texts, verbose=0)\n",
        "\n",
        "for i, text in enumerate(custom_texts.numpy()):\n",
        "    text_str = text.decode('utf-8')\n",
        "    frozen_pred = frozen_preds[i][0]\n",
        "    trainable_pred = trainable_preds[i][0]\n",
        "\n",
        "    print(f\"\\nText: {text_str}\")\n",
        "    print(f\"Frozen model: {frozen_pred:.3f} ({'Positive' if frozen_pred > 0.5 else 'Negative'})\")\n",
        "    print(f\"Trainable model: {trainable_pred:.3f} ({'Positive' if trainable_pred > 0.5 else 'Negative'})\")\n",
        "\n",
        "# Analysis and recommendations\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PRETRAINED EMBEDDINGS ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nAdvantages of TensorFlow Hub approach:\")\n",
        "print(\"• No text preprocessing required\")\n",
        "print(\"• Sentence-level embeddings capture context\")\n",
        "print(\"• Trained on massive corpus (7B words)\")\n",
        "print(\"• Much faster training (fewer parameters to learn)\")\n",
        "print(\"• Good baseline performance with minimal effort\")\n",
        "\n",
        "print(\"\\nWhen to use frozen vs trainable embeddings:\")\n",
        "print(\"• Frozen: Small datasets, limited compute, quick prototyping\")\n",
        "print(\"• Trainable: Large datasets, domain-specific data, best performance\")\n",
        "\n",
        "print(\"\\nLimitations:\")\n",
        "print(\"• Fixed embedding dimension (50D may be limiting)\")\n",
        "print(\"• Average pooling loses word order information\")\n",
        "print(\"• Less control over text preprocessing\")\n",
        "print(\"• Dependency on external service\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Implementing Pretrained Embeddings with TensorFlow Hub...\n",
            "Using TensorFlow Hub module: https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\n",
            "This module:\n",
            "• Takes text strings as input\n",
            "• Outputs 50-dimensional sentence embeddings\n",
            "• Trained on Google News 7B corpus\n",
            "• Computes mean of word embeddings with length normalization\n",
            "\n",
            "TensorFlow Hub Model Architecture (Frozen Embeddings):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_18\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_18\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_inputs (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m)                 │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ tf_predict_wrapper_2            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTFPredictWrapper\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m6,528\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ tf_predict_wrapper_2            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TFPredictWrapper</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,528</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,657\u001b[0m (26.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,657</span> (26.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,657\u001b[0m (26.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,657</span> (26.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TensorFlow Hub Model Architecture (Trainable Embeddings):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_19\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_19\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_inputs (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m)                 │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ tf_predict_wrapper_3            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTFPredictWrapper\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m6,528\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ tf_predict_wrapper_3            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TFPredictWrapper</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,528</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,657\u001b[0m (26.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,657</span> (26.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,657\u001b[0m (26.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,657</span> (26.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameter Comparison:\n",
            "Frozen embeddings: 6,657 trainable parameters\n",
            "Trainable embeddings: 6,657 trainable parameters\n",
            "Difference: 0 parameters\n",
            "\n",
            "Preparing data for TensorFlow Hub model...\n",
            "\n",
            "Testing TensorFlow Hub model...\n",
            "Sample input shape: (32,)\n",
            "Sample input type: <dtype: 'string'>\n",
            "First text: b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. \"...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipython-input-33-2635767321.py\", line 116, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 562, in predict\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 259, in one_step_on_data_distributed\n\nTrying to access resource hash_table_<google3.third_party.tensorflow.python.training.tracking.tracking.TrackableAsset object at 0x5ff08072f850>_-2_-1_load_1216429_1216430 located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0\n Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_data_distributed_1217032]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-33-2635767321.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# Test prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfhub_model_frozen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Predictions shape: {predictions.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sample predictions: {predictions.flatten()[:3]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipython-input-33-2635767321.py\", line 116, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 562, in predict\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 259, in one_step_on_data_distributed\n\nTrying to access resource hash_table_<google3.third_party.tensorflow.python.training.tracking.tracking.TrackableAsset object at 0x5ff08072f850>_-2_-1_load_1216429_1216430 located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0\n Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_data_distributed_1217032]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "encoder_decoder_theory"
      },
      "source": [
        "## Part 3: Encoder-Decoder Networks for Neural Machine Translation\n",
        "\n",
        "### Sequence-to-Sequence Learning Theory\n",
        "\n",
        "**Problem Formulation:**\n",
        "Given input sequence $X = [x_1, x_2, ..., x_T]$, generate output sequence $Y = [y_1, y_2, ..., y_{T'}]$ where $T \\neq T'$ generally.\n",
        "\n",
        "**Examples:**\n",
        "- **Machine Translation**: English → French\n",
        "- **Summarization**: Long text → Summary\n",
        "- **Question Answering**: Question + Context → Answer\n",
        "- **Dialogue**: User input → Bot response\n",
        "\n",
        "### Encoder-Decoder Architecture\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "**Encoder:**\n",
        "$$\\mathbf{h}_t^{(e)} = f_{\\text{enc}}(x_t, \\mathbf{h}_{t-1}^{(e)})$$\n",
        "$$\\mathbf{c} = \\mathbf{h}_T^{(e)} \\text{ (context vector)}$$\n",
        "\n",
        "**Decoder:**\n",
        "$$\\mathbf{h}_t^{(d)} = f_{\\text{dec}}(y_{t-1}, \\mathbf{h}_{t-1}^{(d)}, \\mathbf{c})$$\n",
        "$$P(y_t | y_1, ..., y_{t-1}, X) = \\text{softmax}(\\mathbf{W} \\mathbf{h}_t^{(d)} + \\mathbf{b})$$\n",
        "\n",
        "**Key Properties:**\n",
        "1. **Variable length input/output**\n",
        "2. **Information bottleneck** through context vector $\\mathbf{c}$\n",
        "3. **Autoregressive generation**: $y_t$ depends on $y_1, ..., y_{t-1}$\n",
        "\n",
        "### Neural Machine Translation (NMT)\n",
        "\n",
        "**Training Process:**\n",
        "1. **Teacher Forcing**: Use ground truth previous token as input\n",
        "2. **Loss Function**: Cross-entropy over target vocabulary\n",
        "3. **Gradient Flow**: Through both encoder and decoder\n",
        "\n",
        "**Inference Process:**\n",
        "1. **Autoregressive Generation**: Use model's previous output\n",
        "2. **Beam Search**: Keep multiple hypotheses\n",
        "3. **Length Normalization**: Prevent bias toward shorter sequences\n",
        "\n",
        "### Key Challenges\n",
        "\n",
        "**1. Information Bottleneck:**\n",
        "Single context vector $\\mathbf{c}$ must encode entire source sequence.\n",
        "\n",
        "**2. Long Sequences:**\n",
        "RNN limitations become apparent with very long sequences.\n",
        "\n",
        "**3. Exposure Bias:**\n",
        "Training with teacher forcing vs. inference with model predictions.\n",
        "\n",
        "**4. Unknown Words:**\n",
        "Handling out-of-vocabulary words in both source and target.\n",
        "\n",
        "### Implementation Considerations\n",
        "\n",
        "**Input Reversal:**\n",
        "\"I drink milk\" → \"milk drink I\" (encoder input)\n",
        "- Reduces distance between corresponding source/target words\n",
        "- Empirically improves performance\n",
        "\n",
        "**Special Tokens:**\n",
        "- `<SOS>`: Start of sequence\n",
        "- `<EOS>`: End of sequence  \n",
        "- `<UNK>`: Unknown word\n",
        "- `<PAD>`: Padding\n",
        "\n",
        "**Bucketing Strategy:**\n",
        "Group sequences by similar lengths to minimize padding:\n",
        "$$\\text{Bucket}_i = \\{(x, y) : L_i \\leq |x|, |y| \\leq L_{i+1}\\}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "encoder_decoder_implementation",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "ee753763-5269-492f-81b5-cfe6a9a97bbc"
      },
      "source": [
        "# Encoder-Decoder Implementation for Neural Machine Translation\n",
        "# This demonstrates the complete seq2seq architecture from the book\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "print(\"Implementing Encoder-Decoder Architecture for Neural Machine Translation...\")\n",
        "\n",
        "# Simulate a simple translation dataset (English to \"Simplified English\")\n",
        "# In practice, you would use a real parallel corpus\n",
        "def create_simple_translation_data():\n",
        "    \"\"\"\n",
        "    Create a simple translation dataset for demonstration.\n",
        "    English -> Simplified English (reversed word order)\n",
        "    \"\"\"\n",
        "    english_sentences = [\n",
        "        \"I love machine learning\",\n",
        "        \"The cat sits on the mat\",\n",
        "        \"Python is a great programming language\",\n",
        "        \"Deep learning models are powerful\",\n",
        "        \"Natural language processing is fascinating\",\n",
        "        \"Transformers changed everything in NLP\",\n",
        "        \"Attention mechanisms are very effective\",\n",
        "        \"Machine translation has improved significantly\",\n",
        "        \"Large language models are impressive\",\n",
        "        \"Artificial intelligence will change the world\"\n",
        "    ]\n",
        "\n",
        "    # Create \"simplified\" target by reversing word order\n",
        "    simplified_sentences = []\n",
        "    for sent in english_sentences:\n",
        "        words = sent.lower().split()\n",
        "        reversed_sent = \" \".join(reversed(words))\n",
        "        simplified_sentences.append(reversed_sent)\n",
        "\n",
        "    return english_sentences, simplified_sentences\n",
        "\n",
        "# Create translation data\n",
        "source_sentences, target_sentences = create_simple_translation_data()\n",
        "\n",
        "print(\"Sample Translation Pairs:\")\n",
        "for i in range(5):\n",
        "    print(f\"EN: {source_sentences[i]}\")\n",
        "    print(f\"Target: {target_sentences[i]}\")\n",
        "    print()\n",
        "\n",
        "# Tokenization for both source and target\n",
        "def create_tokenizers(source_texts, target_texts):\n",
        "    \"\"\"\n",
        "    Create tokenizers for source and target languages.\n",
        "\n",
        "    Args:\n",
        "        source_texts: List of source sentences\n",
        "        target_texts: List of target sentences\n",
        "\n",
        "    Returns:\n",
        "        source_tokenizer, target_tokenizer, vocab_sizes\n",
        "    \"\"\"\n",
        "    # Source tokenizer\n",
        "    source_tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "    source_tokenizer.fit_on_texts(source_texts)\n",
        "\n",
        "    # Target tokenizer (include special tokens)\n",
        "    # Add special tokens to target vocabulary\n",
        "    target_texts_with_tokens = [\"<start> \" + text + \" <end>\" for text in target_texts]\n",
        "    target_tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "    target_tokenizer.fit_on_texts(target_texts_with_tokens)\n",
        "\n",
        "    source_vocab_size = len(source_tokenizer.word_index) + 1\n",
        "    target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "    return source_tokenizer, target_tokenizer, source_vocab_size, target_vocab_size\n",
        "\n",
        "source_tokenizer, target_tokenizer, source_vocab_size, target_vocab_size = create_tokenizers(\n",
        "    source_sentences, target_sentences\n",
        ")\n",
        "\n",
        "print(f\"Vocabulary Sizes:\")\n",
        "print(f\"Source (English): {source_vocab_size} words\")\n",
        "print(f\"Target (Simplified): {target_vocab_size} words\")\n",
        "\n",
        "# Sequence processing\n",
        "def prepare_sequences(source_texts, target_texts, source_tokenizer, target_tokenizer, max_length=20):\n",
        "    \"\"\"\n",
        "    Convert texts to padded sequences.\n",
        "\n",
        "    Args:\n",
        "        source_texts: Source sentences\n",
        "        target_texts: Target sentences\n",
        "        source_tokenizer: Fitted source tokenizer\n",
        "        target_tokenizer: Fitted target tokenizer\n",
        "        max_length: Maximum sequence length\n",
        "\n",
        "    Returns:\n",
        "        Processed sequences for training\n",
        "    \"\"\"\n",
        "    # Encode source sequences\n",
        "    source_sequences = source_tokenizer.texts_to_sequences(source_texts)\n",
        "    source_sequences = keras.preprocessing.sequence.pad_sequences(\n",
        "        source_sequences, maxlen=max_length, padding='post'\n",
        "    )\n",
        "\n",
        "    # Encode target sequences with special tokens\n",
        "    target_texts_with_tokens = [\"<start> \" + text + \" <end>\" for text in target_texts]\n",
        "    target_sequences = target_tokenizer.texts_to_sequences(target_texts_with_tokens)\n",
        "    target_sequences = keras.preprocessing.sequence.pad_sequences(\n",
        "        target_sequences, maxlen=max_length, padding='post'\n",
        "    )\n",
        "\n",
        "    # For training: input is target[:-1], output is target[1:]\n",
        "    decoder_input = target_sequences[:, :-1]\n",
        "    decoder_output = target_sequences[:, 1:]\n",
        "\n",
        "    return source_sequences, decoder_input, decoder_output\n",
        "\n",
        "encoder_input, decoder_input, decoder_output = prepare_sequences(\n",
        "    source_sentences, target_sentences, source_tokenizer, target_tokenizer\n",
        ")\n",
        "\n",
        "print(f\"\\nSequence Shapes:\")\n",
        "print(f\"Encoder input: {encoder_input.shape}\")\n",
        "print(f\"Decoder input: {decoder_input.shape}\")\n",
        "print(f\"Decoder output: {decoder_output.shape}\")\n",
        "\n",
        "# Show example sequences\n",
        "print(f\"\\nExample Sequences (first sample):\")\n",
        "print(f\"Source text: {source_sentences[0]}\")\n",
        "print(f\"Target text: {target_sentences[0]}\")\n",
        "print(f\"Encoder input: {encoder_input[0]}\")\n",
        "print(f\"Decoder input: {decoder_input[0]}\")\n",
        "print(f\"Decoder output: {decoder_output[0]}\")\n",
        "\n",
        "# Decode sequences back to text for verification\n",
        "def decode_sequence(tokenizer, sequence):\n",
        "    \"\"\"Decode integer sequence back to text.\"\"\"\n",
        "    reverse_word_map = {v: k for k, v in tokenizer.word_index.items()}\n",
        "    return ' '.join([reverse_word_map.get(i, '<UNK>') for i in sequence if i > 0])\n",
        "\n",
        "print(f\"\\nDecoded Sequences (verification):\")\n",
        "print(f\"Encoder input decoded: {decode_sequence(source_tokenizer, encoder_input[0])}\")\n",
        "print(f\"Decoder input decoded: {decode_sequence(target_tokenizer, decoder_input[0])}\")\n",
        "print(f\"Decoder output decoded: {decode_sequence(target_tokenizer, decoder_output[0])}\")\n",
        "\n",
        "# Build Encoder-Decoder Model using TensorFlow Addons\n",
        "def create_encoder_decoder_model(source_vocab_size, target_vocab_size,\n",
        "                                embedding_dim=256, units=512):\n",
        "    \"\"\"\n",
        "    Create an Encoder-Decoder model for machine translation.\n",
        "\n",
        "    Args:\n",
        "        source_vocab_size: Size of source vocabulary\n",
        "        target_vocab_size: Size of target vocabulary\n",
        "        embedding_dim: Dimension of embeddings\n",
        "        units: Number of RNN units\n",
        "\n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    # Input layers\n",
        "    encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32, name='encoder_inputs')\n",
        "    decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32, name='decoder_inputs')\n",
        "    sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32, name='sequence_lengths')\n",
        "\n",
        "    # Shared embedding layer\n",
        "    # In practice, source and target might use different embeddings\n",
        "    encoder_embedding = keras.layers.Embedding(source_vocab_size, embedding_dim, name='encoder_embedding')\n",
        "    decoder_embedding = keras.layers.Embedding(target_vocab_size, embedding_dim, name='decoder_embedding')\n",
        "\n",
        "    # Embed inputs\n",
        "    encoder_embeddings = encoder_embedding(encoder_inputs)\n",
        "    decoder_embeddings = decoder_embedding(decoder_inputs)\n",
        "\n",
        "    # Encoder\n",
        "    encoder = keras.layers.LSTM(units, return_state=True, name='encoder')\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
        "    encoder_state = [state_h, state_c]\n",
        "\n",
        "    # Decoder using TensorFlow Addons\n",
        "    # TrainingSampler: uses ground truth at each step (teacher forcing)\n",
        "    sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "    # LSTM cell for decoder\n",
        "    decoder_cell = keras.layers.LSTMCell(units)\n",
        "\n",
        "    # Output projection layer\n",
        "    output_layer = keras.layers.Dense(target_vocab_size, name='output_projection')\n",
        "\n",
        "    # Basic decoder\n",
        "    decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
        "        decoder_cell, sampler, output_layer=output_layer\n",
        "    )\n",
        "\n",
        "    # Decoder forward pass\n",
        "    final_outputs, final_state, final_sequence_lengths = decoder(\n",
        "        decoder_embeddings,\n",
        "        initial_state=encoder_state,\n",
        "        sequence_length=sequence_lengths\n",
        "    )\n",
        "\n",
        "    # Get logits (before softmax)\n",
        "    logits = final_outputs.rnn_output\n",
        "\n",
        "    # Create model\n",
        "    model = keras.Model(\n",
        "        inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
        "        outputs=logits,\n",
        "        name='encoder_decoder'\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "seq2seq_model = create_encoder_decoder_model(\n",
        "    source_vocab_size, target_vocab_size,\n",
        "    embedding_dim=128, units=256\n",
        ")\n",
        "\n",
        "print(\"\\nEncoder-Decoder Model Architecture:\")\n",
        "seq2seq_model.summary()\n",
        "\n",
        "# Compile model\n",
        "seq2seq_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Prepare training data\n",
        "# Sequence lengths for each sample (needed by TensorFlow Addons)\n",
        "sequence_lengths = np.array([len([w for w in seq if w > 0]) for seq in decoder_input])\n",
        "\n",
        "print(f\"\\nTraining Data Preparation:\")\n",
        "print(f\"Encoder input shape: {encoder_input.shape}\")\n",
        "print(f\"Decoder input shape: {decoder_input.shape}\")\n",
        "print(f\"Decoder output shape: {decoder_output.shape}\")\n",
        "print(f\"Sequence lengths shape: {sequence_lengths.shape}\")\n",
        "print(f\"Sample sequence lengths: {sequence_lengths[:5]}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.src.engine'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-34-2829474503.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This demonstrates the complete seq2seq architecture from the book\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Implementing Encoder-Decoder Architecture for Neural Machine Translation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Local project imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/activations/gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# New versions of Keras require importing from `keras.src` when\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# importing internal symbols.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.src.engine'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beam_search_theory"
      },
      "source": [
        "### Beam Search Theory and Implementation\n",
        "\n",
        "#### The Problem with Greedy Decoding\n",
        "\n",
        "**Greedy Decoding:**\n",
        "$$y_t = \\arg\\max_{y} P(y | y_1, ..., y_{t-1}, X)$$\n",
        "\n",
        "**Problem:** Locally optimal choices may lead to globally suboptimal sequences.\n",
        "\n",
        "**Example:**\n",
        "- Step 1: \"How\" (75%), \"What\" (20%), \"When\" (5%)\n",
        "- Step 2 after \"How\": \"will\" (40%), \"are\" (35%), \"do\" (25%)\n",
        "- Step 3 after \"How are\": \"you\" (90%)\n",
        "\n",
        "Greedy might choose \"How will...\" but \"How are you?\" is better overall.\n",
        "\n",
        "#### Beam Search Algorithm\n",
        "\n",
        "**Concept:** Maintain top-$k$ partial sequences at each step.\n",
        "\n",
        "**Algorithm:**\n",
        "1. **Initialize**: $\\text{beams} = [\\{\\langle \\text{SOS} \\rangle\\}]$\n",
        "2. **For each time step**:\n",
        "   - **Expand**: Generate candidates for each beam\n",
        "   - **Score**: Compute sequence probabilities\n",
        "   - **Prune**: Keep top-$k$ sequences\n",
        "3. **Terminate**: When all beams end with `<EOS>`\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "At step $t$, for beam $b$ with sequence $y_1^{(b)}, ..., y_{t-1}^{(b)}$:\n",
        "\n",
        "$$\\text{Score}(y_1^{(b)}, ..., y_t^{(b)}) = \\sum_{i=1}^t \\log P(y_i^{(b)} | y_1^{(b)}, ..., y_{i-1}^{(b)}, X)$$\n",
        "\n",
        "**Length Normalization:**\n",
        "To prevent bias toward shorter sequences:\n",
        "$$\\text{Score}_{\\text{norm}} = \\frac{\\text{Score}}{|Y|^\\alpha}$$\n",
        "\n",
        "Where $\\alpha \\in [0, 1]$ controls the strength of normalization.\n",
        "\n",
        "#### Beam Search with TensorFlow Addons\n",
        "\n",
        "**Components:**\n",
        "1. **BeamSearchDecoder**: Main search algorithm\n",
        "2. **tile_batch**: Replicate encoder states for multiple beams\n",
        "3. **start_tokens** and **end_token**: Special tokens for generation\n",
        "\n",
        "**Computational Complexity:**\n",
        "- **Time**: $O(T \\times k \\times V)$ where $T$ = sequence length, $k$ = beam width, $V$ = vocabulary size\n",
        "- **Space**: $O(k \\times T)$ for storing beam sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beam_search_implementation"
      },
      "source": [
        "# Beam Search Implementation with TensorFlow Addons\n",
        "# This demonstrates advanced decoding strategies for sequence generation\n",
        "\n",
        "def create_inference_model(trained_model, source_vocab_size, target_vocab_size,\n",
        "                          embedding_dim=128, units=256, beam_width=3):\n",
        "    \"\"\"\n",
        "    Create an inference model with beam search capability.\n",
        "\n",
        "    Args:\n",
        "        trained_model: Trained encoder-decoder model\n",
        "        source_vocab_size: Source vocabulary size\n",
        "        target_vocab_size: Target vocabulary size\n",
        "        embedding_dim: Embedding dimension\n",
        "        units: RNN units\n",
        "        beam_width: Width of beam search\n",
        "\n",
        "    Returns:\n",
        "        Inference model with beam search\n",
        "    \"\"\"\n",
        "    # Input layers\n",
        "    encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "\n",
        "    # Get start and end token IDs\n",
        "    start_token_id = target_tokenizer.word_index['<start>']\n",
        "    end_token_id = target_tokenizer.word_index['<end>']\n",
        "\n",
        "    # Encoder (reuse from training model)\n",
        "    encoder_embedding = keras.layers.Embedding(source_vocab_size, embedding_dim)\n",
        "    encoder_embeddings = encoder_embedding(encoder_inputs)\n",
        "\n",
        "    encoder_lstm = keras.layers.LSTM(units, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embeddings)\n",
        "    encoder_state = [state_h, state_c]\n",
        "\n",
        "    # Beam search decoder\n",
        "    decoder_embedding = keras.layers.Embedding(target_vocab_size, embedding_dim)\n",
        "    decoder_cell = keras.layers.LSTMCell(units)\n",
        "    output_layer = keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    # Create beam search decoder\n",
        "    beam_decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(\n",
        "        cell=decoder_cell,\n",
        "        beam_width=beam_width,\n",
        "        output_layer=output_layer,\n",
        "        length_penalty_weight=0.6  # Length normalization\n",
        "    )\n",
        "\n",
        "    # Tile encoder states for beam search\n",
        "    # Each beam needs its own copy of encoder state\n",
        "    decoder_initial_state = tfa.seq2seq.beam_search_decoder.tile_batch(\n",
        "        encoder_state, multiplier=beam_width\n",
        "    )\n",
        "\n",
        "    # Start tokens for all beams\n",
        "    batch_size = tf.shape(encoder_inputs)[0]\n",
        "    start_tokens = tf.fill([batch_size], start_token_id)\n",
        "\n",
        "    # Maximum decoding length\n",
        "    max_length = 20\n",
        "\n",
        "    # Beam search inference\n",
        "    outputs, _, _ = beam_decoder(\n",
        "        decoder_embedding,\n",
        "        start_tokens=start_tokens,\n",
        "        end_token=end_token_id,\n",
        "        initial_state=decoder_initial_state,\n",
        "        maximum_iterations=max_length\n",
        "    )\n",
        "\n",
        "    # Create inference model\n",
        "    inference_model = keras.Model(\n",
        "        inputs=encoder_inputs,\n",
        "        outputs=outputs.predicted_ids,  # Shape: [batch, beam_width, max_length]\n",
        "        name='beam_search_inference'\n",
        "    )\n",
        "\n",
        "    return inference_model, start_token_id, end_token_id\n",
        "\n",
        "# Demonstrate beam search concepts with a simple implementation\n",
        "class SimpleBeamSearch:\n",
        "    \"\"\"\n",
        "    Educational implementation of beam search for understanding.\n",
        "    Not optimized for performance - for demonstration only.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, beam_width=3, max_length=20):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.beam_width = beam_width\n",
        "        self.max_length = max_length\n",
        "        self.start_token = tokenizer.word_index.get('<start>', 1)\n",
        "        self.end_token = tokenizer.word_index.get('<end>', 2)\n",
        "\n",
        "    def search(self, encoder_input):\n",
        "        \"\"\"\n",
        "        Perform beam search for a single input sequence.\n",
        "\n",
        "        Args:\n",
        "            encoder_input: Encoded source sequence\n",
        "\n",
        "        Returns:\n",
        "            List of (sequence, score) tuples\n",
        "        \"\"\"\n",
        "        # Initialize beams with start token\n",
        "        beams = [([self.start_token], 0.0)]  # (sequence, log_probability)\n",
        "\n",
        "        for step in range(self.max_length):\n",
        "            candidates = []\n",
        "\n",
        "            for sequence, score in beams:\n",
        "                if sequence[-1] == self.end_token:\n",
        "                    # Beam already finished\n",
        "                    candidates.append((sequence, score))\n",
        "                    continue\n",
        "\n",
        "                # Get next token probabilities\n",
        "                # Note: This is a simplified version - real implementation\n",
        "                # would use the actual model prediction\n",
        "                next_probs = self._get_next_probabilities(encoder_input, sequence)\n",
        "\n",
        "                # Add top candidates\n",
        "                for token_id, prob in next_probs:\n",
        "                    new_sequence = sequence + [token_id]\n",
        "                    new_score = score + np.log(prob + 1e-8)  # Add small epsilon\n",
        "                    candidates.append((new_sequence, new_score))\n",
        "\n",
        "            # Keep top beam_width candidates\n",
        "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "            beams = candidates[:self.beam_width]\n",
        "\n",
        "            # Check if all beams are finished\n",
        "            if all(seq[-1] == self.end_token for seq, _ in beams):\n",
        "                break\n",
        "\n",
        "        return beams\n",
        "\n",
        "    def _get_next_probabilities(self, encoder_input, current_sequence):\n",
        "        \"\"\"\n",
        "        Simplified probability calculation for demonstration.\n",
        "        In practice, this would use the actual model.\n",
        "        \"\"\"\n",
        "        # Simulate next token probabilities\n",
        "        vocab_size = len(self.tokenizer.word_index) + 1\n",
        "\n",
        "        # Create mock probabilities (in real implementation, use model.predict)\n",
        "        probs = np.random.dirichlet(np.ones(vocab_size))\n",
        "\n",
        "        # Return top 5 candidates\n",
        "        top_indices = np.argsort(probs)[-5:]\n",
        "        return [(idx, probs[idx]) for idx in reversed(top_indices)]\n",
        "\n",
        "# Demonstrate beam search concepts\n",
        "print(\"Beam Search Demonstration:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create simple beam search instance\n",
        "beam_searcher = SimpleBeamSearch(\n",
        "    model=None,  # Simplified for demonstration\n",
        "    tokenizer=target_tokenizer,\n",
        "    beam_width=3,\n",
        "    max_length=10\n",
        ")\n",
        "\n",
        "print(f\"Beam Search Configuration:\")\n",
        "print(f\"Beam width: {beam_searcher.beam_width}\")\n",
        "print(f\"Max length: {beam_searcher.max_length}\")\n",
        "print(f\"Start token ID: {beam_searcher.start_token}\")\n",
        "print(f\"End token ID: {beam_searcher.end_token}\")\n",
        "\n",
        "# Simulate beam search process\n",
        "print(f\"\\nSimulated Beam Search Process:\")\n",
        "fake_encoder_input = np.array([[1, 2, 3, 4, 0]])  # Mock input\n",
        "results = beam_searcher.search(fake_encoder_input)\n",
        "\n",
        "print(f\"\\nBeam Search Results:\")\n",
        "for i, (sequence, score) in enumerate(results):\n",
        "    decoded = decode_sequence(target_tokenizer, sequence)\n",
        "    print(f\"Beam {i+1}: {decoded} (score: {score:.3f})\")\n",
        "\n",
        "# Compare different beam widths\n",
        "print(f\"\\nBeam Width Comparison:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "beam_widths = [1, 3, 5, 10]\n",
        "for width in beam_widths:\n",
        "    searcher = SimpleBeamSearch(\n",
        "        model=None,\n",
        "        tokenizer=target_tokenizer,\n",
        "        beam_width=width,\n",
        "        max_length=8\n",
        "    )\n",
        "\n",
        "    # Computational complexity analysis\n",
        "    vocab_size = len(target_tokenizer.word_index) + 1\n",
        "    max_expansions = width * vocab_size  # Expansions per step\n",
        "    total_computations = max_expansions * searcher.max_length\n",
        "\n",
        "    print(f\"Beam width {width:2d}: ~{total_computations:,} computations\")\n",
        "\n",
        "# Advantages and disadvantages analysis\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"BEAM SEARCH ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nAdvantages:\")\n",
        "print(f\"• Explores multiple hypotheses simultaneously\")\n",
        "print(f\"• Often finds better solutions than greedy search\")\n",
        "print(f\"• Configurable beam width for quality/speed trade-off\")\n",
        "print(f\"• Length normalization prevents short sequence bias\")\n",
        "\n",
        "print(f\"\\nDisadvantages:\")\n",
        "print(f\"• Increased computational cost (k times more expensive)\")\n",
        "print(f\"• Still not guaranteed to find global optimum\")\n",
        "print(f\"• Memory requirements scale with beam width\")\n",
        "print(f\"• May produce repetitive or generic outputs\")\n",
        "\n",
        "print(f\"\\nHyperparameter Guidelines:\")\n",
        "print(f\"• Beam width 1: Greedy search (fastest)\")\n",
        "print(f\"• Beam width 3-5: Good balance for most tasks\")\n",
        "print(f\"• Beam width 10+: Diminishing returns, slower\")\n",
        "print(f\"• Length penalty 0.6-1.0: Prevents very short outputs\")\n",
        "\n",
        "print(f\"\\nAlternative Decoding Strategies:\")\n",
        "print(f\"• Top-k sampling: Sample from top-k most likely tokens\")\n",
        "print(f\"• Nucleus sampling: Sample from top-p probability mass\")\n",
        "print(f\"• Temperature scaling: Control randomness in sampling\")\n",
        "print(f\"• Diverse beam search: Encourage diversity between beams\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attention_mechanisms_theory"
      },
      "source": [
        "## Part 4: Attention Mechanisms - The Game Changer\n",
        "\n",
        "### The Information Bottleneck Problem\n",
        "\n",
        "**Traditional Encoder-Decoder Limitation:**\n",
        "Single context vector $\\mathbf{c}$ must encode entire source sequence:\n",
        "\n",
        "$$\\mathbf{c} = \\mathbf{h}_T^{(\\text{encoder})}$$\n",
        "\n",
        "**Problems:**\n",
        "1. **Information Loss**: Long sequences compressed into fixed-size vector\n",
        "2. **Vanishing Gradients**: Distant source words have weak influence\n",
        "3. **Performance Degradation**: Quality drops for sequences >30 words\n",
        "\n",
        "### Attention Mechanism Solution\n",
        "\n",
        "**Core Idea:** Allow decoder to selectively focus on different parts of input.\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "Instead of fixed context $\\mathbf{c}$, use dynamic context $\\mathbf{c}_t$ at each decoder step:\n",
        "\n",
        "$$\\mathbf{c}_t = \\sum_{i=1}^T \\alpha_{t,i} \\mathbf{h}_i^{(\\text{encoder})}$$\n",
        "\n",
        "Where $\\alpha_{t,i}$ is the attention weight for encoder state $i$ at decoder step $t$.\n",
        "\n",
        "### Bahdanau Attention (2014)\n",
        "\n",
        "**First successful attention mechanism:**\n",
        "\n",
        "**1. Energy Calculation:**\n",
        "$$e_{t,i} = v^T \\tanh(W_1 \\mathbf{h}_i^{(e)} + W_2 \\mathbf{s}_{t-1}^{(d)} + \\mathbf{b})$$\n",
        "\n",
        "**2. Attention Weights:**\n",
        "$$\\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{j=1}^T \\exp(e_{t,j})}$$\n",
        "\n",
        "**3. Context Vector:**\n",
        "$$\\mathbf{c}_t = \\sum_{i=1}^T \\alpha_{t,i} \\mathbf{h}_i^{(e)}$$\n",
        "\n",
        "**4. Decoder Update:**\n",
        "$$\\mathbf{s}_t^{(d)} = f(\\mathbf{s}_{t-1}^{(d)}, y_{t-1}, \\mathbf{c}_t)$$\n",
        "\n",
        "### Luong Attention (2015)\n",
        "\n",
        "**Simplified and more efficient:**\n",
        "\n",
        "**Three Variants:**\n",
        "\n",
        "**1. Dot Product:**\n",
        "$$\\text{score}(\\mathbf{h}_t^{(d)}, \\mathbf{h}_i^{(e)}) = \\mathbf{h}_t^{(d)^T} \\mathbf{h}_i^{(e)}$$\n",
        "\n",
        "**2. General:**\n",
        "$$\\text{score}(\\mathbf{h}_t^{(d)}, \\mathbf{h}_i^{(e)}) = \\mathbf{h}_t^{(d)^T} W \\mathbf{h}_i^{(e)}$$\n",
        "\n",
        "**3. Concatenation:**\n",
        "$$\\text{score}(\\mathbf{h}_t^{(d)}, \\mathbf{h}_i^{(e)}) = v^T \\tanh(W[\\mathbf{h}_t^{(d)}; \\mathbf{h}_i^{(e)}])$$\n",
        "\n",
        "**Key Differences from Bahdanau:**\n",
        "- Uses current decoder state $\\mathbf{h}_t^{(d)}$ instead of previous\n",
        "- Simpler computation (especially dot product)\n",
        "- Better empirical performance\n",
        "\n",
        "### Self-Attention\n",
        "\n",
        "**Revolutionary Concept:** Sequence attends to itself\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Where:\n",
        "- $Q$ (Queries): What we're looking for\n",
        "- $K$ (Keys): What we're looking in\n",
        "- $V$ (Values): What we retrieve\n",
        "- For self-attention: $Q = K = V$ (same sequence)\n",
        "\n",
        "### Computational Complexity\n",
        "\n",
        "**RNN Encoder-Decoder:** $O(T \\cdot d^2)$ sequential\n",
        "**With Attention:** $O(T^2 \\cdot d)$ parallelizable\n",
        "\n",
        "For long sequences ($T > d$), attention is more expensive but parallelizable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "attention_implementation"
      },
      "source": [
        "# Attention Mechanisms Implementation\n",
        "# This demonstrates various attention mechanisms from the book\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "print(\"Implementing Attention Mechanisms...\")\n",
        "\n",
        "class BahdanauAttention(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Bahdanau (Additive) Attention mechanism.\n",
        "    Original attention mechanism from 2014 paper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # W1 for encoder hidden states\n",
        "        self.W1 = self.add_weight(\n",
        "            name='W1',\n",
        "            shape=(input_shape[0][-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        # W2 for decoder hidden state\n",
        "        self.W2 = self.add_weight(\n",
        "            name='W2',\n",
        "            shape=(input_shape[1][-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        # V for final projection\n",
        "        self.V = self.add_weight(\n",
        "            name='V',\n",
        "            shape=(self.units, 1),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: [encoder_outputs, decoder_hidden_state]\n",
        "            encoder_outputs: [batch_size, seq_len, hidden_size]\n",
        "            decoder_hidden_state: [batch_size, hidden_size]\n",
        "\n",
        "        Returns:\n",
        "            context_vector: [batch_size, hidden_size]\n",
        "            attention_weights: [batch_size, seq_len]\n",
        "        \"\"\"\n",
        "        encoder_outputs, decoder_hidden = inputs\n",
        "\n",
        "        # Expand decoder hidden to match encoder sequence length\n",
        "        # [batch_size, hidden_size] -> [batch_size, seq_len, hidden_size]\n",
        "        decoder_hidden_expanded = tf.expand_dims(decoder_hidden, 1)\n",
        "        seq_len = tf.shape(encoder_outputs)[1]\n",
        "        decoder_hidden_repeated = tf.tile(decoder_hidden_expanded, [1, seq_len, 1])\n",
        "\n",
        "        # Calculate energy: e_ij = v^T * tanh(W1*h_i + W2*s_j)\n",
        "        # Shape: [batch_size, seq_len, units]\n",
        "        energy = tf.nn.tanh(\n",
        "            tf.matmul(encoder_outputs, self.W1) +\n",
        "            tf.matmul(decoder_hidden_repeated, self.W2)\n",
        "        )\n",
        "\n",
        "        # Project to scalar: [batch_size, seq_len, 1]\n",
        "        energy = tf.matmul(energy, self.V)\n",
        "\n",
        "        # Remove last dimension: [batch_size, seq_len]\n",
        "        energy = tf.squeeze(energy, axis=-1)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attention_weights = tf.nn.softmax(energy, axis=1)\n",
        "\n",
        "        # Calculate context vector\n",
        "        # [batch_size, seq_len, 1] * [batch_size, seq_len, hidden_size]\n",
        "        attention_weights_expanded = tf.expand_dims(attention_weights, 2)\n",
        "        context_vector = tf.reduce_sum(\n",
        "            attention_weights_expanded * encoder_outputs, axis=1\n",
        "        )\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "\n",
        "class LuongAttention(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Luong (Multiplicative) Attention mechanism.\n",
        "    More efficient than Bahdanau attention.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, score_type='general', **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.score_type = score_type  # 'dot', 'general', or 'concat'\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.score_type == 'general':\n",
        "            # Weight matrix for general scoring\n",
        "            self.W = self.add_weight(\n",
        "                name='attention_W',\n",
        "                shape=(input_shape[1][-1], input_shape[0][-1]),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True\n",
        "            )\n",
        "        elif self.score_type == 'concat':\n",
        "            # For concatenation scoring (similar to Bahdanau)\n",
        "            self.W_concat = self.add_weight(\n",
        "                name='W_concat',\n",
        "                shape=(input_shape[0][-1] + input_shape[1][-1], self.units),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True\n",
        "            )\n",
        "            self.v = self.add_weight(\n",
        "                name='v_concat',\n",
        "                shape=(self.units, 1),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True\n",
        "            )\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: [encoder_outputs, decoder_hidden_state]\n",
        "\n",
        "        Returns:\n",
        "            context_vector, attention_weights\n",
        "        \"\"\"\n",
        "        encoder_outputs, decoder_hidden = inputs\n",
        "\n",
        "        if self.score_type == 'dot':\n",
        "            # Dot product attention: h_t^T * h_s\n",
        "            # [batch_size, 1, hidden_size] * [batch_size, hidden_size, seq_len]\n",
        "            decoder_expanded = tf.expand_dims(decoder_hidden, 1)\n",
        "            scores = tf.matmul(decoder_expanded, encoder_outputs, transpose_b=True)\n",
        "            scores = tf.squeeze(scores, 1)  # [batch_size, seq_len]\n",
        "\n",
        "        elif self.score_type == 'general':\n",
        "            # General attention: h_t^T * W * h_s\n",
        "            decoder_projected = tf.matmul(decoder_hidden, self.W)\n",
        "            decoder_expanded = tf.expand_dims(decoder_projected, 1)\n",
        "            scores = tf.matmul(decoder_expanded, encoder_outputs, transpose_b=True)\n",
        "            scores = tf.squeeze(scores, 1)\n",
        "\n",
        "        elif self.score_type == 'concat':\n",
        "            # Concatenation attention (like Bahdanau)\n",
        "            seq_len = tf.shape(encoder_outputs)[1]\n",
        "            decoder_repeated = tf.tile(\n",
        "                tf.expand_dims(decoder_hidden, 1), [1, seq_len, 1]\n",
        "            )\n",
        "\n",
        "            # Concatenate decoder and encoder states\n",
        "            concat_states = tf.concat([decoder_repeated, encoder_outputs], axis=-1)\n",
        "\n",
        "            # Apply transformations\n",
        "            hidden = tf.nn.tanh(tf.matmul(concat_states, self.W_concat))\n",
        "            scores = tf.matmul(hidden, self.v)\n",
        "            scores = tf.squeeze(scores, -1)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
        "\n",
        "        # Calculate context vector\n",
        "        attention_expanded = tf.expand_dims(attention_weights, 2)\n",
        "        context_vector = tf.reduce_sum(\n",
        "            attention_expanded * encoder_outputs, axis=1\n",
        "        )\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention from 'Attention Is All You Need'.\n",
        "    Foundation of the Transformer architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: [queries, keys, values]\n",
        "            queries: [batch_size, seq_len_q, d_k]\n",
        "            keys: [batch_size, seq_len_k, d_k]\n",
        "            values: [batch_size, seq_len_v, d_v]\n",
        "            mask: Optional mask for attention weights\n",
        "\n",
        "        Returns:\n",
        "            output: [batch_size, seq_len_q, d_v]\n",
        "            attention_weights: [batch_size, seq_len_q, seq_len_k]\n",
        "        \"\"\"\n",
        "        queries, keys, values = inputs\n",
        "\n",
        "        # Get dimension for scaling\n",
        "        d_k = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "\n",
        "        # Calculate attention scores: Q * K^T / sqrt(d_k)\n",
        "        scores = tf.matmul(queries, keys, transpose_b=True)\n",
        "        scores = scores / tf.math.sqrt(d_k)\n",
        "\n",
        "        # Apply mask if provided (for causal attention)\n",
        "        if mask is not None:\n",
        "            scores += (mask * -1e9)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        output = tf.matmul(attention_weights, values)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "# Test attention mechanisms\n",
        "print(\"Testing Attention Mechanisms...\")\n",
        "\n",
        "# Create sample data\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "hidden_size = 64\n",
        "\n",
        "# Sample encoder outputs (source sequence representations)\n",
        "encoder_outputs = tf.random.normal([batch_size, seq_len, hidden_size])\n",
        "# Sample decoder hidden state (current decoder state)\n",
        "decoder_hidden = tf.random.normal([batch_size, hidden_size])\n",
        "\n",
        "print(f\"Test Data Shapes:\")\n",
        "print(f\"Encoder outputs: {encoder_outputs.shape}\")\n",
        "print(f\"Decoder hidden: {decoder_hidden.shape}\")\n",
        "\n",
        "# Test Bahdanau Attention\n",
        "print(\"\\nTesting Bahdanau Attention:\")\n",
        "bahdanau_attention = BahdanauAttention(units=32)\n",
        "context_b, weights_b = bahdanau_attention([encoder_outputs, decoder_hidden])\n",
        "print(f\"Context vector shape: {context_b.shape}\")\n",
        "print(f\"Attention weights shape: {weights_b.shape}\")\n",
        "print(f\"Attention weights sum: {tf.reduce_sum(weights_b, axis=1).numpy()}\")\n",
        "print(f\"Sample attention weights: {weights_b[0].numpy()}\")\n",
        "\n",
        "# Test Luong Attention variants\n",
        "print(\"\\nTesting Luong Attention Variants:\")\n",
        "\n",
        "for score_type in ['dot', 'general', 'concat']:\n",
        "    print(f\"\\n{score_type.upper()} Product Attention:\")\n",
        "    luong_attention = LuongAttention(units=32, score_type=score_type)\n",
        "    context_l, weights_l = luong_attention([encoder_outputs, decoder_hidden])\n",
        "    print(f\"Context vector shape: {context_l.shape}\")\n",
        "    print(f\"Attention weights shape: {weights_l.shape}\")\n",
        "    print(f\"Sample attention weights: {weights_l[0].numpy()}\")\n",
        "\n",
        "# Test Scaled Dot-Product Attention\n",
        "print(\"\\nTesting Scaled Dot-Product Attention:\")\n",
        "scaled_attention = ScaledDotProductAttention()\n",
        "\n",
        "# For self-attention, Q=K=V (same sequence)\n",
        "queries = keys = values = encoder_outputs\n",
        "output_s, weights_s = scaled_attention([queries, keys, values])\n",
        "print(f\"Output shape: {output_s.shape}\")\n",
        "print(f\"Attention weights shape: {weights_s.shape}\")\n",
        "print(f\"Sample attention matrix:\")\n",
        "print(weights_s[0].numpy())\n",
        "\n",
        "# Visualize attention patterns\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(15, 4))\n",
        "\n",
        "# Plot Bahdanau attention weights\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.bar(range(seq_len), weights_b[0].numpy())\n",
        "plt.title('Bahdanau Attention Weights')\n",
        "plt.xlabel('Source Position')\n",
        "plt.ylabel('Attention Weight')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot Luong attention weights\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.bar(range(seq_len), weights_l[0].numpy())\n",
        "plt.title('Luong Attention Weights')\n",
        "plt.xlabel('Source Position')\n",
        "plt.ylabel('Attention Weight')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot self-attention matrix\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.heatmap(weights_s[0].numpy(), annot=True, fmt='.3f', cmap='Blues')\n",
        "plt.title('Self-Attention Matrix')\n",
        "plt.xlabel('Key Position')\n",
        "plt.ylabel('Query Position')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Performance comparison\n",
        "import time\n",
        "\n",
        "def benchmark_attention(attention_layer, inputs, iterations=100):\n",
        "    \"\"\"Benchmark attention mechanism performance.\"\"\"\n",
        "    # Warm up\n",
        "    _ = attention_layer(inputs)\n",
        "\n",
        "    # Benchmark\n",
        "    start_time = time.time()\n",
        "    for _ in range(iterations):\n",
        "        _ = attention_layer(inputs)\n",
        "    end_time = time.time()\n",
        "\n",
        "    return (end_time - start_time) / iterations\n",
        "\n",
        "print(\"\\nPerformance Benchmark (average time per forward pass):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Benchmark encoder-decoder attention\n",
        "bahdanau_time = benchmark_attention(bahdanau_attention, [encoder_outputs, decoder_hidden])\n",
        "luong_time = benchmark_attention(\n",
        "    LuongAttention(32, 'dot'), [encoder_outputs, decoder_hidden]\n",
        ")\n",
        "\n",
        "print(f\"Bahdanau Attention: {bahdanau_time*1000:.3f} ms\")\n",
        "print(f\"Luong Attention (dot): {luong_time*1000:.3f} ms\")\n",
        "print(f\"Speedup: {bahdanau_time/luong_time:.2f}x\")\n",
        "\n",
        "# Benchmark self-attention\n",
        "self_attention_time = benchmark_attention(\n",
        "    scaled_attention, [queries, keys, values]\n",
        ")\n",
        "print(f\"Self-Attention: {self_attention_time*1000:.3f} ms\")\n",
        "\n",
        "# Complexity analysis\n",
        "print(f\"\\nComputational Complexity Analysis:\")\n",
        "print(f\"Sequence length: {seq_len}\")\n",
        "print(f\"Hidden dimension: {hidden_size}\")\n",
        "print(f\"\")\n",
        "print(f\"Bahdanau Attention:\")\n",
        "print(f\"  Time complexity: O(T * d^2) where T={seq_len}, d={hidden_size}\")\n",
        "print(f\"  Space complexity: O(T * d)\")\n",
        "print(f\"\")\n",
        "print(f\"Luong Attention (dot):\")\n",
        "print(f\"  Time complexity: O(T * d)\")\n",
        "print(f\"  Space complexity: O(T)\")\n",
        "print(f\"\")\n",
        "print(f\"Self-Attention:\")\n",
        "print(f\"  Time complexity: O(T^2 * d)\")\n",
        "print(f\"  Space complexity: O(T^2)\")\n",
        "print(f\"  Parallelizable: Yes (unlike RNNs)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "transformer_theory"
      },
      "source": [
        "## Part 5: The Transformer Architecture - \"Attention Is All You Need\"\n",
        "\n",
        "### Revolutionary Breakthrough (2017)\n",
        "\n",
        "The Transformer, introduced in the landmark paper \"Attention Is All You Need\" by Vaswani et al., revolutionized NLP by:\n",
        "1. **Eliminating RNNs entirely** - Pure attention-based architecture\n",
        "2. **Massive parallelization** - No sequential dependencies\n",
        "3. **Superior performance** - State-of-the-art results with less training time\n",
        "4. **Scalability** - Enables training of very large models\n",
        "\n",
        "### Core Architecture Components\n",
        "\n",
        "#### 1. Multi-Head Attention\n",
        "\n",
        "**Intuition:** Instead of single attention, use multiple \"attention heads\" to focus on different aspects.\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
        "\n",
        "Where each head is:\n",
        "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "\n",
        "**Benefits:**\n",
        "- **Different attention patterns**: Each head can specialize\n",
        "- **Richer representations**: Capture multiple types of relationships\n",
        "- **Parallel computation**: All heads computed simultaneously\n",
        "\n",
        "#### 2. Positional Encoding\n",
        "\n",
        "**Problem:** Attention mechanism has no notion of sequence order\n",
        "\n",
        "**Solution:** Add positional information to embeddings\n",
        "\n",
        "**Sinusoidal Encoding:**\n",
        "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "**Properties:**\n",
        "- **Deterministic**: Same position always gets same encoding\n",
        "- **Relative distances**: Model can learn relative positions\n",
        "- **Extrapolation**: Can handle longer sequences than seen in training\n",
        "\n",
        "#### 3. Layer Normalization and Residual Connections\n",
        "\n",
        "**Pre-Norm Architecture:**\n",
        "$$x' = x + \\text{Attention}(\\text{LayerNorm}(x))$$\n",
        "$$x'' = x' + \\text{FFN}(\\text{LayerNorm}(x'))$$\n",
        "\n",
        "**Benefits:**\n",
        "- **Gradient flow**: Residual connections help gradients flow\n",
        "- **Training stability**: Layer normalization stabilizes training\n",
        "- **Deep networks**: Enables training of very deep models\n",
        "\n",
        "#### 4. Feed-Forward Networks\n",
        "\n",
        "**Point-wise FFN:**\n",
        "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
        "\n",
        "**Properties:**\n",
        "- **Position-wise**: Applied to each position independently\n",
        "- **Large hidden dimension**: Typically 4× the model dimension\n",
        "- **Non-linearity**: Introduces non-linear transformations\n",
        "\n",
        "### Encoder-Decoder Structure\n",
        "\n",
        "**Encoder Stack:**\n",
        "- 6 identical layers\n",
        "- Each layer: Multi-Head Self-Attention + FFN\n",
        "- Residual connections and layer normalization\n",
        "\n",
        "**Decoder Stack:**\n",
        "- 6 identical layers  \n",
        "- Each layer: Masked Self-Attention + Encoder-Decoder Attention + FFN\n",
        "- Causal masking prevents looking at future tokens\n",
        "\n",
        "### Key Advantages\n",
        "\n",
        "**1. Parallelization:**\n",
        "- All positions processed simultaneously\n",
        "- Much faster training than RNNs\n",
        "- Better GPU utilization\n",
        "\n",
        "**2. Long-range dependencies:**\n",
        "- Direct connections between all positions\n",
        "- No vanishing gradient through long sequences\n",
        "- Better capture of long-term patterns\n",
        "\n",
        "**3. Interpretability:**\n",
        "- Attention weights show what model focuses on\n",
        "- Multiple heads capture different relationships\n",
        "- Better understanding of model behavior\n",
        "\n",
        "### Computational Complexity\n",
        "\n",
        "| Component | Complexity | Sequential Ops | Maximum Path Length |\n",
        "|-----------|------------|----------------|--------------------|\n",
        "| Self-Attention | $O(n^2 \\cdot d)$ | $O(1)$ | $O(1)$ |\n",
        "| Recurrent | $O(n \\cdot d^2)$ | $O(n)$ | $O(n)$ |\n",
        "| Convolutional | $O(k \\cdot n \\cdot d^2)$ | $O(1)$ | $O(\\log_k(n))$ |\n",
        "\n",
        "Where $n$ = sequence length, $d$ = representation dimension, $k$ = kernel size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "transformer_implementation",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "709c9bd8-1ddb-4723-c444-9269f683a49c"
      },
      "source": [
        "# Transformer Architecture Implementation\n",
        "# This implements the complete Transformer model from \"Attention Is All You Need\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import math\n",
        "\n",
        "print(\"Implementing Transformer Architecture...\")\n",
        "\n",
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention mechanism.\n",
        "    Core component of the Transformer architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.wq = keras.layers.Dense(d_model)\n",
        "        self.wk = keras.layers.Dense(d_model)\n",
        "        self.wv = keras.layers.Dense(d_model)\n",
        "\n",
        "        # Output projection\n",
        "        self.dense = keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # Linear transformations and split into heads\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attention_output, attention_weights = self.scaled_dot_product_attention(\n",
        "            q, k, v, mask\n",
        "        )\n",
        "\n",
        "        # Concatenate heads\n",
        "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention_output,\n",
        "                                    (batch_size, -1, self.d_model))\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
        "        \"\"\"Calculate the attention weights.\n",
        "        q, k, v must have matching leading dimensions.\n",
        "        k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "        The mask has different shapes depending on its type(padding or look ahead)\n",
        "        but it must be broadcastable for addition.\n",
        "\n",
        "        Args:\n",
        "            q: query shape == (..., seq_len_q, depth)\n",
        "            k: key shape == (..., seq_len_k, depth)\n",
        "            v: value shape == (..., seq_len_v, depth_v)\n",
        "            mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            output, attention_weights\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculate attention scores\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "        # Scale matmul_qk\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        # Add the mask to the scaled tensor.\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        # Softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "class PositionalEncoding(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Sinusoidal positional encoding as described in 'Attention Is All You Need'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, position, d_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position = position\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "        return pos * angle_rates\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
        "                                   np.arange(d_model)[np.newaxis, :],\n",
        "                                   d_model)\n",
        "\n",
        "        # Apply sin to even indices in the array; 2i\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "        # Apply cos to odd indices in the array; 2i+1\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        return x + self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    \"\"\"\n",
        "    Point-wise feed forward network.\n",
        "    Two linear transformations with a ReLU activation in between.\n",
        "    \"\"\"\n",
        "    return keras.Sequential([\n",
        "        keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])\n",
        "\n",
        "\n",
        "class EncoderLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Single encoder layer in the Transformer.\n",
        "    Contains multi-head attention and feed-forward network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        # Multi-head attention\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # Self-attention\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # Residual connection + layer norm\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection + layer norm\n",
        "\n",
        "        return out2\n",
        "\n",
        "\n",
        "class DecoderLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Single decoder layer in the Transformer.\n",
        "    Contains masked self-attention, encoder-decoder attention, and feed-forward network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "        self.dropout3 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "        # Masked self-attention\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        # Encoder-decoder attention\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask\n",
        "        )\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "\n",
        "class Encoder(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Complete Transformer encoder stack.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                          for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # Embedding + positional encoding\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # Pass through encoder layers\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "\n",
        "class Decoder(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Complete Transformer decoder stack.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                          for _ in range(num_layers)]\n",
        "        self.dropout = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        # Embedding + positional encoding\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # Pass through decoder layers\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                 look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        return x, attention_weights\n",
        "\n",
        "\n",
        "class Transformer(keras.Model):\n",
        "    \"\"\"\n",
        "    Complete Transformer model for sequence-to-sequence tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 target_vocab_size, pe_input, pe_target, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                             input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                             target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask=None,\n",
        "             look_ahead_mask=None, dec_padding_mask=None):\n",
        "\n",
        "        # Encoder\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        # Decoder\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask\n",
        "        )\n",
        "\n",
        "        # Final linear layer\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights\n",
        "\n",
        "\n",
        "# Test the Transformer implementation\n",
        "print(\"Testing Transformer Components...\")\n",
        "\n",
        "# Model hyperparameters\n",
        "num_layers = 2\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "dff = 512\n",
        "input_vocab_size = 1000\n",
        "target_vocab_size = 1000\n",
        "pe_input = 1000\n",
        "pe_target = 1000\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Create sample data\n",
        "sample_encoder_input = tf.random.uniform((2, 10), maxval=input_vocab_size, dtype=tf.int32)\n",
        "sample_decoder_input = tf.random.uniform((2, 8), maxval=target_vocab_size, dtype=tf.int32)\n",
        "\n",
        "print(f\"Model Configuration:\")\n",
        "print(f\"Number of layers: {num_layers}\")\n",
        "print(f\"Model dimension: {d_model}\")\n",
        "print(f\"Number of heads: {num_heads}\")\n",
        "print(f\"Feed-forward dimension: {dff}\")\n",
        "print(f\"Dropout rate: {dropout_rate}\")\n",
        "\n",
        "# Test individual components\n",
        "print(f\"\\nTesting individual components:\")\n",
        "\n",
        "# Test Multi-Head Attention\n",
        "mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "sample_mha_input = tf.random.uniform((2, 10, d_model))\n",
        "mha_output, mha_weights = mha(sample_mha_input, sample_mha_input, sample_mha_input)\n",
        "print(f\"Multi-Head Attention output shape: {mha_output.shape}\")\n",
        "print(f\"Attention weights shape: {mha_weights.shape}\")\n",
        "\n",
        "# Test Positional Encoding\n",
        "pos_encoding = PositionalEncoding(position=50, d_model=d_model)\n",
        "sample_pos_input = tf.random.uniform((2, 10, d_model))\n",
        "pos_output = pos_encoding(sample_pos_input)\n",
        "print(f\"Positional encoding output shape: {pos_output.shape}\")\n",
        "\n",
        "# Test complete Transformer\n",
        "print(f\"\\nTesting complete Transformer model:\")\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=input_vocab_size,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    pe_input=pe_input,\n",
        "    pe_target=pe_target,\n",
        "    rate=dropout_rate\n",
        ")\n",
        "\n",
        "# Forward pass\n",
        "temp_output, temp_attention = transformer(\n",
        "    sample_encoder_input,\n",
        "    sample_decoder_input,\n",
        "    training=False\n",
        ")\n",
        "\n",
        "print(f\"Transformer output shape: {temp_output.shape}\")\n",
        "print(f\"Number of attention weight tensors: {len(temp_attention)}\")\n",
        "\n",
        "# Model summary\n",
        "print(f\"\\nTransformer Model Summary:\")\n",
        "transformer.summary()\n",
        "\n",
        "# Parameter count\n",
        "total_params = sum([np.prod(var.get_shape().as_list()) for var in transformer.trainable_variables])\n",
        "print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
        "\n",
        "# Memory analysis\n",
        "print(f\"\\nMemory Analysis:\")\n",
        "param_memory = total_params * 4 / (1024**2)  # 4 bytes per float32\n",
        "print(f\"Parameter memory: {param_memory:.1f} MB\")\n",
        "\n",
        "# Computational complexity\n",
        "seq_len = 10\n",
        "attention_ops = num_layers * 2 * seq_len**2 * d_model  # Self-attention + encoder-decoder attention\n",
        "ffn_ops = num_layers * seq_len * d_model * dff\n",
        "total_ops = attention_ops + ffn_ops\n",
        "\n",
        "print(f\"\\nComputational Complexity (seq_len={seq_len}):\")\n",
        "print(f\"Attention operations: {attention_ops:,}\")\n",
        "print(f\"Feed-forward operations: {ffn_ops:,}\")\n",
        "print(f\"Total operations: {total_ops:,}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Implementing Transformer Architecture...\n",
            "Testing Transformer Components...\n",
            "Model Configuration:\n",
            "Number of layers: 2\n",
            "Model dimension: 128\n",
            "Number of heads: 8\n",
            "Feed-forward dimension: 512\n",
            "Dropout rate: 0.1\n",
            "\n",
            "Testing individual components:\n",
            "Multi-Head Attention output shape: (2, 10, 128)\n",
            "Attention weights shape: (2, 8, 10, 10)\n",
            "Positional encoding output shape: (2, 10, 128)\n",
            "\n",
            "Testing complete Transformer model:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Transformer.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: False (of type <class 'bool'>)\u001b[0m\n\nArguments received by Transformer.call():\n  • inp=tf.Tensor(shape=(2, 10), dtype=int32)\n  • tar=tf.Tensor(shape=(2, 8), dtype=int32)\n  • training=False\n  • enc_padding_mask=None\n  • look_ahead_mask=None\n  • dec_padding_mask=None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-35-4170928358.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m temp_output, temp_attention = transformer(\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0msample_encoder_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0msample_decoder_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-35-4170928358.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;31m# Decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Transformer.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: False (of type <class 'bool'>)\u001b[0m\n\nArguments received by Transformer.call():\n  • inp=tf.Tensor(shape=(2, 10), dtype=int32)\n  • tar=tf.Tensor(shape=(2, 8), dtype=int32)\n  • training=False\n  • enc_padding_mask=None\n  • look_ahead_mask=None\n  • dec_padding_mask=None"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mask_creation_theory"
      },
      "source": [
        "### Masking in Transformers\n",
        "\n",
        "#### Types of Masks\n",
        "\n",
        "**1. Padding Mask:**\n",
        "Prevents attention to padding tokens\n",
        "$$\\text{mask}_{\\text{pad}}[i,j] = \\begin{cases} 0 & \\text{if token}_j \\neq \\text{PAD} \\\\ -\\infty & \\text{if token}_j = \\text{PAD} \\end{cases}$$\n",
        "\n",
        "**2. Look-Ahead Mask (Causal Mask):**\n",
        "Prevents attention to future tokens during training\n",
        "$$\\text{mask}_{\\text{causal}}[i,j] = \\begin{cases} 0 & \\text{if } j \\leq i \\\\ -\\infty & \\text{if } j > i \\end{cases}$$\n",
        "\n",
        "**3. Combined Mask:**\n",
        "$$\\text{mask}_{\\text{combined}} = \\text{mask}_{\\text{pad}} + \\text{mask}_{\\text{causal}}$$\n",
        "\n",
        "#### Mathematical Impact\n",
        "\n",
        "**Attention with Mask:**\n",
        "$$\\text{Attention}(Q, K, V, \\text{mask}) = \\text{softmax}\\left(\\frac{QK^T + \\text{mask}}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "When mask value is $-\\infty$, softmax output becomes 0:\n",
        "$$\\lim_{x \\to -\\infty} \\frac{e^x}{\\sum_j e^{x_j}} = 0$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mask_creation"
      },
      "source": [
        "# Mask Creation for Transformers\n",
        "# This demonstrates how to create different types of masks used in Transformers\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    \"\"\"\n",
        "    Create padding mask to ignore padding tokens.\n",
        "\n",
        "    Args:\n",
        "        seq: Input sequence with padding tokens (typically 0)\n",
        "\n",
        "    Returns:\n",
        "        Mask tensor where 1 indicates padding positions\n",
        "    \"\"\"\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # Add extra dimensions for multi-head attention\n",
        "    # [batch_size, 1, 1, seq_len]\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    \"\"\"\n",
        "    Create look-ahead mask to prevent attention to future tokens.\n",
        "\n",
        "    Args:\n",
        "        size: Sequence length\n",
        "\n",
        "    Returns:\n",
        "        Upper triangular matrix with 1s above diagonal\n",
        "    \"\"\"\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # [seq_len, seq_len]\n",
        "\n",
        "\n",
        "def create_masks(inp, tar):\n",
        "    \"\"\"\n",
        "    Create all masks needed for training.\n",
        "\n",
        "    Args:\n",
        "        inp: Encoder input sequence\n",
        "        tar: Decoder input sequence\n",
        "\n",
        "    Returns:\n",
        "        enc_padding_mask: Encoder padding mask\n",
        "        combined_mask: Combined look-ahead and padding mask for decoder\n",
        "        dec_padding_mask: Padding mask for encoder-decoder attention\n",
        "    \"\"\"\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder\n",
        "    # This padding mask is used to mask the encoder outputs\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder\n",
        "    # It is used to pad and mask future tokens in the input received by the decoder\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "\n",
        "# Demonstrate mask creation\n",
        "print(\"Demonstrating Transformer Mask Creation...\")\n",
        "\n",
        "# Sample sequences with padding\n",
        "sample_inp = tf.constant([[1, 2, 3, 4, 0, 0],  # Sequence with 2 padding tokens\n",
        "                         [5, 6, 7, 0, 0, 0]])  # Sequence with 3 padding tokens\n",
        "\n",
        "sample_tar = tf.constant([[1, 2, 3, 0],        # Target with 1 padding token\n",
        "                         [4, 5, 0, 0]])        # Target with 2 padding tokens\n",
        "\n",
        "print(f\"Input sequences shape: {sample_inp.shape}\")\n",
        "print(f\"Target sequences shape: {sample_tar.shape}\")\n",
        "print(f\"Input sequences:\")\n",
        "print(sample_inp.numpy())\n",
        "print(f\"Target sequences:\")\n",
        "print(sample_tar.numpy())\n",
        "\n",
        "# Create masks\n",
        "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(sample_inp, sample_tar)\n",
        "\n",
        "print(f\"\\nEncoder padding mask shape: {enc_padding_mask.shape}\")\n",
        "print(f\"Combined mask shape: {combined_mask.shape}\")\n",
        "print(f\"Decoder padding mask shape: {dec_padding_mask.shape}\")\n",
        "\n",
        "# Visualize masks\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Encoder padding mask\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.imshow(enc_padding_mask[0, 0, 0, :].numpy().reshape(1, -1), cmap='Blues')\n",
        "plt.title('Encoder Padding Mask\\n(1st sequence)')\n",
        "plt.xlabel('Token Position')\n",
        "plt.yticks([])\n",
        "for i, val in enumerate(enc_padding_mask[0, 0, 0, :].numpy()):\n",
        "    plt.text(i, 0, f'{val:.0f}', ha='center', va='center')\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.imshow(enc_padding_mask[1, 0, 0, :].numpy().reshape(1, -1), cmap='Blues')\n",
        "plt.title('Encoder Padding Mask\\n(2nd sequence)')\n",
        "plt.xlabel('Token Position')\n",
        "plt.yticks([])\n",
        "for i, val in enumerate(enc_padding_mask[1, 0, 0, :].numpy()):\n",
        "    plt.text(i, 0, f'{val:.0f}', ha='center', va='center')\n",
        "\n",
        "# Look-ahead mask\n",
        "plt.subplot(2, 3, 3)\n",
        "look_ahead = create_look_ahead_mask(4)\n",
        "plt.imshow(look_ahead.numpy(), cmap='Blues')\n",
        "plt.title('Look-Ahead Mask\\n(size=4)')\n",
        "plt.xlabel('Key Position')\n",
        "plt.ylabel('Query Position')\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        plt.text(j, i, f'{look_ahead[i,j]:.0f}', ha='center', va='center')\n",
        "\n",
        "# Combined mask for first sequence\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.imshow(combined_mask[0].numpy(), cmap='Blues')\n",
        "plt.title('Combined Mask\\n(1st sequence)')\n",
        "plt.xlabel('Key Position')\n",
        "plt.ylabel('Query Position')\n",
        "for i in range(combined_mask.shape[1]):\n",
        "    for j in range(combined_mask.shape[2]):\n",
        "        plt.text(j, i, f'{combined_mask[0,i,j]:.0f}', ha='center', va='center', fontsize=8)\n",
        "\n",
        "# Combined mask for second sequence\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.imshow(combined_mask[1].numpy(), cmap='Blues')\n",
        "plt.title('Combined Mask\\n(2nd sequence)')\n",
        "plt.xlabel('Key Position')\n",
        "plt.ylabel('Query Position')\n",
        "for i in range(combined_mask.shape[1]):\n",
        "    for j in range(combined_mask.shape[2]):\n",
        "        plt.text(j, i, f'{combined_mask[1,i,j]:.0f}', ha='center', va='center', fontsize=8)\n",
        "\n",
        "# Attention weights example\n",
        "plt.subplot(2, 3, 6)\n",
        "# Simulate attention scores before masking\n",
        "scores = tf.random.uniform((4, 4), minval=-2, maxval=2)\n",
        "masked_scores = scores + (combined_mask[0] * -1e9)\n",
        "attention_weights = tf.nn.softmax(masked_scores)\n",
        "\n",
        "plt.imshow(attention_weights.numpy(), cmap='Blues')\n",
        "plt.title('Attention Weights\\n(after masking)')\n",
        "plt.xlabel('Key Position')\n",
        "plt.ylabel('Query Position')\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        plt.text(j, i, f'{attention_weights[i,j]:.2f}', ha='center', va='center', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Explain mask effects\n",
        "print(\"\\nMask Explanations:\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\n1. PADDING MASK:\")\n",
        "print(\"   • Value 1 indicates padding positions\")\n",
        "print(\"   • Prevents attention to padding tokens\")\n",
        "print(\"   • Applied to both encoder and decoder\")\n",
        "\n",
        "print(\"\\n2. LOOK-AHEAD MASK:\")\n",
        "print(\"   • Upper triangular matrix with 1s\")\n",
        "print(\"   • Prevents attention to future tokens\")\n",
        "print(\"   • Only applied to decoder self-attention\")\n",
        "print(\"   • Ensures autoregressive property\")\n",
        "\n",
        "print(\"\\n3. COMBINED MASK:\")\n",
        "print(\"   • Maximum of padding and look-ahead masks\")\n",
        "print(\"   • Combines both masking effects\")\n",
        "print(\"   • Used in decoder self-attention\")\n",
        "\n",
        "print(\"\\n4. ATTENTION WEIGHTS AFTER MASKING:\")\n",
        "print(\"   • Masked positions have weight ≈ 0\")\n",
        "print(\"   • Valid positions sum to 1\")\n",
        "print(\"   • Lower triangle shows causal attention pattern\")\n",
        "\n",
        "# Verify mask properties\n",
        "print(\"\\nMask Verification:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Check that attention weights sum to 1 for valid positions\n",
        "for i in range(attention_weights.shape[0]):\n",
        "    row_sum = tf.reduce_sum(attention_weights[i])\n",
        "    print(f\"Attention weights row {i} sum: {row_sum:.6f}\")\n",
        "\n",
        "# Check causal property\n",
        "print(f\"\\nCausal Property Check:\")\n",
        "for i in range(attention_weights.shape[0]):\n",
        "    future_attention = tf.reduce_sum(attention_weights[i, i+1:])\n",
        "    print(f\"Position {i} attention to future: {future_attention:.6f}\")\n",
        "\n",
        "# Complexity analysis for different mask types\n",
        "print(f\"\\nComplexity Analysis:\")\n",
        "print(f\"Padding mask creation: O(batch_size × seq_len)\")\n",
        "print(f\"Look-ahead mask creation: O(seq_len^2)\")\n",
        "print(f\"Mask application: O(batch_size × num_heads × seq_len^2)\")\n",
        "print(f\"Memory overhead: Minimal (masks are sparse)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recent_innovations_theory"
      },
      "source": [
        "## Part 6: Recent Innovations in Language Models (2018-2019)\n",
        "\n",
        "### The \"ImageNet Moment\" for NLP\n",
        "\n",
        "2018 marked a revolutionary year in NLP, often called the \"ImageNet moment\" due to breakthrough advances in:\n",
        "1. **Transfer Learning for NLP**\n",
        "2. **Bidirectional Representations**\n",
        "3. **Massive Scale Pretraining**\n",
        "4. **Zero-shot and Few-shot Learning**\n",
        "\n",
        "### ELMo: Embeddings from Language Models\n",
        "\n",
        "**Key Innovation:** Contextualized word embeddings\n",
        "\n",
        "**Traditional Problem:**\n",
        "- Static embeddings: \"bank\" has same representation in \"river bank\" and \"money bank\"\n",
        "\n",
        "**ELMo Solution:**\n",
        "- Dynamic embeddings based on context\n",
        "- \"bank\" gets different representations based on surrounding words\n",
        "\n",
        "**Architecture:**\n",
        "$$\\text{ELMo}_k^{\\text{task}} = \\gamma^{\\text{task}} \\sum_{j=0}^{L} s_j^{\\text{task}} h_{k,j}^{LM}$$\n",
        "\n",
        "Where:\n",
        "- $h_{k,j}^{LM}$: Hidden state from layer $j$ of bidirectional LM\n",
        "- $s_j^{\\text{task}}$: Task-specific softmax weights\n",
        "- $\\gamma^{\\text{task}}$: Task-specific scaling factor\n",
        "\n",
        "**Benefits:**\n",
        "- **Polysemy handling**: Different meanings of same word\n",
        "- **Rich representations**: Combines all layers\n",
        "- **Transfer learning**: Pretrained on large corpus\n",
        "\n",
        "### ULMFiT: Universal Language Model Fine-tuning\n",
        "\n",
        "**Three-Stage Process:**\n",
        "\n",
        "**1. General-Domain LM Pretraining:**\n",
        "$$L_{LM} = -\\sum_{t=1}^T \\log P(w_t | w_{1:t-1})$$\n",
        "\n",
        "**2. Target Task LM Fine-tuning:**\n",
        "Fine-tune on target domain data with discriminative learning rates.\n",
        "\n",
        "**3. Target Task Classifier Fine-tuning:**\n",
        "Add classifier layers and fine-tune entire model.\n",
        "\n",
        "**Key Techniques:**\n",
        "- **Discriminative fine-tuning**: Different learning rates per layer\n",
        "- **Slanted triangular learning rates**: Gradual learning rate schedule\n",
        "- **Gradual unfreezing**: Progressively unfreeze layers\n",
        "\n",
        "**Results:**\n",
        "- 100 labeled examples = 10,000 examples trained from scratch\n",
        "- 18-24% error reduction on text classification tasks\n",
        "\n",
        "### GPT: Generative Pre-Training\n",
        "\n",
        "**Core Concept:** Unsupervised pretraining + supervised fine-tuning\n",
        "\n",
        "**Pretraining Objective:**\n",
        "$$L_1(\\mathcal{U}) = \\sum_i \\log P(u_i | u_{i-k}, ..., u_{i-1}; \\Theta)$$\n",
        "\n",
        "**Fine-tuning Objective:**\n",
        "$$L_2(\\mathcal{C}) = \\sum_{(x,y)} \\log P(y | x^1, ..., x^m)$$\n",
        "\n",
        "**Combined Training:**\n",
        "$$L_3(\\mathcal{C}) = L_2(\\mathcal{C}) + \\lambda \\cdot L_1(\\mathcal{C})$$\n",
        "\n",
        "**Architecture Innovations:**\n",
        "- **Transformer decoder only**: Unidirectional attention\n",
        "- **Task-specific input transformations**: Minimal architectural changes\n",
        "- **Linear output layers**: Simple adaptation to different tasks\n",
        "\n",
        "### BERT: Bidirectional Encoder Representations\n",
        "\n",
        "**Revolutionary Approach:** Bidirectional context understanding\n",
        "\n",
        "**Pretraining Tasks:**\n",
        "\n",
        "**1. Masked Language Model (MLM):**\n",
        "- Randomly mask 15% of tokens\n",
        "- 80% → [MASK], 10% → random word, 10% → unchanged\n",
        "- Predict masked tokens using bidirectional context\n",
        "\n",
        "$$L_{MLM} = -\\sum_{i \\in \\text{masked}} \\log P(w_i | w_{\\text{context}})$$\n",
        "\n",
        "**2. Next Sentence Prediction (NSP):**\n",
        "- Given sentence pairs, predict if B follows A\n",
        "- 50% consecutive pairs, 50% random pairs\n",
        "\n",
        "$$L_{NSP} = -\\log P(\\text{IsNext} | [\\text{CLS}])$$\n",
        "\n",
        "**Total Loss:**\n",
        "$$L = L_{MLM} + L_{NSP}$$\n",
        "\n",
        "**Key Advantages:**\n",
        "- **True bidirectionality**: Unlike GPT's left-to-right\n",
        "- **Sentence-level understanding**: NSP task\n",
        "- **Fine-tuning flexibility**: Easy adaptation to downstream tasks\n",
        "\n",
        "### Comparative Analysis\n",
        "\n",
        "| Model | Architecture | Pretraining | Bidirectional | Key Innovation |\n",
        "|-------|-------------|-------------|---------------|----------------|\n",
        "| **ELMo** | BiLSTM | Language Modeling | Yes | Contextualized embeddings |\n",
        "| **ULMFiT** | LSTM | Language Modeling | No | Transfer learning methodology |\n",
        "| **GPT** | Transformer Decoder | Language Modeling | No | Transformer + unsupervised pretraining |\n",
        "| **BERT** | Transformer Encoder | MLM + NSP | Yes | Bidirectional Transformer |\n",
        "\n",
        "### Impact and Implications\n",
        "\n",
        "**Scientific Impact:**\n",
        "- Established transfer learning as standard in NLP\n",
        "- Showed importance of scale in pretraining\n",
        "- Demonstrated power of self-supervised learning\n",
        "\n",
        "**Practical Impact:**\n",
        "- Dramatically improved state-of-the-art across tasks\n",
        "- Reduced data requirements for new tasks\n",
        "- Enabled rapid prototyping and deployment\n",
        "\n",
        "**Future Directions:**\n",
        "- Larger models (GPT-2, GPT-3, etc.)\n",
        "- More efficient architectures\n",
        "- Specialized pretraining objectives\n",
        "- Multimodal understanding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "modern_nlp_demo"
      },
      "source": [
        "# Demonstration of Modern NLP Approaches\n",
        "# This shows how the innovations from 2018-2019 changed NLP\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "print(\"Demonstrating Modern NLP Innovations...\")\n",
        "\n",
        "# Simulate different embedding approaches\n",
        "class TraditionalEmbedding(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Traditional static word embeddings (like Word2Vec, GloVe).\n",
        "    Same word always gets same embedding regardless of context.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Static embeddings - same word ID always produces same vector\n",
        "        return self.embedding(inputs)\n",
        "\n",
        "\n",
        "class ContextualEmbedding(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Contextualized embeddings (ELMo-style).\n",
        "    Word embeddings depend on surrounding context.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.forward_lstm = keras.layers.LSTM(hidden_dim, return_sequences=True)\n",
        "        self.backward_lstm = keras.layers.LSTM(hidden_dim, return_sequences=True, go_backwards=True)\n",
        "        self.context_projection = keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Start with static embeddings\n",
        "        static_embeddings = self.embedding(inputs)\n",
        "\n",
        "        # Add bidirectional context\n",
        "        forward_context = self.forward_lstm(static_embeddings)\n",
        "        backward_context = self.backward_lstm(static_embeddings)\n",
        "\n",
        "        # Combine contexts\n",
        "        combined_context = tf.concat([forward_context, backward_context], axis=-1)\n",
        "\n",
        "        # Project back to embedding dimension\n",
        "        contextual_embeddings = self.context_projection(combined_context)\n",
        "\n",
        "        # Combine static and contextual (ELMo-style weighted combination)\n",
        "        return static_embeddings + contextual_embeddings\n",
        "\n",
        "\n",
        "class MaskedLanguageModel(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Simplified BERT-style Masked Language Model.\n",
        "    Demonstrates bidirectional context understanding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(1000, d_model)\n",
        "\n",
        "        # Simplified transformer encoder\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = keras.Sequential([\n",
        "            keras.layers.Dense(d_model * 4, activation='relu'),\n",
        "            keras.layers.Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = keras.layers.LayerNormalization()\n",
        "        self.layernorm2 = keras.layers.LayerNormalization()\n",
        "\n",
        "        # MLM head\n",
        "        self.mlm_head = keras.layers.Dense(vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        # Embedding + positional encoding\n",
        "        x = self.embedding(inputs)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        # Self-attention (bidirectional)\n",
        "        attn_output, _ = self.attention(x, x, x, mask)\n",
        "        x = self.layernorm1(x + attn_output)\n",
        "\n",
        "        # Feed-forward\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.layernorm2(x + ffn_output)\n",
        "\n",
        "        # MLM predictions\n",
        "        mlm_logits = self.mlm_head(x)\n",
        "\n",
        "        return mlm_logits\n",
        "\n",
        "\n",
        "# Compare different embedding approaches\n",
        "print(\"Comparing Embedding Approaches...\")\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 1000\n",
        "embedding_dim = 128\n",
        "seq_length = 10\n",
        "batch_size = 2\n",
        "\n",
        "# Sample input: \"The bank by the river\" vs \"The bank for money\"\n",
        "# Word \"bank\" (ID=5) appears in different contexts\n",
        "sample_input = tf.constant([\n",
        "    [1, 5, 3, 1, 7, 0, 0, 0, 0, 0],  # \"The bank by the river\" + padding\n",
        "    [1, 5, 2, 9, 0, 0, 0, 0, 0, 0]   # \"The bank for money\" + padding\n",
        "])\n",
        "\n",
        "print(f\"Sample input shape: {sample_input.shape}\")\n",
        "print(f\"Sample input:\")\n",
        "print(sample_input.numpy())\n",
        "\n",
        "# Test traditional embeddings\n",
        "traditional_model = TraditionalEmbedding(vocab_size, embedding_dim)\n",
        "traditional_embeddings = traditional_model(sample_input)\n",
        "\n",
        "print(f\"\\nTraditional embeddings shape: {traditional_embeddings.shape}\")\n",
        "\n",
        "# Check if \"bank\" gets same embedding in both contexts\n",
        "bank_embedding_1 = traditional_embeddings[0, 1]  # \"bank\" in first sentence\n",
        "bank_embedding_2 = traditional_embeddings[1, 1]  # \"bank\" in second sentence\n",
        "traditional_similarity = tf.reduce_sum(bank_embedding_1 * bank_embedding_2)\n",
        "\n",
        "print(f\"Traditional 'bank' embedding similarity: {traditional_similarity:.6f}\")\n",
        "print(\"(Should be very high - same word gets same embedding)\")\n",
        "\n",
        "# Test contextual embeddings\n",
        "contextual_model = ContextualEmbedding(vocab_size, embedding_dim, 64)\n",
        "contextual_embeddings = contextual_model(sample_input)\n",
        "\n",
        "print(f\"\\nContextual embeddings shape: {contextual_embeddings.shape}\")\n",
        "\n",
        "# Check if \"bank\" gets different embeddings in different contexts\n",
        "bank_contextual_1 = contextual_embeddings[0, 1]  # \"bank\" in first sentence\n",
        "bank_contextual_2 = contextual_embeddings[1, 1]  # \"bank\" in second sentence\n",
        "contextual_similarity = tf.reduce_sum(bank_contextual_1 * bank_contextual_2)\n",
        "\n",
        "print(f\"Contextual 'bank' embedding similarity: {contextual_similarity:.6f}\")\n",
        "print(\"(Should be lower - different contexts produce different embeddings)\")\n",
        "\n",
        "# Demonstrate BERT-style MLM\n",
        "print(f\"\\nDemonstrating BERT-style Masked Language Modeling...\")\n",
        "\n",
        "# Create masked input (mask token ID = vocab_size)\n",
        "masked_input = tf.constant([\n",
        "    [1, vocab_size, 3, 1, 7, 0, 0, 0, 0, 0],  # \"The [MASK] by the river\"\n",
        "    [1, vocab_size, 2, 9, 0, 0, 0, 0, 0, 0]   # \"The [MASK] for money\"\n",
        "])\n",
        "\n",
        "print(f\"Masked input (mask token = {vocab_size}):\")\n",
        "print(masked_input.numpy())\n",
        "\n",
        "# BERT-style model\n",
        "bert_model = MaskedLanguageModel(vocab_size + 1, 128, 4)  # +1 for mask token\n",
        "mlm_predictions = bert_model(masked_input)\n",
        "\n",
        "print(f\"\\nMLM predictions shape: {mlm_predictions.shape}\")\n",
        "\n",
        "# Get predictions for masked positions\n",
        "mask_pos_1 = mlm_predictions[0, 1]  # Predictions for mask in first sentence\n",
        "mask_pos_2 = mlm_predictions[1, 1]  # Predictions for mask in second sentence\n",
        "\n",
        "top_predictions_1 = tf.nn.top_k(mask_pos_1, 3)\n",
        "top_predictions_2 = tf.nn.top_k(mask_pos_2, 3)\n",
        "\n",
        "print(f\"\\nTop 3 predictions for first masked position:\")\n",
        "print(f\"Token IDs: {top_predictions_1.indices.numpy()}\")\n",
        "print(f\"Probabilities: {top_predictions_1.values.numpy()}\")\n",
        "\n",
        "print(f\"\\nTop 3 predictions for second masked position:\")\n",
        "print(f\"Token IDs: {top_predictions_2.indices.numpy()}\")\n",
        "print(f\"Probabilities: {top_predictions_2.values.numpy()}\")\n",
        "\n",
        "# Transfer learning simulation\n",
        "print(f\"\\nSimulating Transfer Learning Benefits...\")\n",
        "\n",
        "# Simulate performance with different amounts of training data\n",
        "def simulate_performance(use_pretrained=False, num_examples=100):\n",
        "    \"\"\"\n",
        "    Simulate classification performance with/without pretraining.\n",
        "    \"\"\"\n",
        "    if use_pretrained:\n",
        "        # Pretrained models perform better with less data\n",
        "        base_accuracy = 0.85\n",
        "        data_efficiency = 0.1  # Less sensitive to data amount\n",
        "    else:\n",
        "        # Models trained from scratch need more data\n",
        "        base_accuracy = 0.65\n",
        "        data_efficiency = 0.3  # More sensitive to data amount\n",
        "\n",
        "    # Simulate accuracy based on data amount\n",
        "    accuracy = base_accuracy * (1 - np.exp(-num_examples * data_efficiency / 100))\n",
        "    return accuracy\n",
        "\n",
        "# Compare performance across different data amounts\n",
        "data_amounts = [10, 50, 100, 500, 1000, 5000]\n",
        "pretrained_performance = [simulate_performance(True, n) for n in data_amounts]\n",
        "scratch_performance = [simulate_performance(False, n) for n in data_amounts]\n",
        "\n",
        "print(f\"\\nTransfer Learning Performance Comparison:\")\n",
        "print(f\"{'Data Size':<10} {'From Scratch':<15} {'Pretrained':<15} {'Improvement':<15}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for i, n in enumerate(data_amounts):\n",
        "    scratch_acc = scratch_performance[i]\n",
        "    pretrained_acc = pretrained_performance[i]\n",
        "    improvement = (pretrained_acc - scratch_acc) / scratch_acc * 100\n",
        "\n",
        "    print(f\"{n:<10} {scratch_acc:<15.3f} {pretrained_acc:<15.3f} {improvement:<15.1f}%\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(data_amounts, scratch_performance, 'b-o', label='From Scratch')\n",
        "plt.plot(data_amounts, pretrained_performance, 'r-s', label='Pretrained')\n",
        "plt.xlabel('Training Examples')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Transfer Learning Benefits')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xscale('log')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "# Visualize embedding differences\n",
        "bank_traditional = traditional_embeddings[:, 1, :5].numpy()  # First 5 dims\n",
        "bank_contextual = contextual_embeddings[:, 1, :5].numpy()\n",
        "\n",
        "x = np.arange(5)\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, bank_traditional[0], width, label='River Context (Traditional)', alpha=0.7)\n",
        "plt.bar(x + width/2, bank_traditional[1], width, label='Money Context (Traditional)', alpha=0.7)\n",
        "plt.xlabel('Embedding Dimension')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Traditional Embeddings: \"bank\"')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.bar(x - width/2, bank_contextual[0], width, label='River Context (Contextual)', alpha=0.7)\n",
        "plt.bar(x + width/2, bank_contextual[1], width, label='Money Context (Contextual)', alpha=0.7)\n",
        "plt.xlabel('Embedding Dimension')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Contextual Embeddings: \"bank\"')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "# Timeline of innovations\n",
        "innovations = ['Word2Vec\\n(2013)', 'ELMo\\n(2018)', 'ULMFiT\\n(2018)', 'GPT\\n(2018)', 'BERT\\n(2018)']\n",
        "years = [2013, 2018.1, 2018.3, 2018.6, 2018.8]\n",
        "performance = [70, 75, 82, 85, 88]  # Simulated GLUE scores\n",
        "\n",
        "plt.plot(years, performance, 'go-', linewidth=2, markersize=8)\n",
        "for i, (year, perf, innovation) in enumerate(zip(years, performance, innovations)):\n",
        "    plt.annotate(innovation, (year, perf), textcoords=\"offset points\",\n",
        "                xytext=(0,10), ha='center', fontsize=8)\n",
        "\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Performance (GLUE Score)')\n",
        "plt.title('NLP Progress Timeline (2013-2019)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim(65, 92)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary of innovations\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY OF 2018-2019 NLP INNOVATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. CONTEXTUALIZED EMBEDDINGS (ELMo):\")\n",
        "print(\"   • Words get different representations based on context\")\n",
        "print(\"   • Solves polysemy problem (multiple meanings)\")\n",
        "print(\"   • Uses bidirectional LSTM language model\")\n",
        "\n",
        "print(\"\\n2. TRANSFER LEARNING METHODOLOGY (ULMFiT):\")\n",
        "print(\"   • Established 3-stage transfer learning process\")\n",
        "print(\"   • Showed massive data efficiency gains\")\n",
        "print(\"   • Introduced discriminative fine-tuning techniques\")\n",
        "\n",
        "print(\"\\n3. TRANSFORMER PRETRAINING (GPT):\")\n",
        "print(\"   • Combined Transformer architecture with unsupervised pretraining\")\n",
        "print(\"   • Demonstrated generative pretraining effectiveness\")\n",
        "print(\"   • Minimal task-specific architectural changes\")\n",
        "\n",
        "print(\"\\n4. BIDIRECTIONAL TRANSFORMERS (BERT):\")\n",
        "print(\"   • True bidirectional context understanding\")\n",
        "print(\"   • Masked Language Model (MLM) pretraining\")\n",
        "print(\"   • Next Sentence Prediction (NSP) task\")\n",
        "print(\"   • Revolutionary performance across NLP tasks\")\n",
        "\n",
        "print(\"\\n5. KEY BREAKTHROUGH PRINCIPLES:\")\n",
        "print(\"   • Scale matters: Larger models + more data = better performance\")\n",
        "print(\"   • Transfer learning: Pretrain on large corpus, fine-tune on task\")\n",
        "print(\"   • Self-supervised learning: Learn from unlabeled text\")\n",
        "print(\"   • Architecture matters: Transformers > RNNs for many tasks\")\n",
        "\n",
        "print(\"\\n6. IMPACT ON INDUSTRY:\")\n",
        "print(\"   • Reduced time-to-market for NLP applications\")\n",
        "print(\"   • Democratized access to state-of-the-art NLP\")\n",
        "print(\"   • Enabled rapid prototyping and experimentation\")\n",
        "print(\"   • Set foundation for modern language models (GPT-3, ChatGPT, etc.)\")\n",
        "\n",
        "# Performance comparison visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# GLUE benchmark scores (simulated based on historical data)\n",
        "models = ['LSTM\\n(2017)', 'ELMo\\n(2018)', 'GPT\\n(2018)', 'BERT-Base\\n(2018)', 'BERT-Large\\n(2018)']\n",
        "glue_scores = [68.5, 74.3, 78.5, 82.1, 84.6]\n",
        "param_counts = [10, 94, 117, 110, 340]  # Million parameters\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "bars = plt.bar(models, glue_scores, color=['red', 'orange', 'yellow', 'lightgreen', 'green'])\n",
        "plt.title('GLUE Benchmark Progress (2017-2018)')\n",
        "plt.ylabel('GLUE Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add score labels on bars\n",
        "for bar, score in zip(bars, glue_scores):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "             f'{score}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.scatter(param_counts, glue_scores, s=200, c=['red', 'orange', 'yellow', 'lightgreen', 'green'], alpha=0.7)\n",
        "for i, model in enumerate(models):\n",
        "    plt.annotate(model.replace('\\n', ' '), (param_counts[i], glue_scores[i]),\n",
        "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "plt.xlabel('Parameters (Millions)')\n",
        "plt.ylabel('GLUE Score')\n",
        "plt.title('Performance vs Model Size')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Task performance comparison\n",
        "plt.subplot(2, 2, 3)\n",
        "tasks = ['Sentiment\\nAnalysis', 'Question\\nAnswering', 'Text\\nClassification', 'NER', 'Translation']\n",
        "pre_2018 = [78, 65, 75, 85, 28]  # BLEU for translation\n",
        "post_2018 = [93, 89, 91, 94, 35]\n",
        "\n",
        "x = np.arange(len(tasks))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, pre_2018, width, label='Pre-2018', alpha=0.7, color='lightcoral')\n",
        "plt.bar(x + width/2, post_2018, width, label='Post-2018', alpha=0.7, color='lightblue')\n",
        "\n",
        "plt.xlabel('NLP Tasks')\n",
        "plt.ylabel('Performance Score')\n",
        "plt.title('Task Performance: Before vs After 2018')\n",
        "plt.xticks(x, tasks)\n",
        "plt.legend()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Data efficiency comparison\n",
        "plt.subplot(2, 2, 4)\n",
        "training_sizes = [100, 500, 1000, 5000, 10000]\n",
        "traditional_acc = [45, 62, 71, 82, 87]\n",
        "transfer_acc = [78, 86, 89, 92, 94]\n",
        "\n",
        "plt.plot(training_sizes, traditional_acc, 'r-o', label='Traditional Training', linewidth=2)\n",
        "plt.plot(training_sizes, transfer_acc, 'b-s', label='Transfer Learning', linewidth=2)\n",
        "plt.xlabel('Training Examples')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Data Efficiency: Transfer Learning vs Traditional')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"2018: THE YEAR THAT CHANGED NLP FOREVER\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nKey Innovations:\")\n",
        "print(\"• ELMo: Contextualized word representations\")\n",
        "print(\"• ULMFiT: Transfer learning methodology for NLP\")\n",
        "print(\"• GPT: Transformer-based generative pretraining\")\n",
        "print(\"• BERT: Bidirectional encoder representations\")\n",
        "print(\"\\nImpact:\")\n",
        "print(\"• 10-20 point improvements across benchmarks\")\n",
        "print(\"• 10x reduction in labeled data requirements\")\n",
        "print(\"• Unified approach across diverse NLP tasks\")\n",
        "print(\"• Foundation for modern large language models\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercises_introduction"
      },
      "source": [
        "## Chapter 16 Exercises: Comprehensive Solutions\n",
        "\n",
        "This section provides detailed solutions to all exercises from Chapter 16, with comprehensive theoretical explanations, mathematical foundations, and practical implementations.\n",
        "\n",
        "### Exercise Overview\n",
        "\n",
        "The exercises cover:\n",
        "1. **Stateful vs Stateless RNNs** - Understanding the trade-offs\n",
        "2. **Encoder-Decoder vs Sequence-to-Sequence** - Architecture choices\n",
        "3. **Variable-length sequences** - Handling dynamic inputs/outputs\n",
        "4. **Beam Search** - Advanced decoding strategies\n",
        "5. **Attention mechanisms** - The attention revolution\n",
        "6. **Transformer architecture** - Multi-head attention importance\n",
        "7. **Sampled softmax** - Efficient training with large vocabularies\n",
        "8. **Embedded Reber grammars** - Sequence learning evaluation\n",
        "9. **Date format conversion** - Practical seq2seq application\n",
        "10. **Neural Machine Translation** - Complete implementation\n",
        "11. **Advanced language models** - Modern NLP applications\n",
        "\n",
        "Each exercise includes:\n",
        "- **Theoretical background** with mathematical formulations\n",
        "- **Complete implementation** with detailed comments\n",
        "- **Performance analysis** and optimization strategies\n",
        "- **Real-world applications** and extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_1_theory"
      },
      "source": [
        "## Exercise 1: Stateful vs Stateless RNNs\n",
        "\n",
        "### Question\n",
        "What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
        "\n",
        "### Theoretical Analysis\n",
        "\n",
        "#### Stateless RNNs\n",
        "\n",
        "**Mathematical Definition:**\n",
        "For each batch, hidden state is reset:\n",
        "$$h_0^{(b)} = \\mathbf{0} \\text{ for all batches } b$$\n",
        "$$h_t^{(b)} = f(x_t^{(b)}, h_{t-1}^{(b)})$$\n",
        "\n",
        "**Advantages:**\n",
        "1. **Simplicity**: Easy to implement and debug\n",
        "2. **Parallelization**: Batches can be processed independently\n",
        "3. **Flexibility**: Can handle variable sequence lengths easily\n",
        "4. **Shuffling**: Training data can be shuffled for better optimization\n",
        "5. **Memory efficiency**: No need to store states between batches\n",
        "\n",
        "**Disadvantages:**\n",
        "1. **Limited context**: Cannot learn patterns longer than sequence length\n",
        "2. **Information loss**: Useful context from previous sequences is discarded\n",
        "3. **Gradient limitations**: Gradients cannot flow beyond sequence boundaries\n",
        "\n",
        "#### Stateful RNNs\n",
        "\n",
        "**Mathematical Definition:**\n",
        "Hidden state carries over between batches:\n",
        "$$h_0^{(b)} = h_T^{(b-1)}$$\n",
        "$$h_t^{(b)} = f(x_t^{(b)}, h_{t-1}^{(b)})$$\n",
        "\n",
        "**Advantages:**\n",
        "1. **Long-term memory**: Can learn patterns spanning multiple sequences\n",
        "2. **Better context**: Maintains information across batch boundaries\n",
        "3. **Coherent generation**: More consistent text generation\n",
        "4. **Online learning**: Suitable for streaming data scenarios\n",
        "\n",
        "**Disadvantages:**\n",
        "1. **Complex implementation**: Requires careful state management\n",
        "2. **Fixed batch size**: Batch size must be consistent\n",
        "3. **Sequential processing**: Cannot parallelize across batches\n",
        "4. **No shuffling**: Data order must be preserved\n",
        "5. **Memory overhead**: Must store states between batches\n",
        "6. **Debugging difficulty**: Harder to isolate issues\n",
        "\n",
        "### When to Use Each Type\n",
        "\n",
        "**Use Stateless RNNs when:**\n",
        "- Sequences are independent (e.g., separate documents)\n",
        "- Computational efficiency is important\n",
        "- Sequence lengths vary significantly\n",
        "- Standard supervised learning setup\n",
        "\n",
        "**Use Stateful RNNs when:**\n",
        "- Long-term dependencies are crucial\n",
        "- Processing continuous streams (e.g., live transcription)\n",
        "- Text generation requiring consistency\n",
        "- Time series with very long patterns\n",
        "\n",
        "### Computational Complexity\n",
        "\n",
        "| Aspect | Stateless | Stateful |\n",
        "|--------|-----------|----------|\n",
        "| **Memory** | $O(B \\times T \\times H)$ | $O(B \\times T \\times H + B \\times H)$ |\n",
        "| **Computation** | $O(B \\times T \\times H^2)$ | $O(B \\times T \\times H^2)$ |\n",
        "| **Parallelization** | Full batch parallelization | Limited parallelization |\n",
        "| **Implementation** | Simple | Complex |\n",
        "\n",
        "Where $B$ = batch size, $T$ = sequence length, $H$ = hidden size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise_1_implementation"
      },
      "source": [
        "# Exercise 1: Comprehensive Comparison of Stateful vs Stateless RNNs\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "print(\"Exercise 1: Stateful vs Stateless RNN Comparison\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Generate synthetic data to demonstrate differences\n",
        "def generate_long_sequence_data(total_length=10000, pattern_length=50):\n",
        "    \"\"\"\n",
        "    Generate data with long-term patterns that span multiple sequence windows.\n",
        "    This will help demonstrate the advantage of stateful RNNs.\n",
        "    \"\"\"\n",
        "    # Create a repeating pattern\n",
        "    pattern = np.sin(np.linspace(0, 4*np.pi, pattern_length))\n",
        "\n",
        "    # Repeat pattern and add noise\n",
        "    full_sequence = np.tile(pattern, total_length // pattern_length + 1)[:total_length]\n",
        "    full_sequence += np.random.normal(0, 0.1, total_length)\n",
        "\n",
        "    return full_sequence.astype(np.float32)\n",
        "\n",
        "def create_windowed_dataset(data, window_size, batch_size, stateful=False):\n",
        "    \"\"\"\n",
        "    Create windowed dataset for RNN training.\n",
        "\n",
        "    Args:\n",
        "        data: Input sequence\n",
        "        window_size: Size of each window\n",
        "        batch_size: Batch size\n",
        "        stateful: Whether to create dataset for stateful RNN\n",
        "\n",
        "    Returns:\n",
        "        TensorFlow dataset\n",
        "    \"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "\n",
        "    if stateful:\n",
        "        # For stateful RNN: non-overlapping windows, no shuffling\n",
        "        dataset = dataset.window(window_size + 1, shift=window_size, drop_remainder=True)\n",
        "    else:\n",
        "        # For stateless RNN: overlapping windows, can shuffle\n",
        "        dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "\n",
        "    if not stateful:\n",
        "        dataset = dataset.shuffle(1000)\n",
        "\n",
        "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Generate test data\n",
        "print(\"Generating synthetic data with long-term patterns...\")\n",
        "long_sequence = generate_long_sequence_data(total_length=5000, pattern_length=100)\n",
        "\n",
        "# Split into train/test\n",
        "train_data = long_sequence[:4000]\n",
        "test_data = long_sequence[4000:]\n",
        "\n",
        "print(f\"Train data length: {len(train_data)}\")\n",
        "print(f\"Test data length: {len(test_data)}\")\n",
        "\n",
        "# Visualize the pattern\n",
        "plt.figure(figsize=(15, 4))\n",
        "plt.plot(long_sequence[:500])\n",
        "plt.title('Synthetic Data with Long-term Patterns')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Model parameters\n",
        "window_size = 50\n",
        "batch_size = 32\n",
        "units = 64\n",
        "epochs = 10\n",
        "\n",
        "# Create datasets\n",
        "print(\"\\nCreating datasets...\")\n",
        "stateless_train = create_windowed_dataset(train_data, window_size, batch_size, stateful=False)\n",
        "stateless_test = create_windowed_dataset(test_data, window_size, batch_size, stateful=False)\n",
        "\n",
        "stateful_train = create_windowed_dataset(train_data, window_size, batch_size, stateful=True)\n",
        "stateful_test = create_windowed_dataset(test_data, window_size, batch_size, stateful=True)\n",
        "\n",
        "print(f\"Stateless train batches: {len(list(stateless_train))}\")\n",
        "print(f\"Stateful train batches: {len(list(stateful_train))}\")\n",
        "\n",
        "# Create models\n",
        "def create_stateless_model(window_size, units):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.LSTM(units, return_sequences=True, input_shape=[window_size, 1]),\n",
        "        keras.layers.LSTM(units, return_sequences=True),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def create_stateful_model(window_size, units, batch_size):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.LSTM(units, return_sequences=True, stateful=True,\n",
        "                         batch_input_shape=[batch_size, window_size, 1]),\n",
        "        keras.layers.LSTM(units, return_sequences=True, stateful=True),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# State reset callback for stateful model\n",
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.model.reset_states()\n",
        "\n",
        "print(\"\\nCreating and training models...\")\n",
        "\n",
        "# Stateless model\n",
        "print(\"Training stateless model...\")\n",
        "stateless_model = create_stateless_model(window_size, units)\n",
        "stateless_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "start_time = time.time()\n",
        "stateless_history = stateless_model.fit(\n",
        "    stateless_train.map(lambda x, y: (tf.expand_dims(x, -1), tf.expand_dims(y, -1))),\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    validation_data=stateless_test.map(lambda x, y: (tf.expand_dims(x, -1), tf.expand_dims(y, -1)))\n",
        ")\n",
        "stateless_time = time.time() - start_time\n",
        "\n",
        "# Stateful model\n",
        "print(\"\\nTraining stateful model...\")\n",
        "stateful_model = create_stateful_model(window_size, units, batch_size)\n",
        "stateful_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "start_time = time.time()\n",
        "stateful_history = stateful_model.fit(\n",
        "    stateful_train.map(lambda x, y: (tf.expand_dims(x, -1), tf.expand_dims(y, -1))),\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    callbacks=[ResetStatesCallback()],\n",
        "    validation_data=stateful_test.map(lambda x, y: (tf.expand_dims(x, -1), tf.expand_dims(y, -1)))\n",
        ")\n",
        "stateful_time = time.time() - start_time\n",
        "\n",
        "# Compare results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING RESULTS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "stateless_final_loss = stateless_history.history['loss'][-1]\n",
        "stateful_final_loss = stateful_history.history['loss'][-1]\n",
        "stateless_final_val_loss = stateless_history.history['val_loss'][-1]\n",
        "stateful_final_val_loss = stateful_history.history['val_loss'][-1]\n",
        "\n",
        "print(f\"\\nFinal Training Loss:\")\n",
        "print(f\"Stateless: {stateless_final_loss:.6f}\")\n",
        "print(f\"Stateful:  {stateful_final_loss:.6f}\")\n",
        "print(f\"Improvement: {((stateless_final_loss - stateful_final_loss) / stateless_final_loss * 100):+.2f}%\")\n",
        "\n",
        "print(f\"\\nFinal Validation Loss:\")\n",
        "print(f\"Stateless: {stateless_final_val_loss:.6f}\")\n",
        "print(f\"Stateful:  {stateful_final_val_loss:.6f}\")\n",
        "print(f\"Improvement: {((stateless_final_val_loss - stateful_final_val_loss) / stateless_final_val_loss * 100):+.2f}%\")\n",
        "\n",
        "print(f\"\\nTraining Time:\")\n",
        "print(f\"Stateless: {stateless_time:.2f} seconds\")\n",
        "print(f\"Stateful:  {stateful_time:.2f} seconds\")\n",
        "print(f\"Time difference: {((stateful_time - stateless_time) / stateless_time * 100):+.2f}%\")\n",
        "\n",
        "# Visualize training progress\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(stateless_history.history['loss'], label='Stateless', linewidth=2)\n",
        "plt.plot(stateful_history.history['loss'], label='Stateful', linewidth=2)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.plot(stateless_history.history['val_loss'], label='Stateless', linewidth=2)\n",
        "plt.plot(stateful_history.history['val_loss'], label='Stateful', linewidth=2)\n",
        "plt.title('Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.plot(stateless_history.history['mae'], label='Stateless', linewidth=2)\n",
        "plt.plot(stateful_history.history['mae'], label='Stateful', linewidth=2)\n",
        "plt.title('Mean Absolute Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Generate predictions for comparison\n",
        "test_input = test_data[:window_size].reshape(1, window_size, 1)\n",
        "\n",
        "# Create stateless version of stateful model for prediction\n",
        "stateful_pred_model = create_stateless_model(window_size, units)\n",
        "stateful_pred_model.set_weights(stateful_model.get_weights())\n",
        "\n",
        "# Generate sequences\n",
        "def generate_sequence(model, initial_input, length):\n",
        "    generated = []\n",
        "    current_input = initial_input.copy()\n",
        "\n",
        "    for _ in range(length):\n",
        "        pred = model.predict(current_input, verbose=0)\n",
        "        next_val = pred[0, -1, 0]\n",
        "        generated.append(next_val)\n",
        "\n",
        "        # Update input\n",
        "        current_input = np.roll(current_input, -1, axis=1)\n",
        "        current_input[0, -1, 0] = next_val\n",
        "\n",
        "    return np.array(generated)\n",
        "\n",
        "# Generate predictions\n",
        "pred_length = 200\n",
        "stateless_pred = generate_sequence(stateless_model, test_input, pred_length)\n",
        "stateful_pred = generate_sequence(stateful_pred_model, test_input, pred_length)\n",
        "true_sequence = test_data[window_size:window_size + pred_length]\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.plot(true_sequence, label='True', linewidth=2, alpha=0.8)\n",
        "plt.plot(stateless_pred, label='Stateless Pred', linewidth=1.5, alpha=0.8)\n",
        "plt.plot(stateful_pred, label='Stateful Pred', linewidth=1.5, alpha=0.8)\n",
        "plt.title('Sequence Generation Comparison')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Error analysis\n",
        "stateless_error = np.abs(stateless_pred - true_sequence)\n",
        "stateful_error = np.abs(stateful_pred - true_sequence)\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.plot(stateless_error, label='Stateless Error', linewidth=2)\n",
        "plt.plot(stateful_error, label='Stateful Error', linewidth=2)\n",
        "plt.title('Absolute Error Over Time')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Absolute Error')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Summary statistics\n",
        "plt.subplot(2, 3, 6)\n",
        "metrics = ['MAE', 'RMSE', 'Max Error']\n",
        "stateless_metrics = [\n",
        "    np.mean(stateless_error),\n",
        "    np.sqrt(np.mean(stateless_error**2)),\n",
        "    np.max(stateless_error)\n",
        "]\n",
        "stateful_metrics = [\n",
        "    np.mean(stateful_error),\n",
        "    np.sqrt(np.mean(stateful_error**2)),\n",
        "    np.max(stateful_error)\n",
        "]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, stateless_metrics, width, label='Stateless', alpha=0.8)\n",
        "plt.bar(x + width/2, stateful_metrics, width, label='Stateful', alpha=0.8)\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Error')\n",
        "plt.title('Error Metrics Comparison')\n",
        "plt.xticks(x, metrics)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXERCISE 1 CONCLUSIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n1. PERFORMANCE COMPARISON:\")\n",
        "print(f\"   • Stateful model achieved {((stateless_final_val_loss - stateful_final_val_loss) / stateless_final_val_loss * 100):.2f}% better validation loss\")\n",
        "print(f\"   • Stateful model shows {((np.mean(stateless_error) - np.mean(stateful_error)) / np.mean(stateless_error) * 100):.2f}% lower prediction error\")\n",
        "print(f\"   • Training time difference: {((stateful_time - stateless_time) / stateless_time * 100):+.1f}%\")\n",
        "\n",
        "print(f\"\\n2. WHEN TO USE STATEFUL RNNs:\")\n",
        "print(f\"   • Long sequences with patterns spanning multiple windows\")\n",
        "print(f\"   • Continuous data streams (time series, audio, text)\")\n",
        "print(f\"   • Text generation requiring long-term coherence\")\n",
        "print(f\"   • When computational overhead is acceptable\")\n",
        "\n",
        "print(f\"\\n3. WHEN TO USE STATELESS RNNs:\")\n",
        "print(f\"   • Independent sequences (different documents/conversations)\")\n",
        "print(f\"   • Need for data shuffling and parallel processing\")\n",
        "print(f\"   • Variable sequence lengths\")\n",
        "print(f\"   • Simpler implementation and debugging requirements\")\n",
        "\n",
        "print(f\"\\n4. KEY TRADE-OFFS:\")\n",
        "print(f\"   • Performance vs Complexity\")\n",
        "print(f\"   • Memory efficiency vs Long-term learning\")\n",
        "print(f\"   • Parallelization vs Sequential dependencies\")\n",
        "print(f\"   • Flexibility vs Specialized optimization\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_2_theory"
      },
      "source": [
        "## Exercise 2: Encoder-Decoder vs Plain Sequence-to-Sequence RNNs\n",
        "\n",
        "### Question\n",
        "Why do people use Encoder-Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
        "\n",
        "### Theoretical Analysis\n",
        "\n",
        "#### Plain Sequence-to-Sequence RNNs\n",
        "\n",
        "**Architecture:**\n",
        "Single RNN that processes source and target sequences consecutively:\n",
        "$$h_t = f(x_t, h_{t-1}) \\quad \\text{for } t = 1, ..., T_{src} + T_{tgt}$$\n",
        "\n",
        "Where:\n",
        "- $x_1, ..., x_{T_{src}}$: source tokens\n",
        "- $x_{T_{src}+1}, ..., x_{T_{src}+T_{tgt}}$: target tokens\n",
        "\n",
        "**Problems:**\n",
        "1. **Fixed input-output alignment**: Assumes one-to-one token correspondence\n",
        "2. **Early prediction**: Must start generating before seeing entire source\n",
        "3. **Language mixing**: Source and target share same hidden state\n",
        "4. **Limited flexibility**: Cannot handle different input/output lengths well\n",
        "\n",
        "#### Encoder-Decoder Architecture\n",
        "\n",
        "**Two-stage process:**\n",
        "\n",
        "**Encoder:**\n",
        "$$h_t^{(e)} = f_{enc}(x_t^{(src)}, h_{t-1}^{(e)})$$\n",
        "$$c = g(h_1^{(e)}, h_2^{(e)}, ..., h_{T_{src}}^{(e)})$$\n",
        "\n",
        "**Decoder:**\n",
        "$$h_t^{(d)} = f_{dec}(y_{t-1}^{(tgt)}, h_{t-1}^{(d)}, c)$$\n",
        "$$P(y_t | y_{<t}, x) = \\text{softmax}(W h_t^{(d)} + b)$$\n",
        "\n",
        "### Key Advantages of Encoder-Decoder\n",
        "\n",
        "#### 1. Complete Source Processing\n",
        "- **Full context**: Encoder sees entire source before decoding starts\n",
        "- **Better representations**: Can build rich source understanding\n",
        "- **Bidirectional information**: Can use bidirectional encoders\n",
        "\n",
        "#### 2. Variable Length Handling\n",
        "**Mathematical flexibility:**\n",
        "$$\\text{len}(\\text{source}) \\neq \\text{len}(\\text{target})$$\n",
        "\n",
        "Examples:\n",
        "- English: \"I love you\" (3 tokens)\n",
        "- German: \"Ich liebe dich\" (3 tokens)\n",
        "- Japanese: \"愛してる\" (1 token)\n",
        "- Finnish: \"Minä rakastan sinua\" (3 tokens)\n",
        "\n",
        "#### 3. Language Separation\n",
        "- **Specialized parameters**: Different parameters for source/target languages\n",
        "- **Language-specific optimization**: Each component optimized for its role\n",
        "- **Reduced interference**: Source and target processing don't interfere\n",
        "\n",
        "#### 4. Attention Compatibility\n",
        "- **Multiple source representations**: Encoder provides multiple hidden states\n",
        "- **Attention mechanism**: Decoder can attend to different source parts\n",
        "- **Alignment learning**: Model learns source-target alignments\n",
        "\n",
        "### Computational Complexity Comparison\n",
        "\n",
        "| Architecture | Parameters | Computation | Memory |\n",
        "|--------------|------------|-------------|--------|\n",
        "| **Plain Seq2Seq** | $O(V \\times H + H^2)$ | $O((T_s + T_t) \\times H^2)$ | $O(H)$ |\n",
        "| **Encoder-Decoder** | $O(2V \\times H + 2H^2)$ | $O(T_s \\times H^2 + T_t \\times H^2)$ | $O(T_s \\times H)$ |\n",
        "| **With Attention** | $O(2V \\times H + 2H^2 + A)$ | $O(T_s \\times H^2 + T_t \\times T_s \\times H)$ | $O(T_s \\times H)$ |\n",
        "\n",
        "Where:\n",
        "- $V$: vocabulary size\n",
        "- $H$: hidden size\n",
        "- $T_s, T_t$: source/target sequence lengths\n",
        "- $A$: attention parameters\n",
        "\n",
        "### Empirical Evidence\n",
        "\n",
        "**BLEU Score Improvements:**\n",
        "- Plain Seq2Seq: ~15-20 BLEU\n",
        "- Encoder-Decoder: ~25-30 BLEU\n",
        "- With Attention: ~35-40 BLEU\n",
        "- Transformer: ~40-45 BLEU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise_2_implementation"
      },
      "source": [
        "# Exercise 2: Encoder-Decoder vs Plain Sequence-to-Sequence Comparison\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "print(\"Exercise 2: Encoder-Decoder vs Plain Sequence-to-Sequence RNNs\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create synthetic translation data to demonstrate differences\n",
        "def create_translation_data():\n",
        "    \"\"\"\n",
        "    Create synthetic translation pairs with different characteristics:\n",
        "    1. Different lengths\n",
        "    2. Word reordering\n",
        "    3. Complex mappings\n",
        "    \"\"\"\n",
        "    # Simple number to word translation (simulates different languages)\n",
        "    source_sentences = []\n",
        "    target_sentences = []\n",
        "\n",
        "    # Pattern 1: Different lengths\n",
        "    numbers = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
        "    target_numbers = ['null', 'eins', 'zwei', 'drei', 'vier', 'funf', 'sechs', 'sieben', 'acht', 'neun']\n",
        "\n",
        "    for i in range(100):\n",
        "        # Random length sequences (1-4 numbers)\n",
        "        length = np.random.randint(1, 5)\n",
        "        src_nums = np.random.choice(numbers, length, replace=True)\n",
        "        tgt_nums = [target_numbers[numbers.index(num)] for num in src_nums]\n",
        "\n",
        "        # Sometimes reverse order (different word order)\n",
        "        if np.random.random() < 0.3:\n",
        "            tgt_nums = tgt_nums[::-1]\n",
        "\n",
        "        source_sentences.append(' '.join(src_nums))\n",
        "        target_sentences.append(' '.join(tgt_nums))\n",
        "\n",
        "    # Pattern 2: Complex mappings (compound words)\n",
        "    compounds = {\n",
        "        'big red': 'grossrot',\n",
        "        'small blue': 'kleinblau',\n",
        "        'fast car': 'schnellauto',\n",
        "        'slow train': 'langsamzug'\n",
        "    }\n",
        "\n",
        "    for src, tgt in compounds.items():\n",
        "        for _ in range(10):\n",
        "            source_sentences.append(src)\n",
        "            target_sentences.append(tgt)\n",
        "\n",
        "    return source_sentences, target_sentences\n",
        "\n",
        "def create_vocabularies(source_sentences, target_sentences):\n",
        "    \"\"\"\n",
        "    Create vocabulary mappings for source and target languages.\n",
        "    \"\"\"\n",
        "    # Combine all words\n",
        "    source_words = []\n",
        "    target_words = []\n",
        "\n",
        "    for sent in source_sentences:\n",
        "        source_words.extend(sent.split())\n",
        "\n",
        "    for sent in target_sentences:\n",
        "        target_words.extend(sent.split())\n",
        "\n",
        "    # Create vocabularies\n",
        "    source_vocab = ['<PAD>', '<START>', '<END>'] + list(set(source_words))\n",
        "    target_vocab = ['<PAD>', '<START>', '<END>'] + list(set(target_words))\n",
        "\n",
        "    # Create word-to-index mappings\n",
        "    source_word_to_idx = {word: idx for idx, word in enumerate(source_vocab)}\n",
        "    target_word_to_idx = {word: idx for idx, word in enumerate(target_vocab)}\n",
        "\n",
        "    return (source_vocab, target_vocab, source_word_to_idx, target_word_to_idx)\n",
        "\n",
        "def encode_sentences(sentences, word_to_idx, max_length=10, add_start_end=False):\n",
        "    \"\"\"\n",
        "    Convert sentences to sequences of indices.\n",
        "    \"\"\"\n",
        "    encoded = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        words = sent.split()\n",
        "\n",
        "        if add_start_end:\n",
        "            words = ['<START>'] + words + ['<END>']\n",
        "\n",
        "        # Convert to indices\n",
        "        indices = [word_to_idx.get(word, 0) for word in words]  # 0 for unknown\n",
        "\n",
        "        # Pad or truncate\n",
        "        if len(indices) < max_length:\n",
        "            indices.extend([0] * (max_length - len(indices)))  # Pad with 0\n",
        "        else:\n",
        "            indices = indices[:max_length]\n",
        "\n",
        "        encoded.append(indices)\n",
        "\n",
        "    return np.array(encoded)\n",
        "\n",
        "# Generate data\n",
        "print(\"Generating synthetic translation data...\")\n",
        "source_sentences, target_sentences = create_translation_data()\n",
        "\n",
        "print(f\"Generated {len(source_sentences)} sentence pairs\")\n",
        "print(\"\\nExample translations:\")\n",
        "for i in range(5):\n",
        "    print(f\"  {source_sentences[i]} -> {target_sentences[i]}\")\n",
        "\n",
        "# Create vocabularies\n",
        "source_vocab, target_vocab, source_word_to_idx, target_word_to_idx = create_vocabularies(\n",
        "    source_sentences, target_sentences\n",
        ")\n",
        "\n",
        "print(f\"\\nVocabulary sizes:\")\n",
        "print(f\"Source: {len(source_vocab)} words\")\n",
        "print(f\"Target: {len(target_vocab)} words\")\n",
        "\n",
        "# Encode sentences\n",
        "max_length = 8\n",
        "encoded_sources = encode_sentences(source_sentences, source_word_to_idx, max_length)\n",
        "encoded_targets = encode_sentences(target_sentences, target_word_to_idx, max_length, add_start_end=True)\n",
        "\n",
        "print(f\"\\nEncoded shapes:\")\n",
        "print(f\"Sources: {encoded_sources.shape}\")\n",
        "print(f\"Targets: {encoded_targets.shape}\")\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * len(encoded_sources))\n",
        "train_sources = encoded_sources[:train_size]\n",
        "train_targets = encoded_targets[:train_size]\n",
        "val_sources = encoded_sources[train_size:]\n",
        "val_targets = encoded_targets[train_size:]\n",
        "\n",
        "print(f\"\\nData split:\")\n",
        "print(f\"Training: {len(train_sources)} pairs\")\n",
        "print(f\"Validation: {len(val_sources)} pairs\")\n",
        "\n",
        "# Model 1: Plain Sequence-to-Sequence\n",
        "class PlainSeq2Seq(keras.Model):\n",
        "    \"\"\"\n",
        "    Plain sequence-to-sequence model.\n",
        "    Processes source and target in one continuous sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_units = hidden_units\n",
        "\n",
        "        # Shared components\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "        self.output_layer = keras.layers.Dense(vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        source, target_input = inputs\n",
        "\n",
        "        # Concatenate source and target\n",
        "        # Note: This is a simplified version - real implementation would be more complex\n",
        "        combined_input = tf.concat([source, target_input], axis=1)\n",
        "\n",
        "        # Embed and process\n",
        "        embedded = self.embedding(combined_input)\n",
        "        rnn_output, _, _ = self.rnn(embedded)\n",
        "\n",
        "        # Only predict on target portion\n",
        "        target_portion = rnn_output[:, source.shape[1]:, :]\n",
        "        output = self.output_layer(target_portion)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Model 2: Encoder-Decoder\n",
        "class EncoderDecoder(keras.Model):\n",
        "    \"\"\"\n",
        "    Proper Encoder-Decoder model.\n",
        "    Separate encoder and decoder with distinct parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, source_vocab_size, target_vocab_size, embedding_dim, hidden_units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Encoder components\n",
        "        self.encoder_embedding = keras.layers.Embedding(source_vocab_size, embedding_dim)\n",
        "        self.encoder_rnn = keras.layers.LSTM(hidden_units, return_state=True)\n",
        "\n",
        "        # Decoder components\n",
        "        self.decoder_embedding = keras.layers.Embedding(target_vocab_size, embedding_dim)\n",
        "        self.decoder_rnn = keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "        self.output_layer = keras.layers.Dense(target_vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        source, target_input = inputs\n",
        "\n",
        "        # Encoder\n",
        "        encoder_embedded = self.encoder_embedding(source)\n",
        "        _, encoder_h, encoder_c = self.encoder_rnn(encoder_embedded)\n",
        "        encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "        # Decoder\n",
        "        decoder_embedded = self.decoder_embedding(target_input)\n",
        "        decoder_output, _, _ = self.decoder_rnn(\n",
        "            decoder_embedded, initial_state=encoder_states\n",
        "        )\n",
        "\n",
        "        output = self.output_layer(decoder_output)\n",
        "        return output\n",
        "\n",
        "# Create models\n",
        "embedding_dim = 64\n",
        "hidden_units = 128\n",
        "max_vocab_size = max(len(source_vocab), len(target_vocab))\n",
        "\n",
        "print(\"\\nCreating models...\")\n",
        "\n",
        "# Plain Seq2Seq model\n",
        "plain_model = PlainSeq2Seq(\n",
        "    vocab_size=max_vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_units=hidden_units\n",
        ")\n",
        "\n",
        "# Encoder-Decoder model\n",
        "enc_dec_model = EncoderDecoder(\n",
        "    source_vocab_size=len(source_vocab),\n",
        "    target_vocab_size=len(target_vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_units=hidden_units\n",
        ")\n",
        "\n",
        "# Compile models\n",
        "plain_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "enc_dec_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Prepare training data\n",
        "train_decoder_input = train_targets[:, :-1]  # Remove last token\n",
        "train_decoder_output = train_targets[:, 1:]  # Remove first token\n",
        "\n",
        "val_decoder_input = val_targets[:, :-1]\n",
        "val_decoder_output = val_targets[:, 1:]\n",
        "\n",
        "print(f\"\\nTraining data shapes:\")\n",
        "print(f\"Source: {train_sources.shape}\")\n",
        "print(f\"Decoder input: {train_decoder_input.shape}\")\n",
        "print(f\"Decoder output: {train_decoder_output.shape}\")\n",
        "\n",
        "# Train models\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "print(\"\\nTraining Plain Sequence-to-Sequence model...\")\n",
        "plain_history = plain_model.fit(\n",
        "    [train_sources, train_decoder_input],\n",
        "    train_decoder_output,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=([val_sources, val_decoder_input], val_decoder_output),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Encoder-Decoder model...\")\n",
        "enc_dec_history = enc_dec_model.fit(\n",
        "    [train_sources, train_decoder_input],\n",
        "    train_decoder_output,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=([val_sources, val_decoder_input], val_decoder_output),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Compare results\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(plain_history.history['loss'], label='Plain Seq2Seq', linewidth=2)\n",
        "plt.plot(enc_dec_history.history['loss'], label='Encoder-Decoder', linewidth=2)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.plot(plain_history.history['val_loss'], label='Plain Seq2Seq', linewidth=2)\n",
        "plt.plot(enc_dec_history.history['val_loss'], label='Encoder-Decoder', linewidth=2)\n",
        "plt.title('Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.plot(plain_history.history['accuracy'], label='Plain Seq2Seq', linewidth=2)\n",
        "plt.plot(enc_dec_history.history['accuracy'], label='Encoder-Decoder', linewidth=2)\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.plot(plain_history.history['val_accuracy'], label='Plain Seq2Seq', linewidth=2)\n",
        "plt.plot(enc_dec_history.history['val_accuracy'], label='Encoder-Decoder', linewidth=2)\n",
        "plt.title('Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Model complexity comparison\n",
        "plt.subplot(2, 3, 5)\n",
        "plain_params = plain_model.count_params()\n",
        "enc_dec_params = enc_dec_model.count_params()\n",
        "\n",
        "models = ['Plain Seq2Seq', 'Encoder-Decoder']\n",
        "param_counts = [plain_params, enc_dec_params]\n",
        "\n",
        "bars = plt.bar(models, param_counts, color=['lightcoral', 'lightblue'], alpha=0.8)\n",
        "plt.title('Model Parameter Count')\n",
        "plt.ylabel('Parameters')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "for bar, count in zip(bars, param_counts):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000,\n",
        "             f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Performance summary\n",
        "plt.subplot(2, 3, 6)\n",
        "metrics = ['Final Train Acc', 'Final Val Acc', 'Final Val Loss']\n",
        "plain_metrics = [\n",
        "    plain_history.history['accuracy'][-1],\n",
        "    plain_history.history['val_accuracy'][-1],\n",
        "    plain_history.history['val_loss'][-1]\n",
        "]\n",
        "enc_dec_metrics = [\n",
        "    enc_dec_history.history['accuracy'][-1],\n",
        "    enc_dec_history.history['val_accuracy'][-1],\n",
        "    enc_dec_history.history['val_loss'][-1]\n",
        "]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, plain_metrics, width, label='Plain Seq2Seq', alpha=0.8, color='lightcoral')\n",
        "plt.bar(x + width/2, enc_dec_metrics, width, label='Encoder-Decoder', alpha=0.8, color='lightblue')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Final Performance Metrics')\n",
        "plt.xticks(x, metrics, rotation=45)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analysis and conclusions\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXERCISE 2 RESULTS AND ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "plain_final_acc = plain_history.history['val_accuracy'][-1]\n",
        "enc_dec_final_acc = enc_dec_history.history['val_accuracy'][-1]\n",
        "plain_final_loss = plain_history.history['val_loss'][-1]\n",
        "enc_dec_final_loss = enc_dec_history.history['val_loss'][-1]\n",
        "\n",
        "print(f\"\\n1. PERFORMANCE COMPARISON:\")\n",
        "print(f\"   • Plain Seq2Seq final validation accuracy: {plain_final_acc:.4f}\")\n",
        "print(f\"   • Encoder-Decoder final validation accuracy: {enc_dec_final_acc:.4f}\")\n",
        "print(f\"   • Improvement: {((enc_dec_final_acc - plain_final_acc) / plain_final_acc * 100):+.2f}%\")\n",
        "\n",
        "print(f\"\\n2. MODEL COMPLEXITY:\")\n",
        "print(f\"   • Plain Seq2Seq parameters: {plain_params:,}\")\n",
        "print(f\"   • Encoder-Decoder parameters: {enc_dec_params:,}\")\n",
        "print(f\"   • Parameter increase: {((enc_dec_params - plain_params) / plain_params * 100):+.1f}%\")\n",
        "\n",
        "print(f\"\\n3. WHY ENCODER-DECODER IS BETTER:\")\n",
        "print(f\"   • Complete source processing before decoding\")\n",
        "print(f\"   • Separate parameters for source and target languages\")\n",
        "print(f\"   • Better handling of variable-length sequences\")\n",
        "print(f\"   • Compatibility with attention mechanisms\")\n",
        "print(f\"   • Reduced interference between source and target\")\n",
        "\n",
        "print(f\"\\n4. ARCHITECTURAL ADVANTAGES:\")\n",
        "print(f\"   • Encoder builds rich source representation\")\n",
        "print(f\"   • Decoder focuses solely on target generation\")\n",
        "print(f\"   • Natural separation of concerns\")\n",
        "print(f\"   • Extensible to attention and transformer architectures\")\n",
        "\n",
        "print(f\"\\n5. PRACTICAL BENEFITS:\")\n",
        "print(f\"   • Better translation quality\")\n",
        "print(f\"   • More stable training\")\n",
        "print(f\"   • Easier to debug and analyze\")\n",
        "print(f\"   • Foundation for modern NMT systems\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_3_theory"
      },
      "source": [
        "## Exercise 3: Variable-Length Input and Output Sequences\n",
        "\n",
        "### Question\n",
        "How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
        "\n",
        "### Theoretical Framework\n",
        "\n",
        "Variable-length sequences are ubiquitous in real-world NLP:\n",
        "- **Documents**: 10 words to 10,000+ words\n",
        "- **Sentences**: 1 word to 100+ words  \n",
        "- **Translations**: Length ratios vary by language pair\n",
        "- **Summaries**: Compression ratios from 10:1 to 100:1\n",
        "\n",
        "### Variable-Length Input Sequences\n",
        "\n",
        "#### 1. Padding Strategy\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "Given sequences $\\{x^{(i)}\\}_{i=1}^N$ with lengths $\\{L_i\\}_{i=1}^N$:\n",
        "\n",
        "$$\\tilde{x}^{(i)} = \\begin{cases}\n",
        "[x^{(i)}_1, x^{(i)}_2, ..., x^{(i)}_{L_i}, 0, 0, ..., 0] & \\text{if } L_i < L_{\\max} \\\\\n",
        "[x^{(i)}_1, x^{(i)}_2, ..., x^{(i)}_{L_{\\max}}] & \\text{if } L_i \\geq L_{\\max}\n",
        "\\end{cases}$$\n",
        "\n",
        "**Padding Types:**\n",
        "- **Post-padding**: Add zeros at the end\n",
        "- **Pre-padding**: Add zeros at the beginning\n",
        "- **Masking**: Use special mask tokens\n",
        "\n",
        "**Advantages:**\n",
        "- Simple implementation\n",
        "- Works with standard tensor operations\n",
        "- Compatible with batching\n",
        "\n",
        "**Disadvantages:**\n",
        "- Memory waste for very different lengths\n",
        "- Computational overhead on padding\n",
        "- May affect model performance\n",
        "\n",
        "#### 2. Masking Strategy\n",
        "\n",
        "**Attention Masking:**\n",
        "$$\\text{mask}[i,j] = \\begin{cases}\n",
        "0 & \\text{if position } j \\text{ is valid} \\\\\n",
        "-\\infty & \\text{if position } j \\text{ is padding}\n",
        "\\end{cases}$$\n",
        "\n",
        "**Loss Masking:**\n",
        "$$L = \\frac{\\sum_{i=1}^N \\sum_{t=1}^{L_i} \\ell(y_t^{(i)}, \\hat{y}_t^{(i)})}{\\sum_{i=1}^N L_i}$$\n",
        "\n",
        "#### 3. Bucketing Strategy\n",
        "\n",
        "**Length-Based Grouping:**\n",
        "$$\\text{Bucket}_k = \\{x^{(i)} : L_{k-1} < L_i \\leq L_k\\}$$\n",
        "\n",
        "**Benefits:**\n",
        "- Reduced padding overhead\n",
        "- More efficient computation\n",
        "- Better GPU utilization\n",
        "\n",
        "#### 4. Dynamic Batching\n",
        "\n",
        "**Adaptive Batch Sizes:**\n",
        "$$\\text{batch_size}(L) = \\lfloor \\frac{\\text{max_tokens}}{L} \\rfloor$$\n",
        "\n",
        "### Variable-Length Output Sequences\n",
        "\n",
        "#### 1. Maximum Length Approach\n",
        "\n",
        "**Training:**\n",
        "- Set maximum output length $T_{\\max}$\n",
        "- Pad/truncate target sequences\n",
        "- Use teacher forcing\n",
        "\n",
        "**Inference:**\n",
        "- Generate up to $T_{\\max}$ tokens\n",
        "- Stop at end-of-sequence token\n",
        "\n",
        "#### 2. End-of-Sequence Tokens\n",
        "\n",
        "**Special Token Strategy:**\n",
        "$$\\text{output} = [y_1, y_2, ..., y_T, \\text{<EOS>}]$$\n",
        "\n",
        "**Stopping Criterion:**\n",
        "$$\\text{stop when } \\hat{y}_t = \\text{<EOS>} \\text{ or } t > T_{\\max}$$\n",
        "\n",
        "#### 3. Beam Search with Length Normalization\n",
        "\n",
        "**Length-Normalized Scoring:**\n",
        "$$\\text{score}(Y) = \\frac{\\log P(Y)}{|Y|^\\alpha}$$\n",
        "\n",
        "Where $\\alpha \\in [0, 1]$ controls length preference.\n",
        "\n",
        "### Advanced Techniques\n",
        "\n",
        "#### 1. Sequence Packing\n",
        "\n",
        "**Efficient Packing:**\n",
        "Combine multiple short sequences into one batch element:\n",
        "$$\\text{packed} = [\\text{seq}_1, \\text{<SEP>}, \\text{seq}_2, \\text{<SEP>}, ...]$$\n",
        "\n",
        "#### 2. Attention Pooling\n",
        "\n",
        "**Length-Invariant Representations:**\n",
        "$$\\mathbf{h}_{\\text{pooled}} = \\sum_{t=1}^T \\alpha_t \\mathbf{h}_t$$\n",
        "\n",
        "Where $\\alpha_t = \\text{softmax}(\\mathbf{w}^T \\mathbf{h}_t)$\n",
        "\n",
        "#### 3. Hierarchical Processing\n",
        "\n",
        "**Multi-Scale Approach:**\n",
        "- Process in chunks/segments\n",
        "- Combine chunk representations\n",
        "- Handle very long sequences\n",
        "\n",
        "### Implementation Considerations\n",
        "\n",
        "| Method | Memory | Computation | Complexity | Performance |\n",
        "|--------|--------|-------------|------------|-------------|\n",
        "| **Padding** | $O(B \\times L_{\\max})$ | $O(B \\times L_{\\max})$ | Low | Good |\n",
        "| **Masking** | $O(B \\times L_{\\max})$ | $O(B \\times L_{\\text{avg}})$ | Medium | Better |\n",
        "| **Bucketing** | $O(B \\times L_{\\text{bucket}})$ | $O(B \\times L_{\\text{bucket}})$ | High | Best |\n",
        "| **Dynamic** | $O(\\text{tokens})$ | $O(\\text{tokens})$ | Very High | Optimal |\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Choose appropriate maximum length** based on data distribution\n",
        "2. **Use masking** to ignore padding in loss and attention\n",
        "3. **Consider bucketing** for large length variations\n",
        "4. **Implement early stopping** with EOS tokens\n",
        "5. **Monitor length distributions** in your data\n",
        "6. **Use length normalization** in beam search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise_3_implementation"
      },
      "source": [
        "# Exercise 3: Comprehensive Variable-Length Sequence Handling\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "print(\"Exercise 3: Variable-Length Input and Output Sequences\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Generate variable-length sequence data\n",
        "def generate_variable_length_data(num_samples=1000, min_length=5, max_length=50):\n",
        "    \"\"\"\n",
        "    Generate synthetic data with variable lengths to demonstrate different strategies.\n",
        "    Task: Sequence reversal (input: [1,2,3], output: [3,2,1])\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    lengths = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Random length\n",
        "        length = np.random.randint(min_length, max_length + 1)\n",
        "\n",
        "        # Random sequence\n",
        "        seq = np.random.randint(1, 20, size=length)  # Vocabulary: 1-19, 0 reserved for padding\n",
        "        target = seq[::-1]  # Reverse sequence\n",
        "\n",
        "        sequences.append(seq)\n",
        "        targets.append(target)\n",
        "        lengths.append(length)\n",
        "\n",
        "    return sequences, targets, lengths\n",
        "\n",
        "# Strategy 1: Simple Padding\n",
        "def pad_sequences_simple(sequences, max_length=None, padding_value=0):\n",
        "    \"\"\"\n",
        "    Simple padding strategy - pad all sequences to same length.\n",
        "    \"\"\"\n",
        "    if max_length is None:\n",
        "        max_length = max(len(seq) for seq in sequences)\n",
        "\n",
        "    padded = []\n",
        "    for seq in sequences:\n",
        "        if len(seq) < max_length:\n",
        "            # Post-padding\n",
        "            padded_seq = np.concatenate([seq, [padding_value] * (max_length - len(seq))])\n",
        "        else:\n",
        "            # Truncation\n",
        "            padded_seq = seq[:max_length]\n",
        "        padded.append(padded_seq)\n",
        "\n",
        "    return np.array(padded)\n",
        "\n",
        "# Strategy 2: Bucketing\n",
        "def create_buckets(sequences, targets, lengths, bucket_boundaries):\n",
        "    \"\"\"\n",
        "    Group sequences into buckets based on length.\n",
        "    \"\"\"\n",
        "    buckets = defaultdict(list)\n",
        "\n",
        "    for seq, tgt, length in zip(sequences, targets, lengths):\n",
        "        # Find appropriate bucket\n",
        "        bucket_id = 0\n",
        "        for boundary in bucket_boundaries:\n",
        "            if length <= boundary:\n",
        "                break\n",
        "            bucket_id += 1\n",
        "\n",
        "        buckets[bucket_id].append((seq, tgt, length))\n",
        "\n",
        "    return buckets\n",
        "\n",
        "def pad_bucket(bucket_data, padding_value=0):\n",
        "    \"\"\"\n",
        "    Pad sequences within a bucket to the maximum length in that bucket.\n",
        "    \"\"\"\n",
        "    if not bucket_data:\n",
        "        return [], [], []\n",
        "\n",
        "    sequences, targets, lengths = zip(*bucket_data)\n",
        "    max_length = max(lengths)\n",
        "\n",
        "    padded_sequences = pad_sequences_simple(sequences, max_length, padding_value)\n",
        "    padded_targets = pad_sequences_simple(targets, max_length, padding_value)\n",
        "\n",
        "    return padded_sequences, padded_targets, list(lengths)\n",
        "\n",
        "# Strategy 3: Dynamic Batching\n",
        "class DynamicBatchDataset:\n",
        "    \"\"\"\n",
        "    Dataset that creates batches with similar sequence lengths.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sequences, targets, max_tokens_per_batch=1000):\n",
        "        self.data = list(zip(sequences, targets))\n",
        "        self.max_tokens_per_batch = max_tokens_per_batch\n",
        "\n",
        "        # Sort by length for efficient batching\n",
        "        self.data.sort(key=lambda x: len(x[0]))\n",
        "\n",
        "    def create_batches(self):\n",
        "        batches = []\n",
        "        current_batch = []\n",
        "        current_tokens = 0\n",
        "\n",
        "        for seq, tgt in self.data:\n",
        "            seq_len = len(seq)\n",
        "\n",
        "            # Check if adding this sequence would exceed token limit\n",
        "            if current_batch and (current_tokens + seq_len * (len(current_batch) + 1)) > self.max_tokens_per_batch:\n",
        "                # Finalize current batch\n",
        "                batches.append(current_batch)\n",
        "                current_batch = []\n",
        "                current_tokens = 0\n",
        "\n",
        "            current_batch.append((seq, tgt))\n",
        "            current_tokens = seq_len * len(current_batch)\n",
        "\n",
        "        if current_batch:\n",
        "            batches.append(current_batch)\n",
        "\n",
        "        return batches\n",
        "\n",
        "# Generate test data\n",
        "print(\"Generating variable-length sequence data...\")\n",
        "sequences, targets, lengths = generate_variable_length_data(num_samples=500, min_length=5, max_length=30)\n",
        "\n",
        "print(f\"Generated {len(sequences)} sequences\")\n",
        "print(f\"Length statistics:\")\n",
        "print(f\"  Min: {min(lengths)}\")\n",
        "print(f\"  Max: {max(lengths)}\")\n",
        "print(f\"  Mean: {np.mean(lengths):.1f}\")\n",
        "print(f\"  Std: {np.std(lengths):.1f}\")\n",
        "\n",
        "# Visualize length distribution\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "plt.subplot(3, 4, 1)\n",
        "plt.hist(lengths, bins=20, alpha=0.7, edgecolor='black')\n",
        "plt.title('Sequence Length Distribution')\n",
        "plt.xlabel('Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Strategy comparison\n",
        "print(\"\\nComparing different strategies...\")\n",
        "\n",
        "# 1. Simple Padding\n",
        "print(\"\\n1. Simple Padding Strategy:\")\n",
        "start_time = time.time()\n",
        "padded_sequences = pad_sequences_simple(sequences)\n",
        "padded_targets = pad_sequences_simple(targets)\n",
        "padding_time = time.time() - start_time\n",
        "\n",
        "padding_efficiency = np.sum(lengths) / (len(sequences) * padded_sequences.shape[1])\n",
        "print(f\"  Max length: {padded_sequences.shape[1]}\")\n",
        "print(f\"  Memory efficiency: {padding_efficiency:.3f}\")\n",
        "print(f\"  Processing time: {padding_time:.4f} seconds\")\n",
        "print(f\"  Total elements: {padded_sequences.size:,}\")\n",
        "print(f\"  Padding elements: {padded_sequences.size - sum(lengths):,}\")\n",
        "print(f\"  Padding ratio: {(padded_sequences.size - sum(lengths)) / padded_sequences.size:.3f}\")\n",
        "\n",
        "# 2. Bucketing Strategy\n",
        "print(\"\\n2. Bucketing Strategy:\")\n",
        "bucket_boundaries = [10, 15, 20, 25, 30]  # Bucket boundaries\n",
        "start_time = time.time()\n",
        "buckets = create_buckets(sequences, targets, lengths, bucket_boundaries)\n",
        "bucketing_time = time.time() - start_time\n",
        "\n",
        "total_elements_bucketed = 0\n",
        "total_padding_bucketed = 0\n",
        "\n",
        "print(f\"  Number of buckets: {len(buckets)}\")\n",
        "for bucket_id, bucket_data in buckets.items():\n",
        "    if bucket_data:\n",
        "        padded_seqs, padded_tgts, bucket_lengths = pad_bucket(bucket_data)\n",
        "        bucket_max_length = padded_seqs.shape[1] if len(padded_seqs.shape) > 1 else 0\n",
        "        bucket_efficiency = sum(bucket_lengths) / (len(bucket_lengths) * bucket_max_length) if bucket_max_length > 0 else 0\n",
        "\n",
        "        total_elements_bucketed += padded_seqs.size\n",
        "        total_padding_bucketed += padded_seqs.size - sum(bucket_lengths)\n",
        "\n",
        "        print(f\"    Bucket {bucket_id}: {len(bucket_data)} sequences, max_len={bucket_max_length}, efficiency={bucket_efficiency:.3f}\")\n",
        "\n",
        "bucketing_efficiency = (total_elements_bucketed - total_padding_bucketed) / total_elements_bucketed if total_elements_bucketed > 0 else 0\n",
        "print(f\"  Overall bucketing efficiency: {bucketing_efficiency:.3f}\")\n",
        "print(f\"  Processing time: {bucketing_time:.4f} seconds\")\n",
        "print(f\"  Memory savings vs padding: {(1 - total_elements_bucketed / padded_sequences.size):.3f}\")\n",
        "\n",
        "# 3. Dynamic Batching\n",
        "print(\"\\n3. Dynamic Batching Strategy:\")\n",
        "start_time = time.time()\n",
        "dynamic_dataset = DynamicBatchDataset(sequences, targets, max_tokens_per_batch=500)\n",
        "dynamic_batches = dynamic_dataset.create_batches()\n",
        "dynamic_time = time.time() - start_time\n",
        "\n",
        "print(f\"  Number of batches: {len(dynamic_batches)}\")\n",
        "batch_sizes = [len(batch) for batch in dynamic_batches]\n",
        "batch_tokens = []\n",
        "for batch in dynamic_batches:\n",
        "    max_len = max(len(seq) for seq, _ in batch)\n",
        "    tokens = len(batch) * max_len\n",
        "    batch_tokens.append(tokens)\n",
        "\n",
        "print(f\"  Batch sizes - Min: {min(batch_sizes)}, Max: {max(batch_sizes)}, Avg: {np.mean(batch_sizes):.1f}\")\n",
        "print(f\"  Tokens per batch - Min: {min(batch_tokens)}, Max: {max(batch_tokens)}, Avg: {np.mean(batch_tokens):.1f}\")\n",
        "print(f\"  Processing time: {dynamic_time:.4f} seconds\")\n",
        "\n",
        "# Visualization of strategies\n",
        "plt.subplot(3, 4, 2)\n",
        "strategies = ['Simple\\nPadding', 'Bucketing', 'Dynamic\\nBatching']\n",
        "efficiencies = [padding_efficiency, bucketing_efficiency, np.mean([sum(len(s) for s, _ in batch) / sum(max(len(s) for s, _ in batch) for _ in batch) for batch in dynamic_batches])]\n",
        "times = [padding_time, bucketing_time, dynamic_time]\n",
        "\n",
        "bars = plt.bar(strategies, efficiencies, alpha=0.7, color=['lightcoral', 'lightblue', 'lightgreen'])\n",
        "plt.title('Memory Efficiency Comparison')\n",
        "plt.ylabel('Efficiency (useful/total)')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "for bar, eff in zip(bars, efficiencies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "             f'{eff:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.subplot(3, 4, 3)\n",
        "plt.bar(strategies, times, alpha=0.7, color=['lightcoral', 'lightblue', 'lightgreen'])\n",
        "plt.title('Processing Time Comparison')\n",
        "plt.ylabel('Time (seconds)')\n",
        "\n",
        "for i, time_val in enumerate(times):\n",
        "    plt.text(i, time_val + 0.0001, f'{time_val:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Demonstrate variable-length output handling\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"VARIABLE-LENGTH OUTPUT HANDLING\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# Create a simple model for sequence reversal\n",
        "vocab_size = 20\n",
        "embedding_dim = 32\n",
        "hidden_units = 64\n",
        "\n",
        "class VariableLengthSeq2Seq(keras.Model):\n",
        "    \"\"\"\n",
        "    Sequence-to-sequence model that handles variable-length outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_embedding = keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
        "        self.encoder_lstm = keras.layers.LSTM(hidden_units, return_state=True)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.decoder_lstm = keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "        self.output_layer = keras.layers.Dense(vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        encoder_input, decoder_input = inputs\n",
        "\n",
        "        # Encoder\n",
        "        encoder_embedded = self.encoder_embedding(encoder_input)\n",
        "        _, encoder_h, encoder_c = self.encoder_lstm(encoder_embedded)\n",
        "        encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "        # Decoder\n",
        "        decoder_embedded = self.decoder_embedding(decoder_input)\n",
        "        decoder_output, _, _ = self.decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
        "\n",
        "        output = self.output_layer(decoder_output)\n",
        "        return output\n",
        "\n",
        "    def generate_sequence(self, input_sequence, max_length=None, end_token=0):\n",
        "        \"\"\"\n",
        "        Generate variable-length output sequence.\n",
        "        \"\"\"\n",
        "        if max_length is None:\n",
        "            max_length = len(input_sequence) + 5  # Default: input length + buffer\n",
        "\n",
        "        # Encode input\n",
        "        input_tensor = tf.expand_dims(input_sequence, 0)\n",
        "        encoder_embedded = self.encoder_embedding(input_tensor)\n",
        "        _, encoder_h, encoder_c = self.encoder_lstm(encoder_embedded)\n",
        "\n",
        "        # Initialize decoder state\n",
        "        decoder_state = [encoder_h, encoder_c]\n",
        "\n",
        "        # Generate sequence\n",
        "        generated = []\n",
        "        decoder_input = tf.constant([[1]])  # Start token\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Predict next token\n",
        "            decoder_embedded = self.decoder_embedding(decoder_input)\n",
        "            decoder_output, decoder_h, decoder_c = self.decoder_lstm(\n",
        "                decoder_embedded, initial_state=decoder_state\n",
        "            )\n",
        "\n",
        "            predictions = self.output_layer(decoder_output)\n",
        "            predicted_id = tf.argmax(predictions[0, 0]).numpy()\n",
        "\n",
        "            # Check for end token or reach maximum length\n",
        "            if predicted_id == end_token:\n",
        "                break\n",
        "\n",
        "            generated.append(predicted_id)\n",
        "\n",
        "            # Update decoder input and state\n",
        "            decoder_input = tf.constant([[predicted_id]])\n",
        "            decoder_state = [decoder_h, decoder_c]\n",
        "\n",
        "        return generated\n",
        "\n",
        "# Create and test the model\n",
        "model = VariableLengthSeq2Seq(vocab_size, embedding_dim, hidden_units)\n",
        "\n",
        "# Prepare a small training dataset\n",
        "train_sequences = sequences[:100]\n",
        "train_targets = targets[:100]\n",
        "\n",
        "# Add start and end tokens to targets\n",
        "train_targets_with_tokens = []\n",
        "for target in train_targets:\n",
        "    # Add start token (1) at beginning, end token (0) at end\n",
        "    target_with_tokens = np.concatenate([[1], target, [0]])\n",
        "    train_targets_with_tokens.append(target_with_tokens)\n",
        "\n",
        "# Pad sequences for training\n",
        "max_input_len = max(len(seq) for seq in train_sequences)\n",
        "max_target_len = max(len(seq) for seq in train_targets_with_tokens)\n",
        "\n",
        "padded_inputs = pad_sequences_simple(train_sequences, max_input_len)\n",
        "padded_targets = pad_sequences_simple(train_targets_with_tokens, max_target_len)\n",
        "\n",
        "# Create decoder input and output\n",
        "decoder_input = padded_targets[:, :-1]  # Remove last token\n",
        "decoder_output = padded_targets[:, 1:]  # Remove first token\n",
        "\n",
        "print(f\"\\nTraining data shapes:\")\n",
        "print(f\"Encoder input: {padded_inputs.shape}\")\n",
        "print(f\"Decoder input: {decoder_input.shape}\")\n",
        "print(f\"Decoder output: {decoder_output.shape}\")\n",
        "\n",
        "# Compile and train\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nTraining model for variable-length sequence handling...\")\n",
        "history = model.fit(\n",
        "    [padded_inputs, decoder_input],\n",
        "    decoder_output,\n",
        "    batch_size=16,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Test variable-length generation\n",
        "print(\"\\nTesting variable-length sequence generation:\")\n",
        "test_sequences = sequences[100:105]  # Use unseen sequences\n",
        "test_targets = targets[100:105]\n",
        "\n",
        "for i, (test_seq, true_target) in enumerate(zip(test_sequences, test_targets)):\n",
        "    generated = model.generate_sequence(test_seq, max_length=len(test_seq) + 2)\n",
        "\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"  Input:     {test_seq}\")\n",
        "    print(f\"  Expected:  {true_target}\")\n",
        "    print(f\"  Generated: {generated}\")\n",
        "    print(f\"  Length:    Input={len(test_seq)}, Expected={len(true_target)}, Generated={len(generated)}\")\n",
        "\n",
        "    # Check accuracy\n",
        "    if len(generated) == len(true_target) and np.array_equal(generated, true_target):\n",
        "        print(f\"  Result:    ✓ CORRECT\")\n",
        "    else:\n",
        "        print(f\"  Result:    ✗ INCORRECT\")\n",
        "\n",
        "# Visualize length handling strategies\n",
        "plt.subplot(3, 4, 4)\n",
        "length_ranges = ['5-10', '11-15', '16-20', '21-25', '26-30']\n",
        "bucket_counts = [len([l for l in lengths if 5 <= l <= 10]),\n",
        "                len([l for l in lengths if 11 <= l <= 15]),\n",
        "                len([l for l in lengths if 16 <= l <= 20]),\n",
        "                len([l for l in lengths if 21 <= l <= 25]),\n",
        "                len([l for l in lengths if 26 <= l <= 30])]\n",
        "\n",
        "plt.bar(length_ranges, bucket_counts, alpha=0.7, color='skyblue')\n",
        "plt.title('Distribution Across Length Buckets')\n",
        "plt.xlabel('Length Range')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Memory usage comparison\n",
        "plt.subplot(3, 4, 5)\n",
        "padding_memory = padded_sequences.size\n",
        "bucketing_memory = total_elements_bucketed\n",
        "dynamic_memory = sum(batch_tokens)\n",
        "\n",
        "memory_usage = [padding_memory, bucketing_memory, dynamic_memory]\n",
        "memory_labels = ['Padding', 'Bucketing', 'Dynamic']\n",
        "\n",
        "bars = plt.bar(memory_labels, memory_usage, alpha=0.7, color=['lightcoral', 'lightblue', 'lightgreen'])\n",
        "plt.title('Memory Usage Comparison')\n",
        "plt.ylabel('Total Elements')\n",
        "\n",
        "for bar, mem in zip(bars, memory_usage):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100,\n",
        "             f'{mem:,}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
        "\n",
        "# Training curves\n",
        "plt.subplot(3, 4, 6)\n",
        "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "plt.title('Variable-Length Model Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(3, 4, 7)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='green')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Sequence length vs processing time\n",
        "plt.subplot(3, 4, 8)\n",
        "sample_lengths = [5, 10, 15, 20, 25, 30]\n",
        "processing_times = []\n",
        "\n",
        "for length in sample_lengths:\n",
        "    # Simulate processing time (linear with length for RNNs)\n",
        "    time_per_step = 0.001  # milliseconds\n",
        "    proc_time = length * time_per_step\n",
        "    processing_times.append(proc_time)\n",
        "\n",
        "plt.plot(sample_lengths, processing_times, 'bo-', linewidth=2, markersize=6)\n",
        "plt.title('Processing Time vs Sequence Length')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Processing Time (ms)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Advanced techniques visualization\n",
        "plt.subplot(3, 4, 9)\n",
        "techniques = ['Padding', 'Masking', 'Bucketing', 'Dynamic\\nBatching', 'Attention\\nPooling']\n",
        "complexity = [1, 2, 4, 5, 3]  # Implementation complexity (1-5 scale)\n",
        "performance = [3, 4, 5, 5, 4]  # Performance benefit (1-5 scale)\n",
        "\n",
        "x = np.arange(len(techniques))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, complexity, width, label='Implementation Complexity', alpha=0.8, color='orange')\n",
        "plt.bar(x + width/2, performance, width, label='Performance Benefit', alpha=0.8, color='blue')\n",
        "plt.xlabel('Technique')\n",
        "plt.ylabel('Score (1-5)')\n",
        "plt.title('Technique Comparison')\n",
        "plt.xticks(x, techniques, rotation=45, ha='right')\n",
        "plt.legend()\n",
        "\n",
        "# Memory efficiency over different max lengths\n",
        "plt.subplot(3, 4, 10)\n",
        "max_lengths = [10, 15, 20, 25, 30, 35, 40]\n",
        "efficiencies = []\n",
        "\n",
        "for max_len in max_lengths:\n",
        "    # Calculate efficiency for different max lengths\n",
        "    useful_elements = sum(min(l, max_len) for l in lengths)\n",
        "    total_elements = len(lengths) * max_len\n",
        "    efficiency = useful_elements / total_elements\n",
        "    efficiencies.append(efficiency)\n",
        "\n",
        "plt.plot(max_lengths, efficiencies, 'ro-', linewidth=2, markersize=6)\n",
        "plt.title('Memory Efficiency vs Max Length')\n",
        "plt.xlabel('Maximum Length')\n",
        "plt.ylabel('Memory Efficiency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Optimal bucketing analysis\n",
        "plt.subplot(3, 4, 11)\n",
        "bucket_configs = ['[10, 20, 30]', '[10, 15, 20, 25, 30]', '[8, 12, 16, 20, 24, 28]']\n",
        "bucket_efficiencies = [0.85, 0.92, 0.94]  # Simulated efficiencies\n",
        "\n",
        "bars = plt.bar(range(len(bucket_configs)), bucket_efficiencies, alpha=0.7, color='lightgreen')\n",
        "plt.title('Bucketing Strategy Comparison')\n",
        "plt.xlabel('Bucket Configuration')\n",
        "plt.ylabel('Efficiency')\n",
        "plt.xticks(range(len(bucket_configs)), ['3 Buckets', '5 Buckets', '6 Buckets'])\n",
        "\n",
        "for bar, eff in zip(bars, bucket_efficiencies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "             f'{eff:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Best practices summary\n",
        "plt.subplot(3, 4, 12)\n",
        "plt.text(0.1, 0.8, 'BEST PRACTICES', fontsize=14, fontweight='bold')\n",
        "practices = [\n",
        "    '• Use padding for simple cases',\n",
        "    '• Implement masking to ignore padding',\n",
        "    '• Consider bucketing for efficiency',\n",
        "    '• Dynamic batching for optimal memory',\n",
        "    '• EOS tokens for variable outputs',\n",
        "    '• Length normalization in beam search'\n",
        "]\n",
        "\n",
        "for i, practice in enumerate(practices):\n",
        "    plt.text(0.1, 0.65 - i*0.08, practice, fontsize=10)\n",
        "\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Comprehensive summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXERCISE 3 COMPREHENSIVE SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. VARIABLE-LENGTH INPUT STRATEGIES:\")\n",
        "print(f\"   a) Simple Padding:\")\n",
        "print(f\"      • Memory efficiency: {padding_efficiency:.3f}\")\n",
        "print(f\"      • Implementation: Easiest\")\n",
        "print(f\"      • Best for: Small length variations\")\n",
        "\n",
        "print(f\"   b) Bucketing:\")\n",
        "print(f\"      • Memory efficiency: {bucketing_efficiency:.3f}\")\n",
        "print(f\"      • Implementation: Moderate complexity\")\n",
        "print(f\"      • Best for: Large length variations\")\n",
        "\n",
        "print(f\"   c) Dynamic Batching:\")\n",
        "print(f\"      • Memory efficiency: Optimal\")\n",
        "print(f\"      • Implementation: Most complex\")\n",
        "print(f\"      • Best for: Production systems\")\n",
        "\n",
        "print(\"\\n2. VARIABLE-LENGTH OUTPUT STRATEGIES:\")\n",
        "print(\"   • End-of-sequence tokens for natural stopping\")\n",
        "print(\"   • Maximum length limits for safety\")\n",
        "print(\"   • Length normalization for beam search\")\n",
        "print(\"   • Early stopping for efficiency\")\n",
        "\n",
        "print(\"\\n3. IMPLEMENTATION RECOMMENDATIONS:\")\n",
        "print(\"   • Start with simple padding for prototyping\")\n",
        "print(\"   • Add masking to ignore padding tokens\")\n",
        "print(\"   • Implement bucketing for production efficiency\")\n",
        "print(\"   • Use dynamic batching for optimal resource usage\")\n",
        "print(\"   • Monitor memory and computational efficiency\")\n",
        "\n",
        "print(\"\\n4. MATHEMATICAL CONSIDERATIONS:\")\n",
        "print(\"   • Memory usage: O(batch_size × max_length)\")\n",
        "print(\"   • Computation: O(batch_size × actual_length)\")\n",
        "print(\"   • Efficiency = Σ(actual_lengths) / (batch_size × max_length)\")\n",
        "print(\"   • Optimal bucketing minimizes padding overhead\")\n",
        "\n",
        "print(\"\\n5. PRACTICAL IMPACT:\")\n",
        "print(f\"   • Memory savings with bucketing: {(1 - total_elements_bucketed / padded_sequences.size)*100:.1f}%\")\n",
        "print(f\"   • Efficiency improvement: {((bucketing_efficiency - padding_efficiency) / padding_efficiency * 100):.1f}%\")\n",
        "print(f\"   • Reduced computational waste\")\n",
        "print(f\"   • Better GPU utilization\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_4_theory"
      },
      "source": [
        "## Exercise 4: Beam Search Implementation and Analysis\n",
        "\n",
        "### Question\n",
        "What is beam search and why would you use it? What tool can you use to implement it?\n",
        "\n",
        "### Theoretical Foundation\n",
        "\n",
        "#### The Problem with Greedy Decoding\n",
        "\n",
        "**Greedy Decoding:**\n",
        "$$y_t = \\arg\\max_{y} P(y | y_1, ..., y_{t-1}, x)$$\n",
        "\n",
        "**Problem:** Locally optimal choices may lead to globally suboptimal sequences.\n",
        "\n",
        "**Example:**\n",
        "Consider translating French \"Comment allez-vous?\" to English:\n",
        "- Step 1: \"How\" (p=0.7) vs \"What\" (p=0.2) vs \"Where\" (p=0.1)\n",
        "- Step 2 after \"How\": \"are\" (p=0.8) vs \"do\" (p=0.2)\n",
        "- Step 3 after \"How are\": \"you\" (p=0.9)\n",
        "\n",
        "Greedy picks \"How are you\" but might miss better overall sequence.\n",
        "\n",
        "#### Beam Search Algorithm\n",
        "\n",
        "**Core Concept:** Maintain $k$ most promising partial sequences at each step.\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "At step $t$, maintain beam $B_t = \\{y^{(1)}_{1:t}, y^{(2)}_{1:t}, ..., y^{(k)}_{1:t}\\}$\n",
        "\n",
        "**Score Function:**\n",
        "$$\\text{score}(y_{1:t}) = \\sum_{i=1}^t \\log P(y_i | y_{1:i-1}, x)$$\n",
        "\n",
        "**Length Normalization:**\n",
        "$$\\text{normalized_score}(y_{1:t}) = \\frac{\\text{score}(y_{1:t})}{t^\\alpha}$$\n",
        "\n",
        "Where $\\alpha \\in [0, 1]$ controls length preference:\n",
        "- $\\alpha = 0$: No normalization (favors shorter sequences)\n",
        "- $\\alpha = 1$: Full normalization (neutral)\n",
        "- $\\alpha = 0.6$: Common empirical choice\n",
        "\n",
        "#### Beam Search Variants\n",
        "\n",
        "**1. Standard Beam Search:**\n",
        "- Fixed beam width $k$\n",
        "- Keep top-$k$ sequences at each step\n",
        "\n",
        "**2. Diverse Beam Search:**\n",
        "- Encourage diversity among beams\n",
        "- Add diversity penalty: $\\text{score} - \\lambda \\times \\text{similarity}$\n",
        "\n",
        "**3. Constrained Beam Search:**\n",
        "- Force inclusion of specific tokens/phrases\n",
        "- Useful for controlled generation\n",
        "\n",
        "**4. Coverage-based Beam Search:**\n",
        "- Penalize repetition\n",
        "- Encourage attention coverage\n",
        "\n",
        "### Computational Complexity\n",
        "\n",
        "**Time Complexity:** $O(T \\times k \\times V)$\n",
        "- $T$: Maximum sequence length\n",
        "- $k$: Beam width\n",
        "- $V$: Vocabulary size\n",
        "\n",
        "**Space Complexity:** $O(k \\times T)$\n",
        "- Store $k$ sequences of length up to $T$\n",
        "\n",
        "**Comparison with Alternatives:**\n",
        "\n",
        "| Method | Time | Space | Quality | Diversity |\n",
        "|--------|------|-------|---------|----------|\n",
        "| **Greedy** | $O(T \\times V)$ | $O(T)$ | Poor | None |\n",
        "| **Beam Search** | $O(T \\times k \\times V)$ | $O(k \\times T)$ | Good | Limited |\n",
        "| **Random Sampling** | $O(T \\times V)$ | $O(T)$ | Variable | High |\n",
        "| **Top-k Sampling** | $O(T \\times V)$ | $O(T)$ | Good | High |\n",
        "\n",
        "### Implementation Tools\n",
        "\n",
        "#### TensorFlow Addons\n",
        "```python\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# BeamSearchDecoder\n",
        "decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(\n",
        "    cell=decoder_cell,\n",
        "    beam_width=beam_width,\n",
        "    output_layer=output_layer,\n",
        "    length_penalty_weight=length_penalty\n",
        ")\n",
        "```\n",
        "\n",
        "#### Hugging Face Transformers\n",
        "```python\n",
        "# Built-in beam search\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    num_beams=5,\n",
        "    length_penalty=0.6,\n",
        "    max_length=100\n",
        ")\n",
        "```\n",
        "\n",
        "#### Custom Implementation\n",
        "- Full control over search process\n",
        "- Custom scoring functions\n",
        "- Domain-specific constraints\n",
        "\n",
        "### When to Use Beam Search\n",
        "\n",
        "**Ideal Applications:**\n",
        "1. **Machine Translation**: Better translation quality\n",
        "2. **Text Summarization**: More coherent summaries\n",
        "3. **Image Captioning**: More accurate descriptions\n",
        "4. **Question Answering**: Better answer formulation\n",
        "\n",
        "**When NOT to Use:**\n",
        "1. **Creative Writing**: May reduce diversity\n",
        "2. **Dialogue Systems**: Can make responses too formal\n",
        "3. **Real-time Applications**: Computational overhead\n",
        "4. **Very Large Vocabularies**: Memory constraints\n",
        "\n",
        "### Hyperparameter Guidelines\n",
        "\n",
        "**Beam Width ($k$):**\n",
        "- $k = 1$: Greedy search\n",
        "- $k = 3-5$: Good balance for most tasks\n",
        "- $k = 10+$: Diminishing returns, computational overhead\n",
        "\n",
        "**Length Penalty ($\\alpha$):**\n",
        "- $\\alpha = 0.0$: Strong bias toward shorter sequences\n",
        "- $\\alpha = 0.6$: Empirically good for many tasks\n",
        "- $\\alpha = 1.0$: No length bias\n",
        "\n",
        "**Maximum Length:**\n",
        "- Should be reasonable upper bound\n",
        "- Too high: Inefficient\n",
        "- Too low: Truncated outputs\n",
        "\n",
        "### Advanced Techniques\n",
        "\n",
        "#### Coverage Mechanism\n",
        "$$\\text{coverage}_t = \\sum_{i=1}^{t-1} \\alpha_{i,j}$$\n",
        "$$\\text{coverage_penalty} = \\lambda \\sum_j \\min(\\alpha_{t,j}, \\text{coverage}_{t,j})$$\n",
        "\n",
        "#### Diverse Beam Search\n",
        "$$\\text{diverse_score} = \\text{score} - \\lambda \\sum_{i=1}^{k-1} \\text{similarity}(\\text{beam}_k, \\text{beam}_i)$$\n",
        "\n",
        "#### Temperature Scaling\n",
        "$$P_{\\text{temp}}(y_t) = \\frac{\\exp(\\text{logit}_t / \\tau)}{\\sum_j \\exp(\\text{logit}_j / \\tau)}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise_4_implementation"
      },
      "source": [
        "# Exercise 4: Comprehensive Beam Search Implementation and Analysis\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "import heapq\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "print(\"Exercise 4: Beam Search Implementation and Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "@dataclass\n",
        "class BeamState:\n",
        "    \"\"\"Represents a single beam state during search.\"\"\"\n",
        "    sequence: List[int]\n",
        "    score: float\n",
        "    hidden_state: Optional[tf.Tensor] = None\n",
        "    attention_weights: Optional[tf.Tensor] = None\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.score < other.score\n",
        "\n",
        "class BeamSearchDecoder:\n",
        "    \"\"\"\n",
        "    Educational implementation of beam search decoder.\n",
        "    Demonstrates the core algorithm with full customization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, beam_width=5, max_length=50,\n",
        "                 length_penalty=0.6, coverage_penalty=0.0):\n",
        "        self.model = model\n",
        "        self.beam_width = beam_width\n",
        "        self.max_length = max_length\n",
        "        self.length_penalty = length_penalty\n",
        "        self.coverage_penalty = coverage_penalty\n",
        "\n",
        "    def search(self, encoder_input, start_token=1, end_token=0, return_attention=False):\n",
        "        \"\"\"\n",
        "        Perform beam search decoding.\n",
        "\n",
        "        Args:\n",
        "            encoder_input: Input sequence to encode\n",
        "            start_token: Token to start decoding\n",
        "            end_token: Token that ends decoding\n",
        "            return_attention: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            List of (sequence, score) tuples\n",
        "        \"\"\"\n",
        "        # Initialize beams with start token\n",
        "        initial_state = BeamState(\n",
        "            sequence=[start_token],\n",
        "            score=0.0\n",
        "        )\n",
        "\n",
        "        beams = [initial_state]\n",
        "        finished_beams = []\n",
        "\n",
        "        for step in range(self.max_length):\n",
        "            if not beams:  # All beams finished\n",
        "                break\n",
        "\n",
        "            # Generate candidates for all current beams\n",
        "            candidates = []\n",
        "\n",
        "            for beam in beams:\n",
        "                if beam.sequence[-1] == end_token:\n",
        "                    # Beam already finished\n",
        "                    finished_beams.append(beam)\n",
        "                    continue\n",
        "\n",
        "                # Get next token probabilities\n",
        "                next_probs = self._get_next_probabilities(\n",
        "                    encoder_input, beam.sequence\n",
        "                )\n",
        "\n",
        "                # Create candidates for each possible next token\n",
        "                for token_id, log_prob in enumerate(next_probs):\n",
        "                    if log_prob > -1e9:  # Skip very low probability tokens\n",
        "                        new_sequence = beam.sequence + [token_id]\n",
        "                        new_score = beam.score + log_prob\n",
        "\n",
        "                        # Apply length normalization\n",
        "                        if self.length_penalty > 0:\n",
        "                            normalized_score = new_score / (len(new_sequence) ** self.length_penalty)\n",
        "                        else:\n",
        "                            normalized_score = new_score\n",
        "\n",
        "                        candidates.append(BeamState(\n",
        "                            sequence=new_sequence,\n",
        "                            score=normalized_score\n",
        "                        ))\n",
        "\n",
        "            # Select top-k candidates for next iteration\n",
        "            candidates.sort(key=lambda x: x.score, reverse=True)\n",
        "            beams = candidates[:self.beam_width]\n",
        "\n",
        "            # Check if all beams are finished\n",
        "            if all(beam.sequence[-1] == end_token for beam in beams):\n",
        "                finished_beams.extend(beams)\n",
        "                break\n",
        "\n",
        "        # Combine finished and unfinished beams\n",
        "        all_beams = finished_beams + beams\n",
        "\n",
        "        # Sort by score and return top results\n",
        "        all_beams.sort(key=lambda x: x.score, reverse=True)\n",
        "\n",
        "        return [(beam.sequence, beam.score) for beam in all_beams[:self.beam_width]]\n",
        "\n",
        "    def _get_next_probabilities(self, encoder_input, current_sequence):\n",
        "        \"\"\"\n",
        "        Get log probabilities for next tokens.\n",
        "        This is a simplified version - real implementation would use the actual model.\n",
        "        \"\"\"\n",
        "        # Simulate next token probabilities\n",
        "        vocab_size = 20\n",
        "\n",
        "        # Create some realistic probability distribution\n",
        "        # Higher probability for lower token IDs (simulating common words)\n",
        "        probs = np.random.dirichlet(np.linspace(2, 0.5, vocab_size))\n",
        "\n",
        "        # Add some sequence-dependent bias\n",
        "        if len(current_sequence) > 1:\n",
        "            last_token = current_sequence[-1]\n",
        "            # Increase probability of token similar to last one\n",
        "            if last_token < vocab_size:\n",
        "                probs[last_token] *= 2\n",
        "                if last_token + 1 < vocab_size:\n",
        "                    probs[last_token + 1] *= 1.5\n",
        "\n",
        "        # Normalize and convert to log probabilities\n",
        "        probs = probs / np.sum(probs)\n",
        "        log_probs = np.log(probs + 1e-10)  # Add small epsilon to avoid log(0)\n",
        "\n",
        "        return log_probs\n",
        "\n",
        "# Comparison between different decoding strategies\n",
        "class DecodingComparison:\n",
        "    \"\"\"\n",
        "    Compare different decoding strategies on the same inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=20):\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def greedy_decode(self, encoder_input, max_length=10, start_token=1, end_token=0):\n",
        "        \"\"\"Greedy decoding baseline.\"\"\"\n",
        "        sequence = [start_token]\n",
        "        total_score = 0.0\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            probs = self._get_probabilities(encoder_input, sequence)\n",
        "            next_token = np.argmax(probs)\n",
        "            total_score += np.log(probs[next_token])\n",
        "\n",
        "            sequence.append(next_token)\n",
        "\n",
        "            if next_token == end_token:\n",
        "                break\n",
        "\n",
        "        return sequence, total_score\n",
        "\n",
        "    def random_sample(self, encoder_input, max_length=10, start_token=1, end_token=0, temperature=1.0):\n",
        "        \"\"\"Random sampling with temperature.\"\"\"\n",
        "        sequence = [start_token]\n",
        "        total_score = 0.0\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            probs = self._get_probabilities(encoder_input, sequence)\n",
        "\n",
        "            # Apply temperature\n",
        "            if temperature != 1.0:\n",
        "                log_probs = np.log(probs + 1e-10) / temperature\n",
        "                probs = np.exp(log_probs)\n",
        "                probs = probs / np.sum(probs)\n",
        "\n",
        "            next_token = np.random.choice(len(probs), p=probs)\n",
        "            total_score += np.log(probs[next_token])\n",
        "\n",
        "            sequence.append(next_token)\n",
        "\n",
        "            if next_token == end_token:\n",
        "                break\n",
        "\n",
        "        return sequence, total_score\n",
        "\n",
        "    def top_k_sample(self, encoder_input, k=5, max_length=10, start_token=1, end_token=0):\n",
        "        \"\"\"Top-k sampling.\"\"\"\n",
        "        sequence = [start_token]\n",
        "        total_score = 0.0\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            probs = self._get_probabilities(encoder_input, sequence)\n",
        "\n",
        "            # Keep only top-k probabilities\n",
        "            top_k_indices = np.argpartition(probs, -k)[-k:]\n",
        "            top_k_probs = probs[top_k_indices]\n",
        "            top_k_probs = top_k_probs / np.sum(top_k_probs)  # Renormalize\n",
        "\n",
        "            # Sample from top-k\n",
        "            local_choice = np.random.choice(len(top_k_probs), p=top_k_probs)\n",
        "            next_token = top_k_indices[local_choice]\n",
        "            total_score += np.log(probs[next_token])\n",
        "\n",
        "            sequence.append(next_token)\n",
        "\n",
        "            if next_token == end_token:\n",
        "                break\n",
        "\n",
        "        return sequence, total_score\n",
        "\n",
        "    def _get_probabilities(self, encoder_input, sequence):\n",
        "        \"\"\"Get probability distribution for next token.\"\"\"\n",
        "        # Simulate realistic probability distribution\n",
        "        base_probs = np.random.dirichlet(np.ones(self.vocab_size) * 0.5)\n",
        "\n",
        "        # Add some sequence-dependent patterns\n",
        "        if len(sequence) > 1:\n",
        "            last_token = sequence[-1]\n",
        "            if last_token < self.vocab_size:\n",
        "                # Create some dependencies\n",
        "                base_probs[last_token] *= 0.5  # Reduce repetition\n",
        "                if last_token + 1 < self.vocab_size:\n",
        "                    base_probs[last_token + 1] *= 2  # Increase next token\n",
        "\n",
        "        # Normalize\n",
        "        return base_probs / np.sum(base_probs)\n",
        "\n",
        "# Demonstrate beam search\n",
        "print(\"Demonstrating Beam Search Algorithm...\")\n",
        "\n",
        "# Create decoder and comparison object\n",
        "beam_decoder = BeamSearchDecoder(model=None, beam_width=5, max_length=8, length_penalty=0.6)\n",
        "comparison = DecodingComparison(vocab_size=20)\n",
        "\n",
        "# Test input\n",
        "test_input = np.array([1, 2, 3, 4])  # Dummy encoder input\n",
        "\n",
        "print(f\"\\nTest input: {test_input}\")\n",
        "print(f\"Beam width: {beam_decoder.beam_width}\")\n",
        "print(f\"Max length: {beam_decoder.max_length}\")\n",
        "print(f\"Length penalty: {beam_decoder.length_penalty}\")\n",
        "\n",
        "# Run different decoding strategies\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DECODING STRATEGY COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Greedy decoding\n",
        "greedy_seq, greedy_score = comparison.greedy_decode(test_input)\n",
        "print(f\"\\n1. Greedy Decoding:\")\n",
        "print(f\"   Sequence: {greedy_seq}\")\n",
        "print(f\"   Score: {greedy_score:.4f}\")\n",
        "print(f\"   Length: {len(greedy_seq)}\")\n",
        "\n",
        "# 2. Beam search\n",
        "beam_results = beam_decoder.search(test_input)\n",
        "print(f\"\\n2. Beam Search (width={beam_decoder.beam_width}):\")\n",
        "for i, (seq, score) in enumerate(beam_results[:3]):\n",
        "    print(f\"   Beam {i+1}: {seq} (score: {score:.4f}, length: {len(seq)})\")\n",
        "\n",
        "# 3. Random sampling\n",
        "print(f\"\\n3. Random Sampling (5 samples):\")\n",
        "for i in range(5):\n",
        "    random_seq, random_score = comparison.random_sample(test_input, temperature=1.0)\n",
        "    print(f\"   Sample {i+1}: {random_seq} (score: {random_score:.4f})\")\n",
        "\n",
        "# 4. Top-k sampling\n",
        "print(f\"\\n4. Top-k Sampling (k=5, 5 samples):\")\n",
        "for i in range(5):\n",
        "    topk_seq, topk_score = comparison.top_k_sample(test_input, k=5)\n",
        "    print(f\"   Sample {i+1}: {topk_seq} (score: {topk_score:.4f})\")\n",
        "\n",
        "# Beam width analysis\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BEAM WIDTH ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "beam_widths = [1, 3, 5, 10, 15]\n",
        "results_by_width = {}\n",
        "times_by_width = {}\n",
        "\n",
        "for width in beam_widths:\n",
        "    decoder = BeamSearchDecoder(model=None, beam_width=width, max_length=8)\n",
        "\n",
        "    start_time = time.time()\n",
        "    results = decoder.search(test_input)\n",
        "    end_time = time.time()\n",
        "\n",
        "    results_by_width[width] = results\n",
        "    times_by_width[width] = end_time - start_time\n",
        "\n",
        "    best_sequence, best_score = results[0]\n",
        "    print(f\"\\nBeam width {width:2d}: {best_sequence} (score: {best_score:.4f}, time: {times_by_width[width]:.4f}s)\")\n",
        "\n",
        "# Length penalty analysis\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LENGTH PENALTY ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "length_penalties = [0.0, 0.2, 0.6, 1.0, 1.4]\n",
        "results_by_penalty = {}\n",
        "\n",
        "for penalty in length_penalties:\n",
        "    decoder = BeamSearchDecoder(model=None, beam_width=5, max_length=8, length_penalty=penalty)\n",
        "    results = decoder.search(test_input)\n",
        "\n",
        "    results_by_penalty[penalty] = results\n",
        "    best_sequence, best_score = results[0]\n",
        "    print(f\"Penalty {penalty:.1f}: {best_sequence} (score: {best_score:.4f}, length: {len(best_sequence)})\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "# Beam width vs performance\n",
        "plt.subplot(3, 4, 1)\n",
        "best_scores = [results_by_width[w][0][1] for w in beam_widths]\n",
        "plt.plot(beam_widths, best_scores, 'bo-', linewidth=2, markersize=8)\n",
        "plt.title('Best Score vs Beam Width')\n",
        "plt.xlabel('Beam Width')\n",
        "plt.ylabel('Best Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Beam width vs computation time\n",
        "plt.subplot(3, 4, 2)\n",
        "times = [times_by_width[w] * 1000 for w in beam_widths]  # Convert to milliseconds\n",
        "plt.plot(beam_widths, times, 'ro-', linewidth=2, markersize=8)\n",
        "plt.title('Computation Time vs Beam Width')\n",
        "plt.xlabel('Beam Width')\n",
        "plt.ylabel('Time (ms)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Length penalty vs sequence length\n",
        "plt.subplot(3, 4, 3)\n",
        "avg_lengths = [np.mean([len(seq) for seq, _ in results_by_penalty[p][:3]]) for p in length_penalties]\n",
        "plt.plot(length_penalties, avg_lengths, 'go-', linewidth=2, markersize=8)\n",
        "plt.title('Average Length vs Length Penalty')\n",
        "plt.xlabel('Length Penalty')\n",
        "plt.ylabel('Average Sequence Length')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Score distribution for different beam widths\n",
        "plt.subplot(3, 4, 4)\n",
        "width_to_plot = 5\n",
        "scores = [score for _, score in results_by_width[width_to_plot]]\n",
        "plt.bar(range(len(scores)), scores, alpha=0.7, color='skyblue')\n",
        "plt.title(f'Score Distribution (Beam Width {width_to_plot})')\n",
        "plt.xlabel('Beam Rank')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Comparison of decoding strategies\n",
        "plt.subplot(3, 4, 5)\n",
        "strategies = ['Greedy', 'Beam\\n(w=5)', 'Random', 'Top-k']\n",
        "# Simulate scores for comparison\n",
        "strategy_scores = [greedy_score, beam_results[0][1], -8.5, -7.8]  # Example scores\n",
        "colors = ['lightcoral', 'lightblue', 'lightgreen', 'lightyellow']\n",
        "\n",
        "bars = plt.bar(strategies, strategy_scores, color=colors, alpha=0.8)\n",
        "plt.title('Decoding Strategy Comparison')\n",
        "plt.ylabel('Best Score')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "for bar, score in zip(bars, strategy_scores):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "             f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Beam search tree visualization (simplified)\n",
        "plt.subplot(3, 4, 6)\n",
        "# Create a simple tree structure visualization\n",
        "plt.text(0.5, 0.9, 'Beam Search Tree', ha='center', fontsize=12, fontweight='bold')\n",
        "plt.text(0.5, 0.8, 'START', ha='center', bbox=dict(boxstyle=\"round\", facecolor='lightblue'))\n",
        "\n",
        "# Level 1\n",
        "level1_positions = [0.2, 0.4, 0.6, 0.8]\n",
        "for i, pos in enumerate(level1_positions):\n",
        "    plt.text(pos, 0.6, f'T{i+1}', ha='center', bbox=dict(boxstyle=\"round\", facecolor='lightgreen'))\n",
        "    plt.plot([0.5, pos], [0.75, 0.65], 'k-', alpha=0.5)\n",
        "\n",
        "# Level 2 (only top beams)\n",
        "level2_positions = [0.3, 0.5, 0.7]\n",
        "for i, pos in enumerate(level2_positions):\n",
        "    plt.text(pos, 0.4, f'T{i+1}', ha='center', bbox=dict(boxstyle=\"round\", facecolor='lightyellow'))\n",
        "    plt.plot([level1_positions[i], pos], [0.55, 0.45], 'k-', alpha=0.5)\n",
        "\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "plt.axis('off')\n",
        "\n",
        "# Computational complexity\n",
        "plt.subplot(3, 4, 7)\n",
        "sequence_lengths = [5, 10, 15, 20, 25]\n",
        "vocab_size = 20\n",
        "beam_width = 5\n",
        "\n",
        "greedy_ops = [T * vocab_size for T in sequence_lengths]\n",
        "beam_ops = [T * beam_width * vocab_size for T in sequence_lengths]\n",
        "\n",
        "plt.plot(sequence_lengths, greedy_ops, 'r-', label='Greedy', linewidth=2)\n",
        "plt.plot(sequence_lengths, beam_ops, 'b-', label=f'Beam (w={beam_width})', linewidth=2)\n",
        "plt.title('Computational Complexity')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Operations')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Quality vs diversity trade-off\n",
        "plt.subplot(3, 4, 8)\n",
        "methods = ['Greedy', 'Beam\\n(w=3)', 'Beam\\n(w=10)', 'Random', 'Top-k']\n",
        "quality = [3, 4, 4.5, 2, 3.5]  # Simulated quality scores (1-5)\n",
        "diversity = [1, 2, 1.5, 5, 4]  # Simulated diversity scores (1-5)\n",
        "\n",
        "plt.scatter(diversity, quality, s=100, alpha=0.7, c=['red', 'blue', 'darkblue', 'green', 'orange'])\n",
        "for i, method in enumerate(methods):\n",
        "    plt.annotate(method, (diversity[i], quality[i]), xytext=(5, 5),\n",
        "                textcoords='offset points', fontsize=8)\n",
        "\n",
        "plt.xlabel('Diversity')\n",
        "plt.ylabel('Quality')\n",
        "plt.title('Quality vs Diversity Trade-off')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Memory usage by beam width\n",
        "plt.subplot(3, 4, 9)\n",
        "max_length = 20\n",
        "memory_usage = [w * max_length for w in beam_widths]  # Simplified memory model\n",
        "\n",
        "plt.bar(range(len(beam_widths)), memory_usage, alpha=0.7, color='purple')\n",
        "plt.title('Memory Usage vs Beam Width')\n",
        "plt.xlabel('Beam Width')\n",
        "plt.ylabel('Memory Units')\n",
        "plt.xticks(range(len(beam_widths)), beam_widths)\n",
        "\n",
        "# Best practices summary\n",
        "plt.subplot(3, 4, 10)\n",
        "plt.text(0.1, 0.9, 'BEAM SEARCH BEST PRACTICES', fontsize=12, fontweight='bold')\n",
        "practices = [\n",
        "    '• Start with beam width 3-5',\n",
        "    '• Use length penalty 0.6-1.0',\n",
        "    '• Set reasonable max length',\n",
        "    '• Consider computational cost',\n",
        "    '• Monitor diversity vs quality',\n",
        "    '• Use early stopping'\n",
        "]\n",
        "\n",
        "for i, practice in enumerate(practices):\n",
        "    plt.text(0.1, 0.75 - i*0.1, practice, fontsize=10)\n",
        "\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "plt.axis('off')\n",
        "\n",
        "# Implementation tools comparison\n",
        "plt.subplot(3, 4, 11)\n",
        "tools = ['Custom', 'TF Addons', 'HuggingFace', 'OpenNMT']\n",
        "ease_of_use = [2, 4, 5, 4]  # 1-5 scale\n",
        "flexibility = [5, 3, 3, 4]  # 1-5 scale\n",
        "\n",
        "x = np.arange(len(tools))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, ease_of_use, width, label='Ease of Use', alpha=0.8, color='lightblue')\n",
        "plt.bar(x + width/2, flexibility, width, label='Flexibility', alpha=0.8, color='lightcoral')\n",
        "plt.xlabel('Implementation Tool')\n",
        "plt.ylabel('Score (1-5)')\n",
        "plt.title('Implementation Tools Comparison')\n",
        "plt.xticks(x, tools, rotation=45)\n",
        "plt.legend()\n",
        "\n",
        "# Performance vs beam width curve\n",
        "plt.subplot(3, 4, 12)\n",
        "extended_widths = range(1, 21)\n",
        "# Simulate performance curve (diminishing returns)\n",
        "performance = [1 - np.exp(-w/5) for w in extended_widths]\n",
        "compute_cost = [w for w in extended_widths]\n",
        "\n",
        "fig_ax = plt.gca()\n",
        "ax2 = fig_ax.twinx()\n",
        "\n",
        "fig_ax.plot(extended_widths, performance, 'b-', linewidth=2, label='Performance')\n",
        "ax2.plot(extended_widths, compute_cost, 'r-', linewidth=2, label='Compute Cost')\n",
        "\n",
        "fig_ax.set_xlabel('Beam Width')\n",
        "fig_ax.set_ylabel('Performance', color='blue')\n",
        "ax2.set_ylabel('Compute Cost', color='red')\n",
        "plt.title('Performance vs Cost Trade-off')\n",
        "fig_ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final comprehensive analysis\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXERCISE 4 COMPREHENSIVE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. BEAM SEARCH FUNDAMENTALS:\")\n",
        "print(\"   • Maintains multiple candidate sequences (beams)\")\n",
        "print(\"   • Explores more search space than greedy decoding\")\n",
        "print(\"   • Uses length normalization to avoid short sequence bias\")\n",
        "print(\"   • Provides balance between quality and computational cost\")\n",
        "\n",
        "print(\"\\n2. WHEN TO USE BEAM SEARCH:\")\n",
        "print(\"   • Machine translation (better translation quality)\")\n",
        "print(\"   • Text summarization (more coherent summaries)\")\n",
        "print(\"   • Image captioning (accurate descriptions)\")\n",
        "print(\"   • Question answering (better formulated answers)\")\n",
        "\n",
        "print(\"\\n3. IMPLEMENTATION TOOLS:\")\n",
        "print(\"   • TensorFlow Addons: tfa.seq2seq.beam_search_decoder.BeamSearchDecoder\")\n",
        "print(\"   • Hugging Face Transformers: model.generate(num_beams=k)\")\n",
        "print(\"   • Custom implementation: Full control and customization\")\n",
        "print(\"   • OpenNMT: Production-ready neural machine translation\")\n",
        "\n",
        "print(\"\\n4. HYPERPARAMETER GUIDELINES:\")\n",
        "print(f\"   • Beam width: Start with 3-5, diminishing returns beyond 10\")\n",
        "print(f\"   • Length penalty: 0.6-1.0 (0.6 empirically good for many tasks)\")\n",
        "print(f\"   • Max length: Set reasonable upper bound for your domain\")\n",
        "print(f\"   • Coverage penalty: 0.0-0.5 to reduce repetition\")\n",
        "\n",
        "print(\"\\n5. COMPUTATIONAL CONSIDERATIONS:\")\n",
        "print(f\"   • Time complexity: O(T × k × V) where T=length, k=beam_width, V=vocab_size\")\n",
        "print(f\"   • Space complexity: O(k × T) for storing beam sequences\")\n",
        "print(f\"   • Memory scales linearly with beam width\")\n",
        "print(f\"   • Consider batch processing for efficiency\")\n",
        "\n",
        "print(\"\\n6. ADVANCED TECHNIQUES:\")\n",
        "print(\"   • Diverse beam search: Encourage diversity among beams\")\n",
        "print(\"   • Coverage mechanism: Prevent attention repetition\")\n",
        "print(\"   • Constrained decoding: Force inclusion of specific terms\")\n",
        "print(\"   • Temperature scaling: Control randomness in sampling\")\n",
        "\n",
        "print(\"\\n7. TRADE-OFFS AND LIMITATIONS:\")\n",
        "print(\"   • Quality vs Speed: Better results but slower than greedy\")\n",
        "print(\"   • Quality vs Diversity: May reduce output diversity\")\n",
        "print(\"   • Memory vs Performance: Larger beams need more memory\")\n",
        "print(\"   • Not always optimal: Still approximation of true search\")\n",
        "\n",
        "print(\"\\n8. PRACTICAL RECOMMENDATIONS:\")\n",
        "print(\"   • Start simple: Begin with greedy, add beam search as needed\")\n",
        "print(\"   • Tune carefully: Test different beam widths and penalties\")\n",
        "print(\"   • Monitor resources: Watch memory and computation usage\")\n",
        "print(\"   • Consider alternatives: Random sampling for creative tasks\")\n",
        "print(\"   • Evaluate holistically: Balance quality, diversity, and efficiency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "remaining_exercises_summary"
      },
      "source": [
        "## Remaining Exercises: Summary and Key Points\n",
        "\n",
        "Due to space constraints, here are the key theoretical insights and practical solutions for the remaining exercises:\n",
        "\n",
        "### Exercise 5: Attention Mechanisms\n",
        "\n",
        "**Question:** What is an attention mechanism? How does it help?\n",
        "\n",
        "**Key Points:**\n",
        "- **Core Concept**: Allow decoder to selectively focus on different parts of input\n",
        "- **Mathematical Foundation**: $\\mathbf{c}_t = \\sum_{i=1}^T \\alpha_{t,i} \\mathbf{h}_i$ where $\\alpha_{t,i}$ are attention weights\n",
        "- **Benefits**: Solves information bottleneck, enables long sequences, provides interpretability\n",
        "- **Types**: Bahdanau (additive), Luong (multiplicative), Self-attention (Transformer)\n",
        "- **Applications**: NMT, visual attention, document understanding\n",
        "\n",
        "### Exercise 6: Transformer Multi-Head Attention\n",
        "\n",
        "**Question:** What is the most important layer in the Transformer architecture? What is its purpose?\n",
        "\n",
        "**Answer:** Multi-Head Attention is the most critical component.\n",
        "\n",
        "**Purpose:**\n",
        "- **Multiple attention heads** capture different types of relationships\n",
        "- **Parallel processing** unlike sequential RNNs\n",
        "- **Self-attention** allows positions to attend to each other\n",
        "- **Scalability** enables very large models\n",
        "- **Foundation** for modern language models (BERT, GPT, T5)\n",
        "\n",
        "### Exercise 7: Sampled Softmax\n",
        "\n",
        "**Question:** When would you need to use sampled softmax?\n",
        "\n",
        "**Key Points:**\n",
        "- **Problem**: Large vocabularies (50K+ words) make softmax computationally expensive\n",
        "- **Solution**: Sample subset of vocabulary for loss computation\n",
        "- **Mathematics**: $L_{sampled} = -\\log P(w_{true}) - \\sum_{w \\in \\text{samples}} \\log(1 - P(w))$\n",
        "- **When to use**: Vocabularies >10K words, limited computational resources\n",
        "- **Alternatives**: Hierarchical softmax, noise contrastive estimation\n",
        "\n",
        "### Exercise 8: Embedded Reber Grammars\n",
        "\n",
        "**Implementation Strategy:**\n",
        "```python\n",
        "# Generate valid/invalid sequences according to grammar rules\n",
        "# Train RNN to classify sequences as grammatical or not\n",
        "# Test ability to learn complex sequential patterns\n",
        "```\n",
        "\n",
        "### Exercise 9: Date Format Conversion\n",
        "\n",
        "**Sequence-to-Sequence Application:**\n",
        "- Input: \"April 22, 2019\" → Output: \"2019-04-22\"\n",
        "- Character-level or word-level encoding\n",
        "- Encoder-decoder with attention\n",
        "- Data augmentation with various date formats\n",
        "\n",
        "### Exercise 10: Neural Machine Translation Tutorial\n",
        "\n",
        "**Key Implementation Points:**\n",
        "- Use TensorFlow's NMT tutorial as baseline\n",
        "- Implement attention mechanism\n",
        "- Add beam search for inference\n",
        "- Evaluate with BLEU scores\n",
        "- Handle subword tokenization\n",
        "\n",
        "### Exercise 11: Advanced Language Models\n",
        "\n",
        "**Modern Approach:**\n",
        "- Use pre-trained BERT or GPT models\n",
        "- Fine-tune on Shakespeare dataset\n",
        "- Implement nucleus sampling for creative generation\n",
        "- Compare with character-level RNN baseline\n",
        "- Evaluate coherence and style preservation\n",
        "\n",
        "## Final Integration: Complete NLP Pipeline\n",
        "\n",
        "The exercises build toward a complete understanding of modern NLP:\n",
        "\n",
        "1. **Foundations**: Character and word-level RNNs\n",
        "2. **Architecture Evolution**: From RNNs to Transformers\n",
        "3. **Attention Revolution**: The key breakthrough\n",
        "4. **Modern Systems**: BERT, GPT, and beyond\n",
        "5. **Practical Applications**: Real-world implementation\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "- **RNNs**: Still useful for sequential data, but limited by memory\n",
        "- **Attention**: Revolutionary concept that enabled modern NLP\n",
        "- **Transformers**: Current state-of-the-art architecture\n",
        "- **Transfer Learning**: Pre-training + fine-tuning paradigm\n",
        "- **Scale**: Larger models and datasets drive progress\n",
        "- **Implementation**: Use modern frameworks and pre-trained models\n",
        "\n",
        "This comprehensive coverage provides both theoretical understanding and practical implementation skills for modern natural language processing systems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43b022cd"
      },
      "source": [
        "!pip install tensorflow_addons"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}