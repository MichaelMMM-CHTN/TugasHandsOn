{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# Chapter 18: Reinforcement Learning\n",
        "## Comprehensive Theory, Implementation, and Analysis\n",
        "\n",
        "**Based on \"Hands-On Machine Learning\" by Aurélien Géron**\n",
        "\n",
        "---\n",
        "\n",
        "### Table of Contents\n",
        "1. [Introduction to Reinforcement Learning](#introduction)\n",
        "2. [Policy Search](#policy-search)\n",
        "3. [OpenAI Gym Environment](#openai-gym)\n",
        "4. [Neural Network Policies](#neural-policies)\n",
        "5. [Credit Assignment Problem](#credit-assignment)\n",
        "6. [Policy Gradients (REINFORCE)](#policy-gradients)\n",
        "7. [Markov Decision Processes](#mdp)\n",
        "8. [Temporal Difference Learning](#td-learning)\n",
        "9. [Q-Learning](#q-learning)\n",
        "10. [Deep Q-Learning (DQN)](#deep-q-learning)\n",
        "11. [DQN Variants](#dqn-variants)\n",
        "12. [TF-Agents Library](#tf-agents)\n",
        "13. [Exercise Solutions](#exercises)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_cell"
      },
      "source": [
        "## Setup and Installation\n",
        "\n",
        "First, let's install all necessary dependencies for our Reinforcement Learning experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_code"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade pip\n",
        "!pip install tensorflow==2.13.0\n",
        "!pip install gym==0.21.0\n",
        "!pip install 'gym[atari]'\n",
        "!pip install tf-agents\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install imageio\n",
        "!pip install pillow\n",
        "\n",
        "# For rendering environments in Colab\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports_cell"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import gym\n",
        "from collections import deque\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For rendering in Colab\n",
        "from pyvirtualdisplay import Display\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Set up virtual display for rendering\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Gym version:\", gym.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "introduction"
      },
      "source": [
        "# 1. Introduction to Reinforcement Learning\n",
        "\n",
        "## Theoretical Foundation\n",
        "\n",
        "**Reinforcement Learning (RL)** is a paradigm of machine learning where an agent learns to make decisions by interacting with an environment. Unlike supervised learning, where we have labeled examples, or unsupervised learning, where we find patterns in unlabeled data, RL learns through trial and error, receiving rewards or penalties for actions.\n",
        "\n",
        "### Key Components of RL:\n",
        "\n",
        "1. **Agent**: The learner or decision maker\n",
        "2. **Environment**: Everything the agent interacts with\n",
        "3. **State (s)**: Current situation of the agent\n",
        "4. **Action (a)**: Choice made by the agent\n",
        "5. **Reward (r)**: Feedback from the environment\n",
        "6. **Policy (π)**: Strategy used by the agent to determine actions\n",
        "\n",
        "### Mathematical Framework:\n",
        "\n",
        "At each time step t:\n",
        "- Agent observes state $s_t$\n",
        "- Takes action $a_t$ according to policy $π(a_t|s_t)$\n",
        "- Receives reward $r_{t+1}$ and new state $s_{t+1}$\n",
        "\n",
        "**Objective**: Maximize expected cumulative reward:\n",
        "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$$\n",
        "\n",
        "Where $\\gamma \\in [0,1]$ is the discount factor.\n",
        "\n",
        "### Types of RL Problems:\n",
        "\n",
        "1. **Episodic**: Tasks with clear beginning and end (e.g., games)\n",
        "2. **Continuing**: Ongoing tasks without natural termination\n",
        "3. **Discrete**: Finite action spaces\n",
        "4. **Continuous**: Infinite action spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl_examples"
      },
      "outputs": [],
      "source": [
        "# Demonstration: Simple RL Environment\n",
        "class SimpleEnvironment:\n",
        "    \"\"\"\n",
        "    A simple grid world environment for demonstration.\n",
        "    Agent starts at (0,0) and tries to reach goal at (2,2).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.grid_size = 3\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment to initial state\"\"\"\n",
        "        self.agent_pos = [0, 0]\n",
        "        self.goal_pos = [2, 2]\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"Return current state as tuple\"\"\"\n",
        "        return tuple(self.agent_pos)\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Take action and return (new_state, reward, done)\n",
        "        Actions: 0=up, 1=right, 2=down, 3=left\n",
        "        \"\"\"\n",
        "        # Define action effects\n",
        "        actions = [[-1, 0], [0, 1], [1, 0], [0, -1]]  # up, right, down, left\n",
        "\n",
        "        # Apply action\n",
        "        new_pos = [\n",
        "            self.agent_pos[0] + actions[action][0],\n",
        "            self.agent_pos[1] + actions[action][1]\n",
        "        ]\n",
        "\n",
        "        # Check boundaries\n",
        "        new_pos[0] = max(0, min(self.grid_size - 1, new_pos[0]))\n",
        "        new_pos[1] = max(0, min(self.grid_size - 1, new_pos[1]))\n",
        "\n",
        "        self.agent_pos = new_pos\n",
        "\n",
        "        # Calculate reward\n",
        "        if self.agent_pos == self.goal_pos:\n",
        "            reward = 10  # Goal reached\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -1  # Step penalty\n",
        "            done = False\n",
        "\n",
        "        return self.get_state(), reward, done\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Visualize current state\"\"\"\n",
        "        grid = np.zeros((self.grid_size, self.grid_size))\n",
        "        grid[self.agent_pos[0], self.agent_pos[1]] = 1  # Agent\n",
        "        grid[self.goal_pos[0], self.goal_pos[1]] = 2   # Goal\n",
        "        return grid\n",
        "\n",
        "# Test the environment\n",
        "env = SimpleEnvironment()\n",
        "print(\"Initial state:\", env.get_state())\n",
        "print(\"Grid visualization:\")\n",
        "print(env.render())\n",
        "print(\"\\nTaking action 1 (right):\")\n",
        "state, reward, done = env.step(1)\n",
        "print(f\"New state: {state}, Reward: {reward}, Done: {done}\")\n",
        "print(\"Grid visualization:\")\n",
        "print(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "policy-search"
      },
      "source": [
        "# 2. Policy Search\n",
        "\n",
        "## Theoretical Foundation\n",
        "\n",
        "**Policy Search** is a family of RL algorithms that directly optimize the policy without explicitly computing value functions. The policy $π(a|s; θ)$ is parameterized by parameters $θ$.\n",
        "\n",
        "### Types of Policies:\n",
        "\n",
        "1. **Deterministic Policy**: $a = π(s)$\n",
        "2. **Stochastic Policy**: $π(a|s) = P(a|s)$\n",
        "\n",
        "### Policy Search Methods:\n",
        "\n",
        "1. **Brute Force**: Try different parameter combinations\n",
        "2. **Genetic Algorithms**: Evolve population of policies\n",
        "3. **Policy Gradients**: Use gradient ascent to optimize policy\n",
        "\n",
        "### Mathematical Formulation:\n",
        "\n",
        "**Objective Function**: Expected return\n",
        "$$J(θ) = E_{τ∼π_θ}[G(τ)]$$\n",
        "\n",
        "Where $τ$ is a trajectory and $G(τ)$ is its return.\n",
        "\n",
        "**Policy Gradient Theorem**:\n",
        "$$∇_θ J(θ) = E_{τ∼π_θ}[∇_θ \\log π_θ(a_t|s_t) G_t]$$\n",
        "\n",
        "This allows us to estimate gradients using sample trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "policy_search_demo"
      },
      "outputs": [],
      "source": [
        "# Demonstration: Simple Policy Search\n",
        "class RandomPolicy:\n",
        "    \"\"\"\n",
        "    A simple random policy for the grid world.\n",
        "    \"\"\"\n",
        "    def __init__(self, action_space=4):\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"Return random action\"\"\"\n",
        "        return np.random.randint(self.action_space)\n",
        "\n",
        "class ParameterizedPolicy:\n",
        "    \"\"\"\n",
        "    A simple parameterized policy using action probabilities.\n",
        "    \"\"\"\n",
        "    def __init__(self, action_space=4):\n",
        "        self.action_space = action_space\n",
        "        # Initialize parameters (action probabilities)\n",
        "        self.params = np.ones(action_space) / action_space\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"Sample action based on current parameters\"\"\"\n",
        "        return np.random.choice(self.action_space, p=self.params)\n",
        "\n",
        "    def update_params(self, direction, step_size=0.1):\n",
        "        \"\"\"Update parameters in given direction\"\"\"\n",
        "        self.params += step_size * direction\n",
        "        # Ensure probabilities sum to 1\n",
        "        self.params = np.abs(self.params)\n",
        "        self.params /= np.sum(self.params)\n",
        "\n",
        "def evaluate_policy(policy, env, num_episodes=100):\n",
        "    \"\"\"\n",
        "    Evaluate policy performance over multiple episodes.\n",
        "    \"\"\"\n",
        "    total_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        while steps < 50:  # Max steps to prevent infinite loops\n",
        "            action = policy.get_action(state)\n",
        "            state, reward, done = env.step(action)\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        total_rewards.append(episode_reward)\n",
        "\n",
        "    return np.mean(total_rewards), np.std(total_rewards)\n",
        "\n",
        "# Compare random vs parameterized policy\n",
        "env = SimpleEnvironment()\n",
        "\n",
        "# Random policy\n",
        "random_policy = RandomPolicy()\n",
        "random_mean, random_std = evaluate_policy(random_policy, env)\n",
        "\n",
        "# Parameterized policy (initially random)\n",
        "param_policy = ParameterizedPolicy()\n",
        "param_mean, param_std = evaluate_policy(param_policy, env)\n",
        "\n",
        "print(f\"Random Policy - Mean Reward: {random_mean:.2f} ± {random_std:.2f}\")\n",
        "print(f\"Parameterized Policy - Mean Reward: {param_mean:.2f} ± {param_std:.2f}\")\n",
        "print(f\"\\nInitial policy parameters: {param_policy.params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "openai-gym"
      },
      "source": [
        "# 3. OpenAI Gym Environment\n",
        "\n",
        "## Theoretical Foundation\n",
        "\n",
        "**OpenAI Gym** provides a standardized interface for RL environments. It offers:\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "1. **Observation Space**: Defines the format of observations\n",
        "2. **Action Space**: Defines available actions\n",
        "3. **Reward Function**: Defines the reward structure\n",
        "4. **Episode Termination**: Defines when episodes end\n",
        "\n",
        "### Standard Interface:\n",
        "\n",
        "```python\n",
        "env = gym.make('EnvName-v0')\n",
        "observation = env.reset()\n",
        "for t in range(1000):\n",
        "    action = env.action_space.sample()  # Random action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        break\n",
        "```\n",
        "\n",
        "### CartPole Environment:\n",
        "\n",
        "**State Space**: 4-dimensional continuous\n",
        "- Position of cart: $x \\in (-4.8, 4.8)$\n",
        "- Velocity of cart: $\\dot{x} \\in (-∞, ∞)$\n",
        "- Angle of pole: $θ \\in (-24°, 24°)$\n",
        "- Angular velocity: $\\dot{θ} \\in (-∞, ∞)$\n",
        "\n",
        "**Action Space**: Discrete\n",
        "- 0: Push cart to the left\n",
        "- 1: Push cart to the right\n",
        "\n",
        "**Reward**: +1 for every step the pole remains upright\n",
        "\n",
        "**Termination**: Pole angle > 15° or cart position > 2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gym_demo"
      },
      "outputs": [],
      "source": [
        "# Comprehensive OpenAI Gym demonstration\n",
        "import gym\n",
        "\n",
        "# Create CartPole environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "print(\"Environment: CartPole-v1\")\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Action space sample: {env.action_space.sample()}\")\n",
        "\n",
        "# Analyze observation space\n",
        "obs = env.reset()\n",
        "print(f\"\\nInitial observation: {obs}\")\n",
        "print(\"Observation components:\")\n",
        "print(f\"  Cart Position: {obs[0]:.4f}\")\n",
        "print(f\"  Cart Velocity: {obs[1]:.4f}\")\n",
        "print(f\"  Pole Angle: {obs[2]:.4f} rad ({np.degrees(obs[2]):.2f}°)\")\n",
        "print(f\"  Pole Angular Velocity: {obs[3]:.4f}\")\n",
        "\n",
        "# Run random policy\n",
        "def run_random_episode(env, render=False):\n",
        "    \"\"\"\n",
        "    Run one episode with random actions.\n",
        "    \"\"\"\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    observations = [obs]\n",
        "\n",
        "    while True:\n",
        "        if render:\n",
        "            env.render()\n",
        "\n",
        "        action = env.action_space.sample()\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        observations.append(obs)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    if render:\n",
        "        env.close()\n",
        "\n",
        "    return total_reward, steps, observations\n",
        "\n",
        "# Run multiple random episodes and analyze\n",
        "random_scores = []\n",
        "random_steps = []\n",
        "\n",
        "for i in range(100):\n",
        "    score, steps, _ = run_random_episode(env)\n",
        "    random_scores.append(score)\n",
        "    random_steps.append(steps)\n",
        "\n",
        "print(f\"\\nRandom Policy Performance (100 episodes):\")\n",
        "print(f\"Mean Score: {np.mean(random_scores):.2f} ± {np.std(random_scores):.2f}\")\n",
        "print(f\"Max Score: {np.max(random_scores)}\")\n",
        "print(f\"Min Score: {np.min(random_scores)}\")\n",
        "print(f\"Mean Steps: {np.mean(random_steps):.2f} ± {np.std(random_steps):.2f}\")\n",
        "\n",
        "# Visualize score distribution\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(random_scores, bins=20, alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Episode Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Random Policy Score Distribution')\n",
        "plt.axvline(np.mean(random_scores), color='red', linestyle='--', label=f'Mean: {np.mean(random_scores):.1f}')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(random_scores[:50], alpha=0.7)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Random Policy Performance Over Time')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neural-policies"
      },
      "source": [
        "# 4. Neural Network Policies\n",
        "\n",
        "## Theoretical Foundation\n",
        "\n",
        "**Neural Network Policies** use deep learning to approximate the policy function. Instead of simple parameterized policies, we use neural networks to map states to action probabilities.\n",
        "\n",
        "### Architecture Design:\n",
        "\n",
        "For **discrete action spaces**:\n",
        "$$π_θ(a|s) = \\frac{\\exp(f_θ(s)_a)}{\\sum_{a'} \\exp(f_θ(s)_{a'})}$$\n",
        "\n",
        "Where $f_θ(s)$ is the neural network output (logits).\n",
        "\n",
        "For **continuous action spaces**:\n",
        "$$π_θ(a|s) = \\mathcal{N}(μ_θ(s), σ_θ(s))$$\n",
        "\n",
        "Where $μ_θ(s)$ and $σ_θ(s)$ are network outputs for mean and variance.\n",
        "\n",
        "### Key Considerations:\n",
        "\n",
        "1. **Exploration vs Exploitation**: Stochastic policies naturally explore\n",
        "2. **Network Architecture**: Depends on state representation\n",
        "3. **Output Layer**: Softmax for discrete, Gaussian for continuous\n",
        "4. **Training Stability**: Requires careful hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neural_policy_demo"
      },
      "outputs": [],
      "source": [
        "# Neural Network Policy Implementation\n",
        "class NeuralNetworkPolicy:\n",
        "    \"\"\"\n",
        "    A neural network policy for CartPole environment.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size=4, action_size=2, hidden_size=64):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Build neural network\n",
        "        self.model = self._build_model(hidden_size)\n",
        "\n",
        "    def _build_model(self, hidden_size):\n",
        "        \"\"\"\n",
        "        Build the neural network architecture.\n",
        "        \"\"\"\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Dense(hidden_size, activation='relu', input_shape=(self.state_size,)),\n",
        "            keras.layers.Dense(hidden_size, activation='relu'),\n",
        "            keras.layers.Dense(self.action_size, activation='softmax')  # Probability distribution\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Get action probabilities and sample an action.\n",
        "        \"\"\"\n",
        "        state = np.array([state])  # Add batch dimension\n",
        "        action_probs = self.model.predict(state, verbose=0)[0]\n",
        "        action = np.random.choice(self.action_size, p=action_probs)\n",
        "        return action, action_probs\n",
        "\n",
        "    def get_action_deterministic(self, state):\n",
        "        \"\"\"\n",
        "        Get action deterministically (for evaluation).\n",
        "        \"\"\"\n",
        "        state = np.array([state])\n",
        "        action_probs = self.model.predict(state, verbose=0)[0]\n",
        "        action = np.argmax(action_probs)\n",
        "        return action, action_probs\n",
        "\n",
        "# Create and test neural network policy\n",
        "env = gym.make('CartPole-v1')\n",
        "nn_policy = NeuralNetworkPolicy()\n",
        "\n",
        "print(\"Neural Network Policy Architecture:\")\n",
        "nn_policy.model.summary()\n",
        "\n",
        "# Test policy behavior\n",
        "state = env.reset()\n",
        "print(f\"\\nInitial state: {state}\")\n",
        "\n",
        "action, probs = nn_policy.get_action(state)\n",
        "print(f\"Action probabilities: {probs}\")\n",
        "print(f\"Selected action: {action}\")\n",
        "\n",
        "# Analyze policy behavior over multiple states\n",
        "states = []\n",
        "actions = []\n",
        "probabilities = []\n",
        "\n",
        "for _ in range(100):\n",
        "    state = env.reset()\n",
        "    action, probs = nn_policy.get_action(state)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    probabilities.append(probs)\n",
        "\n",
        "states = np.array(states)\n",
        "actions = np.array(actions)\n",
        "probabilities = np.array(probabilities)\n",
        "\n",
        "# Visualize policy behavior\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot action probabilities vs cart position\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.scatter(states[:, 0], probabilities[:, 0], alpha=0.6, label='Left (Action 0)')\n",
        "plt.scatter(states[:, 0], probabilities[:, 1], alpha=0.6, label='Right (Action 1)')\n",
        "plt.xlabel('Cart Position')\n",
        "plt.ylabel('Action Probability')\n",
        "plt.title('Action Probs vs Cart Position')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot action probabilities vs pole angle\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.scatter(states[:, 2], probabilities[:, 0], alpha=0.6, label='Left (Action 0)')\n",
        "plt.scatter(states[:, 2], probabilities[:, 1], alpha=0.6, label='Right (Action 1)')\n",
        "plt.xlabel('Pole Angle (rad)')\n",
        "plt.ylabel('Action Probability')\n",
        "plt.title('Action Probs vs Pole Angle')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Action distribution\n",
        "plt.subplot(2, 3, 3)\n",
        "action_counts = np.bincount(actions)\n",
        "plt.bar(['Left (0)', 'Right (1)'], action_counts)\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Action Distribution')\n",
        "\n",
        "# State distribution\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.hist(states[:, 0], bins=20, alpha=0.7, label='Cart Position')\n",
        "plt.xlabel('Cart Position')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Cart Position Distribution')\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.hist(states[:, 2], bins=20, alpha=0.7, label='Pole Angle')\n",
        "plt.xlabel('Pole Angle (rad)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Pole Angle Distribution')\n",
        "\n",
        "# Probability distribution analysis\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.hist(probabilities[:, 0], bins=20, alpha=0.7, label='P(Left)')\n",
        "plt.hist(probabilities[:, 1], bins=20, alpha=0.7, label='P(Right)')\n",
        "plt.xlabel('Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Action Probability Distribution')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nMean action probabilities:\")\n",
        "print(f\"P(Left): {np.mean(probabilities[:, 0]):.3f}\")\n",
        "print(f\"P(Right): {np.mean(probabilities[:, 1]):.3f}\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "credit-assignment"
      },
      "source": [
        "# 5. Evaluating Actions: The Credit Assignment Problem\n",
        "\n",
        "## Theoretical Foundation\n",
        "\n",
        "The **Credit Assignment Problem** is fundamental in RL: when an agent receives a reward, which previous actions deserve credit (or blame)?\n",
        "\n",
        "### Key Challenges:\n",
        "\n",
        "1. **Temporal Credit Assignment**: Which actions in a sequence caused the outcome?\n",
        "2. **Delayed Rewards**: Rewards often come long after the relevant actions\n",
        "3. **Sparse Rewards**: Many environments provide infrequent feedback\n",
        "\n",
        "### Solutions:\n",
        "\n",
        "#### 1. Discounted Returns\n",
        "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$$\n",
        "\n",
        "Where $\\gamma \\in [0,1]$ is the discount factor:\n",
        "- $\\gamma = 0$: Only immediate rewards matter\n",
        "- $\\gamma = 1$: All future rewards equally important\n",
        "- $\\gamma \\in (0,1)$: Balance between immediate and future rewards\n",
        "\n",
        "#### 2. Advantage Function\n",
        "$$A(s,a) = Q(s,a) - V(s)$$\n",
        "\n",
        "Measures how much better action $a$ is compared to the average.\n",
        "\n",
        "#### 3. Temporal Difference Error\n",
        "$$\\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)$$\n",
        "\n",
        "Measures prediction error for immediate learning.\n",
        "\n",
        "### Baseline Subtraction:\n",
        "\n",
        "To reduce variance, we subtract a baseline $b(s)$:\n",
        "$$∇_θ J(θ) = E[∇_θ \\log π_θ(a_t|s_t) (G_t - b(s_t))]$$\n",
        "\n",
        "Common baselines:\n",
        "- Mean return\n",
        "- State value function $V(s)$\n",
        "- Moving average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "credit_assignment_demo"
      },
      "outputs": [],
      "source": [
        "# Credit Assignment Problem Demonstration\n",
        "\n",
        "def compute_returns(rewards, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Compute discounted returns for a trajectory.\n",
        "\n",
        "    Args:\n",
        "        rewards: List of rewards [r1, r2, ..., rT]\n",
        "        gamma: Discount factor\n",
        "\n",
        "    Returns:\n",
        "        returns: List of discounted returns [G1, G2, ..., GT]\n",
        "    \"\"\"\n",
        "    returns = []\n",
        "    G = 0\n",
        "\n",
        "    # Compute returns backwards\n",
        "    for reward in reversed(rewards):\n",
        "        G = reward + gamma * G\n",
        "        returns.insert(0, G)\n",
        "\n",
        "    return returns\n",
        "\n",
        "def normalize_returns(returns):\n",
        "    \"\"\"\n",
        "    Normalize returns to have zero mean and unit variance.\n",
        "    \"\"\"\n",
        "    returns = np.array(returns)\n",
        "    return (returns - np.mean(returns)) / (np.std(returns) + 1e-8)\n",
        "\n",
        "# Example trajectory with different reward patterns\n",
        "# Scenario 1: Immediate reward\n",
        "rewards_immediate = [1, 0, 0, 0, 0]\n",
        "\n",
        "# Scenario 2: Delayed reward\n",
        "rewards_delayed = [0, 0, 0, 0, 1]\n",
        "\n",
        "# Scenario 3: Mixed rewards\n",
        "rewards_mixed = [0.1, -0.1, 0.2, -0.1, 0.5]\n",
        "\n",
        "# Scenario 4: Sparse rewards\n",
        "rewards_sparse = [0, 0, 1, 0, 0, 0, 0, 2, 0, 0]\n",
        "\n",
        "scenarios = {\n",
        "    'Immediate Reward': rewards_immediate,\n",
        "    'Delayed Reward': rewards_delayed,\n",
        "    'Mixed Rewards': rewards_mixed,\n",
        "    'Sparse Rewards': rewards_sparse\n",
        "}\n",
        "\n",
        "# Analyze different discount factors\n",
        "gammas = [0.0, 0.5, 0.9, 0.99]\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "for i, (scenario_name, rewards) in enumerate(scenarios.items()):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "\n",
        "    for gamma in gammas:\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "        plt.plot(returns, marker='o', label=f'γ={gamma}', linewidth=2)\n",
        "\n",
        "    plt.title(f'{scenario_name}\\nRewards: {rewards}')\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Return (G_t)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Demonstrate advantage computation\n",
        "def demonstrate_advantage_computation():\n",
        "    \"\"\"\n",
        "    Demonstrate how advantage computation helps with credit assignment.\n",
        "    \"\"\"\n",
        "    # Simulate multiple episodes\n",
        "    np.random.seed(42)\n",
        "    n_episodes = 1000\n",
        "    episode_returns = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        # Random episode length\n",
        "        length = np.random.randint(5, 20)\n",
        "        # Random rewards (some positive, some negative)\n",
        "        rewards = np.random.normal(0, 1, length)\n",
        "        # Add occasional large reward\n",
        "        if np.random.random() < 0.1:\n",
        "            rewards[-1] += 10\n",
        "\n",
        "        returns = compute_returns(rewards, gamma=0.99)\n",
        "        episode_returns.extend(returns)\n",
        "\n",
        "    # Compute baseline (mean return)\n",
        "    baseline = np.mean(episode_returns)\n",
        "\n",
        "    # Compute advantages\n",
        "    advantages = np.array(episode_returns) - baseline\n",
        "\n",
        "    print(f\"Return Statistics:\")\n",
        "    print(f\"  Mean: {np.mean(episode_returns):.3f}\")\n",
        "    print(f\"  Std: {np.std(episode_returns):.3f}\")\n",
        "    print(f\"  Min: {np.min(episode_returns):.3f}\")\n",
        "    print(f\"  Max: {np.max(episode_returns):.3f}\")\n",
        "\n",
        "    print(f\"\\nAdvantage Statistics:\")\n",
        "    print(f\"  Mean: {np.mean(advantages):.3f}\")\n",
        "    print(f\"  Std: {np.std(advantages):.3f}\")\n",
        "    print(f\"  Min: {np.min(advantages):.3f}\")\n",
        "    print(f\"  Max: {np.max(advantages):.3f}\")\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.hist(episode_returns, bins=50, alpha=0.7, edgecolor='black')\n",
        "    plt.axvline(baseline, color='red', linestyle='--', label=f'Baseline: {baseline:.2f}')\n",
        "    plt.xlabel('Return')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Return Distribution')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.hist(advantages, bins=50, alpha=0.7, edgecolor='black')\n",
        "    plt.axvline(0, color='red', linestyle='--', label='Zero Advantage')\n",
        "    plt.xlabel('Advantage')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Advantage Distribution')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.scatter(episode_returns[:1000], advantages[:1000], alpha=0.5)\n",
        "    plt.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
        "    plt.axvline(baseline, color='red', linestyle='--', alpha=0.5)\n",
        "    plt.xlabel('Return')\n",
        "    plt.ylabel('Advantage')\n",
        "    plt.title('Returns vs Advantages')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return episode_returns, advantages, baseline\n",
        "\n",
        "returns, advantages, baseline = demonstrate_advantage_computation()\n",
        "\n",
        "# Show variance reduction effect\n",
        "original_variance = np.var(returns)\n",
        "advantage_variance = np.var(advantages)\n",
        "variance_reduction = (original_variance - advantage_variance) / original_variance * 100\n",
        "\n",
        "print(f\"\\nVariance Reduction Analysis:\")\n",
        "print(f\"Original variance: {original_variance:.3f}\")\n",
        "print(f\"Advantage variance: {advantage_variance:.3f}\")\n",
        "print(f\"Variance reduction: {variance_reduction:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "policy-gradients"
      },
      "source": [
        "# 6. Policy Gradients (REINFORCE Algorithm)\n",
        "\n",
        "## Theoretical Foundation\n",
        "\n",
        "**REINFORCE** is a monte-carlo policy gradient algorithm that directly optimizes the policy using gradient ascent.\n",
        "\n",
        "### Mathematical Derivation:\n",
        "\n",
        "**Objective**: Maximize expected return\n",
        "$$J(θ) = E_{τ∼π_θ}[G(τ)]$$\n",
        "\n",
        "**Policy Gradient Theorem**:\n",
        "$$∇_θ J(θ) = E_{τ∼π_θ}[∇_θ \\log π_θ(a_t|s_t) G_t]$$\n",
        "\n",
        "**REINFORCE Update Rule**:\n",
        "$$θ_{t+1} = θ_t + α ∇_θ \\log π_θ(a_t|s_t) G_t$$\n",
        "\n",
        "### Algorithm Steps:\n",
        "\n",
        "1. **Sample** trajectories using current policy $π_θ$\n",
        "2. **Compute** returns $G_t$ for each time step\n",
        "3. **Estimate** policy gradient using samples\n",
        "4. **Update** policy parameters using gradient ascent\n",
        "\n",
        "### Key Properties:\n",
        "\n",
        "- **Unbiased**: Gradient estimates are unbiased\n",
        "- **High Variance**: Monte Carlo estimates have high variance\n",
        "- **Model-Free**: Doesn't require environment model\n",
        "- **On-Policy**: Uses samples from current policy\n",
        "\n",
        "### Variance Reduction Techniques:\n",
        "\n",
        "1. **Baseline Subtraction**: $G_t - b(s_t)$\n",
        "2. **Advantage Function**: $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$\n",
        "3. **Causality**: Only use future rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reinforce_implementation"
      },
      "outputs": [],
      "source": [
        "# Complete REINFORCE Algorithm Implementation\n",
        "class REINFORCEAgent:\n",
        "    \"\"\"\n",
        "    REINFORCE algorithm implementation for CartPole.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size=4, action_size=2, lr=0.01, gamma=0.99):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Build policy network\n",
        "        self.policy_network = self._build_policy_network()\n",
        "        self.optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "        # Storage for episode data\n",
        "        self.reset_episode_data()\n",
        "\n",
        "        # Training history\n",
        "        self.episode_rewards = []\n",
        "        self.episode_lengths = []\n",
        "        self.losses = []\n",
        "\n",
        "    def _build_policy_network(self):\n",
        "        \"\"\"\n",
        "        Build the policy neural network.\n",
        "        \"\"\"\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Dense(128, activation='relu', input_shape=(self.state_size,)),\n",
        "            keras.layers.Dense(128, activation='relu'),\n",
        "            keras.layers.Dense(self.action_size, activation='softmax')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def reset_episode_data(self):\n",
        "        \"\"\"\n",
        "        Reset storage for new episode.\n",
        "        \"\"\"\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Sample action from policy and store data.\n",
        "        \"\"\"\n",
        "        state = np.array([state])\n",
        "        action_probs = self.policy_network(state)[0]\n",
        "        action = np.random.choice(self.action_size, p=action_probs.numpy())\n",
        "\n",
        "        # Store for training\n",
        "        self.states.append(state[0])\n",
        "        self.actions.append(action)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def store_reward(self, reward):\n",
        "        \"\"\"\n",
        "        Store reward for current episode.\n",
        "        \"\"\"\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "    def compute_returns(self, rewards, gamma):\n",
        "        \"\"\"\n",
        "        Compute discounted returns.\n",
        "        \"\"\"\n",
        "        returns = []\n",
        "        G = 0\n",
        "\n",
        "        for reward in reversed(rewards):\n",
        "            G = reward + gamma * G\n",
        "            returns.insert(0, G)\n",
        "\n",
        "        return returns\n",
        "\n",
        "    def train_episode(self, use_baseline=True):\n",
        "        \"\"\"\n",
        "        Train on collected episode data using REINFORCE.\n",
        "        \"\"\"\n",
        "        # Compute returns\n",
        "        returns = self.compute_returns(self.rewards, self.gamma)\n",
        "        returns = np.array(returns, dtype=np.float32)\n",
        "\n",
        "        # Baseline subtraction for variance reduction\n",
        "        if use_baseline:\n",
        "            baseline = np.mean(returns)\n",
        "            advantages = returns - baseline\n",
        "        else:\n",
        "            advantages = returns\n",
        "\n",
        "        # Normalize advantages\n",
        "        if len(advantages) > 1:\n",
        "            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n",
        "\n",
        "        # Convert to tensors\n",
        "        states = tf.convert_to_tensor(self.states, dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(self.actions, dtype=tf.int32)\n",
        "        advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
        "\n",
        "        # Compute policy gradient and update\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass\n",
        "            action_probs = self.policy_network(states)\n",
        "\n",
        "            # Compute log probabilities of taken actions\n",
        "            action_one_hot = tf.one_hot(actions, self.action_size)\n",
        "            log_probs = tf.reduce_sum(action_one_hot * tf.math.log(action_probs + 1e-8), axis=1)\n",
        "\n",
        "            # Policy gradient loss (negative because we want to maximize)\n",
        "            loss = -tf.reduce_mean(log_probs * advantages)\n",
        "\n",
        "        # Compute gradients and update\n",
        "        gradients = tape.gradient(loss, self.policy_network.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.policy_network.trainable_variables))\n",
        "\n",
        "        # Store training metrics\n",
        "        self.episode_rewards.append(sum(self.rewards))\n",
        "        self.episode_lengths.append(len(self.rewards))\n",
        "        self.losses.append(loss.numpy())\n",
        "\n",
        "        return loss.numpy()\n",
        "\n",
        "    def get_action_deterministic(self, state):\n",
        "        \"\"\"\n",
        "        Get action deterministically for evaluation.\n",
        "        \"\"\"\n",
        "        state = np.array([state])\n",
        "        action_probs = self.policy_network(state)[0]\n",
        "        return np.argmax(action_probs.numpy())\n",
        "\n",
        "# Training function\n",
        "def train_reinforce_agent(agent, env, num_episodes=1000, max_steps=500,\n",
        "                         render_freq=None, eval_freq=100):\n",
        "    \"\"\"\n",
        "    Train REINFORCE agent.\n",
        "    \"\"\"\n",
        "    training_scores = []\n",
        "    evaluation_scores = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        # Reset for new episode\n",
        "        agent.reset_episode_data()\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        # Run episode\n",
        "        for step in range(max_steps):\n",
        "            action = agent.get_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.store_reward(reward)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Train on episode\n",
        "        loss = agent.train_episode()\n",
        "        training_scores.append(total_reward)\n",
        "\n",
        "        # Evaluation\n",
        "        if episode % eval_freq == 0:\n",
        "            eval_score = evaluate_policy(agent, env, num_episodes=5)\n",
        "            evaluation_scores.append(eval_score)\n",
        "\n",
        "            print(f\"Episode {episode}: \"\n",
        "                  f\"Training Score: {total_reward:.1f}, \"\n",
        "                  f\"Eval Score: {eval_score:.1f}, \"\n",
        "                  f\"Loss: {loss:.4f}\")\n",
        "\n",
        "    return training_scores, evaluation_scores\n",
        "\n",
        "def evaluate_policy(agent, env, num_episodes=10, max_steps=500):\n",
        "    \"\"\"\n",
        "    Evaluate agent policy deterministically.\n",
        "    \"\"\"\n",
        "    total_rewards = []\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for _ in range(max_steps):\n",
        "            action = agent.get_action_deterministic(state)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        total_rewards.append(episode_reward)\n",
        "\n",
        "    return np.mean(total_rewards)\n",
        "\n",
        "# Create environment and agent\n",
        "env = gym.make('CartPole-v1')\n",
        "agent = REINFORCEAgent(lr=0.001, gamma=0.99)\n",
        "\n",
        "print(\"REINFORCE Agent Architecture:\")\n",
        "agent.policy_network.summary()\n",
        "\n",
        "# Train the agent\n",
        "print(\"\\nTraining REINFORCE Agent...\")\n",
        "training_scores, eval_scores = train_reinforce_agent(\n",
        "    agent, env, num_episodes=500, eval_freq=50\n",
        ")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reinforce_analysis"
      },
      "outputs": [],
      "source": [
        "# Analyze REINFORCE training results\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Training progress\n",
        "plt.subplot(2, 3, 1)\n",
        "window_size = 50\n",
        "moving_avg = pd.Series(training_scores).rolling(window_size).mean()\n",
        "plt.plot(training_scores, alpha=0.3, color='blue', label='Episode Score')\n",
        "plt.plot(moving_avg, color='red', linewidth=2, label=f'{window_size}-Episode Average')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Training Progress')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Evaluation scores\n",
        "plt.subplot(2, 3, 2)\n",
        "eval_episodes = list(range(0, len(training_scores), 50))\n",
        "plt.plot(eval_episodes, eval_scores, 'o-', color='green', linewidth=2)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Evaluation Score')\n",
        "plt.title('Evaluation Progress')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Loss progression\n",
        "plt.subplot(2, 3, 3)\n",
        "loss_moving_avg = pd.Series(agent.losses).rolling(window_size).mean()\n",
        "plt.plot(agent.losses, alpha=0.3, color='orange', label='Loss')\n",
        "plt.plot(loss_moving_avg, color='red', linewidth=2, label=f'{window_size}-Episode Average')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Policy Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Episode length progression\n",
        "plt.subplot(2, 3, 4)\n",
        "length_moving_avg = pd.Series(agent.episode_lengths).rolling(window_size).mean()\n",
        "plt.plot(agent.episode_lengths, alpha=0.3, color='purple', label='Episode Length')\n",
        "plt.plot(length_moving_avg, color='red', linewidth=2, label=f'{window_size}-Episode Average')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Episode Length')\n",
        "plt.title('Episode Length Progress')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Score distribution over time\n",
        "plt.subplot(2, 3, 5)\n",
        "early_scores = training_scores[:100]\n",
        "late_scores = training_scores[-100:]\n",
        "plt.hist(early_scores, bins=20, alpha=0.5, label='Early Training (0-100)', color='red')\n",
        "plt.hist(late_scores, bins=20, alpha=0.5, label='Late Training (-100:-1)', color='blue')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Score Distribution Comparison')\n",
        "plt.legend()\n",
        "\n",
        "# Learning curve analysis\n",
        "plt.subplot(2, 3, 6)\n",
        "segment_size = 50\n",
        "segments = len(training_scores) // segment_size\n",
        "segment_means = []\n",
        "segment_stds = []\n",
        "\n",
        "for i in range(segments):\n",
        "    start_idx = i * segment_size\n",
        "    end_idx = (i + 1) * segment_size\n",
        "    segment_scores = training_scores[start_idx:end_idx]\n",
        "    segment_means.append(np.mean(segment_scores))\n",
        "    segment_stds.append(np.std(segment_scores))\n",
        "\n",
        "segment_episodes = [(i + 0.5) * segment_size for i in range(segments)]\n",
        "plt.errorbar(segment_episodes, segment_means, yerr=segment_stds,\n",
        "             fmt='o-', capsize=5, capthick=2)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean Score ± Std')\n",
        "plt.title('Learning Curve with Variance')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"Training Summary:\")\n",
        "print(f\"Total Episodes: {len(training_scores)}\")\n",
        "print(f\"Final 100 Episode Average: {np.mean(training_scores[-100:]):.2f} ± {np.std(training_scores[-100:]):.2f}\")\n",
        "print(f\"Best Episode Score: {np.max(training_scores)}\")\n",
        "print(f\"Final Evaluation Score: {eval_scores[-1]:.2f}\")\n",
        "print(f\"Training Improvement: {np.mean(training_scores[-100:]) - np.mean(training_scores[:100]):.2f}\")\n",
        "\n",
        "# Test final policy\n",
        "print(\"\\nTesting Final Policy:\")\n",
        "env = gym.make('CartPole-v1')\n",
        "final_score = evaluate_policy(agent, env, num_episodes=20)\n",
        "print(f\"Final Policy Average Score (20 episodes): {final_score:.2f}\")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdp"
      },
      "source": [
        "# 7. Markov Decision Processes (MDPs)\n",
        "\n",
        "## Theoretical Foundation\n",
        "\n",
        "A **Markov Decision Process** is a mathematical framework for modeling decision-making problems in stochastic environments.\n",
        "\n",
        "### MDP Components:\n",
        "\n",
        "An MDP is defined by the tuple $(S, A, P, R, γ)$:\n",
        "\n",
        "- **S**: Set of states\n",
        "- **A**: Set of actions\n",
        "- **P**: Transition probabilities $P(s'|s,a)$\n",
        "- **R**: Reward function $R(s,a,s')$\n",
        "- **γ**: Discount factor $γ ∈ [0,1]$\n",
        "\n",
        "### Markov Property:\n",
        "\n",
        "**Key Assumption**: The future is independent of the past given the present\n",
        "$$P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P(s_{t+1}|s_t, a_t)$$\n",
        "\n",
        "### Value Functions:\n",
        "\n",
        "#### State Value Function:\n",
        "$$V^π(s) = E_π[G_t | S_t = s] = E_π\\left[\\sum_{k=0}^{∞} γ^k R_{t+k+1} | S_t = s\\right]$$\n",
        "\n",
        "#### Action Value Function (Q-function):\n",
        "$$Q^π(s,a) = E_π[G_t | S_t = s, A_t = a] = E_π\\left[\\sum_{k=0}^{∞} γ^k R_{t+k+1} | S_t = s, A_t = a\\right]$$\n",
        "\n",
        "### Bellman Equations:\n",
        "\n",
        "#### Bellman Equation for $V^π$:\n",
        "$$V^π(s) = \\sum_a π(a|s) \\sum_{s'} P(s'|s,a)[R(s,a,s') + γV^π(s')]$$\n",
        "\n",
        "#### Bellman Equation for $Q^π$:\n",
        "$$Q^π(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + γ\\sum_{a'} π(a'|s')Q^π(s',a')]$$\n",
        "\n",
        "### Optimality:\n",
        "\n",
        "#### Optimal Value Functions:\n",
        "$$V^*(s) = \\max_π V^π(s)$$\n",
        "$$Q^*(s,a) = \\max_π Q^π(s,a)$$\n",
        "\n",
        "#### Bellman Optimality Equations:\n",
        "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + γV^*(s')]$$\n",
        "$$Q^*(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + γ\\max_{a'} Q^*(s',a')]$$\n",
        "\n",
        "#### Optimal Policy:\n",
        "$$π^*(s) = \\arg\\max_a Q^*(s,a)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdp_implementation"
      },
      "outputs": [],
      "source": [
        "# MDP Implementation and Value Iteration\n",
        "\n",
        "class GridWorldMDP:\n",
        "    \"\"\"\n",
        "    A grid world MDP for demonstrating value iteration and policy iteration.\n",
        "    \"\"\"\n",
        "    def __init__(self, height=4, width=4, goal_states=None, obstacle_states=None):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.n_states = height * width\n",
        "        self.n_actions = 4  # up, right, down, left\n",
        "\n",
        "        # Define goal and obstacle states\n",
        "        self.goal_states = goal_states or [(3, 3)]\n",
        "        self.obstacle_states = obstacle_states or [(1, 1), (2, 2)]\n",
        "\n",
        "        # Action mappings\n",
        "        self.actions = {\n",
        "            0: (-1, 0),  # up\n",
        "            1: (0, 1),   # right\n",
        "            2: (1, 0),   # down\n",
        "            3: (0, -1)   # left\n",
        "        }\n",
        "\n",
        "        # Build transition probabilities and rewards\n",
        "        self.build_mdp()\n",
        "\n",
        "    def state_to_coords(self, state):\n",
        "        \"\"\"Convert state index to (row, col) coordinates.\"\"\"\n",
        "        return (state // self.width, state % self.width)\n",
        "\n",
        "    def coords_to_state(self, row, col):\n",
        "        \"\"\"Convert (row, col) coordinates to state index.\"\"\"\n",
        "        return row * self.width + col\n",
        "\n",
        "    def is_valid_state(self, row, col):\n",
        "        \"\"\"Check if coordinates represent a valid state.\"\"\"\n",
        "        return (0 <= row < self.height and\n",
        "                0 <= col < self.width and\n",
        "                (row, col) not in self.obstacle_states)\n",
        "\n",
        "    def build_mdp(self):\n",
        "        \"\"\"\n",
        "        Build transition probabilities and reward function.\n",
        "        \"\"\"\n",
        "        # Initialize transition probabilities P(s'|s,a)\n",
        "        self.P = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
        "\n",
        "        # Initialize rewards R(s,a,s')\n",
        "        self.R = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
        "\n",
        "        for state in range(self.n_states):\n",
        "            row, col = self.state_to_coords(state)\n",
        "\n",
        "            # Skip obstacle states\n",
        "            if (row, col) in self.obstacle_states:\n",
        "                continue\n",
        "\n",
        "            # Goal states are terminal\n",
        "            if (row, col) in self.goal_states:\n",
        "                for action in range(self.n_actions):\n",
        "                    self.P[state, action, state] = 1.0\n",
        "                    self.R[state, action, state] = 0.0\n",
        "                continue\n",
        "\n",
        "            for action in range(self.n_actions):\n",
        "                # Intended action (probability 0.8)\n",
        "                intended_dr, intended_dc = self.actions[action]\n",
        "                intended_row = row + intended_dr\n",
        "                intended_col = col + intended_dc\n",
        "\n",
        "                if self.is_valid_state(intended_row, intended_col):\n",
        "                    next_state = self.coords_to_state(intended_row, intended_col)\n",
        "                    self.P[state, action, next_state] += 0.8\n",
        "                else:\n",
        "                    # Stay in place if hitting wall\n",
        "                    self.P[state, action, state] += 0.8\n",
        "\n",
        "                # Perpendicular actions (probability 0.1 each)\n",
        "                perp_actions = [(action - 1) % 4, (action + 1) % 4]\n",
        "\n",
        "                for perp_action in perp_actions:\n",
        "                    perp_dr, perp_dc = self.actions[perp_action]\n",
        "                    perp_row = row + perp_dr\n",
        "                    perp_col = col + perp_dc\n",
        "\n",
        "                    if self.is_valid_state(perp_row, perp_col):\n",
        "                        next_state = self.coords_to_state(perp_row, perp_col)\n",
        "                        self.P[state, action, next_state] += 0.1\n",
        "                    else:\n",
        "                        # Stay in place if hitting wall\n",
        "                        self.P[state, action, state] += 0.1\n",
        "\n",
        "                # Set rewards\n",
        "                for next_state in range(self.n_states):\n",
        "                    if self.P[state, action, next_state] > 0:\n",
        "                        next_row, next_col = self.state_to_coords(next_state)\n",
        "\n",
        "                        if (next_row, next_col) in self.goal_states:\n",
        "                            self.R[state, action, next_state] = 10.0\n",
        "                        else:\n",
        "                            self.R[state, action, next_state] = -0.1  # Living penalty\n",
        "\n",
        "    def value_iteration(self, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
        "        \"\"\"\n",
        "        Perform value iteration to find optimal value function and policy.\n",
        "        \"\"\"\n",
        "        # Initialize value function\n",
        "        V = np.zeros(self.n_states)\n",
        "\n",
        "        history = []\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            V_old = V.copy()\n",
        "\n",
        "            for state in range(self.n_states):\n",
        "                # Compute Q-values for all actions\n",
        "                q_values = np.zeros(self.n_actions)\n",
        "\n",
        "                for action in range(self.n_actions):\n",
        "                    q_values[action] = np.sum(\n",
        "                        self.P[state, action, :] *\n",
        "                        (self.R[state, action, :] + gamma * V_old)\n",
        "                    )\n",
        "\n",
        "                # Update value function\n",
        "                V[state] = np.max(q_values)\n",
        "\n",
        "            # Track convergence\n",
        "            delta = np.max(np.abs(V - V_old))\n",
        "            history.append(delta)\n",
        "\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "        # Extract optimal policy\n",
        "        policy = np.zeros(self.n_states, dtype=int)\n",
        "\n",
        "        for state in range(self.n_states):\n",
        "            q_values = np.zeros(self.n_actions)\n",
        "\n",
        "            for action in range(self.n_actions):\n",
        "                q_values[action] = np.sum(\n",
        "                    self.P[state, action, :] *\n",
        "                    (self.R[state, action, :] + gamma * V)\n",
        "                )\n",
        "\n",
        "            policy[state] = np.argmax(q_values)\n",
        "\n",
        "        return V, policy, history\n",
        "\n",
        "    def policy_evaluation(self, policy, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
        "        \"\"\"\n",
        "        Evaluate a given policy.\n",
        "        \"\"\"\n",
        "        V = np.zeros(self.n_states)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            V_old = V.copy()\n",
        "\n",
        "            for state in range(self.n_states):\n",
        "                action = policy[state]\n",
        "                V[state] = np.sum(\n",
        "                    self.P[state, action, :] *\n",
        "                    (self.R[state, action, :] + gamma * V_old)\n",
        "                )\n",
        "\n",
        "            if np.max(np.abs(V - V_old)) < theta:\n",
        "                break\n",
        "\n",
        "        return V\n",
        "\n",
        "    def policy_iteration(self, gamma=0.9, max_iterations=100):\n",
        "        \"\"\"\n",
        "        Perform policy iteration.\n",
        "        \"\"\"\n",
        "        # Initialize random policy\n",
        "        policy = np.random.randint(0, self.n_actions, self.n_states)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            # Policy evaluation\n",
        "            V = self.policy_evaluation(policy, gamma)\n",
        "\n",
        "            # Policy improvement\n",
        "            policy_stable = True\n",
        "\n",
        "            for state in range(self.n_states):\n",
        "                old_action = policy[state]\n",
        "\n",
        "                q_values = np.zeros(self.n_actions)\n",
        "                for action in range(self.n_actions):\n",
        "                    q_values[action] = np.sum(\n",
        "                        self.P[state, action, :] *\n",
        "                        (self.R[state, action, :] + gamma * V)\n",
        "                    )\n",
        "\n",
        "                policy[state] = np.argmax(q_values)\n",
        "\n",
        "                if old_action != policy[state]:\n",
        "                    policy_stable = False\n",
        "\n",
        "            if policy_stable:\n",
        "                break\n",
        "\n",
        "        return V, policy\n",
        "\n",
        "    def visualize_policy(self, policy, values=None):\n",
        "        \"\"\"\n",
        "        Visualize policy and optionally value function.\n",
        "        \"\"\"\n",
        "        action_symbols = ['↑', '→', '↓', '←']\n",
        "\n",
        "        if values is not None:\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        else:\n",
        "            fig, ax1 = plt.subplots(1, 1, figsize=(6, 5))\n",
        "\n",
        "        # Policy visualization\n",
        "        policy_grid = np.empty((self.height, self.width), dtype=object)\n",
        "\n",
        "        for state in range(self.n_states):\n",
        "            row, col = self.state_to_coords(state)\n",
        "\n",
        "            if (row, col) in self.obstacle_states:\n",
        "                policy_grid[row, col] = '■'\n",
        "            elif (row, col) in self.goal_states:\n",
        "                policy_grid[row, col] = 'G'\n",
        "            else:\n",
        "                policy_grid[row, col] = action_symbols[policy[state]]\n",
        "\n",
        "        # Create color map\n",
        "        colors = np.ones((self.height, self.width, 3))\n",
        "        for row, col in self.obstacle_states:\n",
        "            colors[row, col] = [0.3, 0.3, 0.3]  # Gray for obstacles\n",
        "        for row, col in self.goal_states:\n",
        "            colors[row, col] = [0.2, 0.8, 0.2]  # Green for goals\n",
        "\n",
        "        ax1.imshow(colors)\n",
        "\n",
        "        # Add policy symbols\n",
        "        for i in range(self.height):\n",
        "            for j in range(self.width):\n",
        "                ax1.text(j, i, policy_grid[i, j], ha='center', va='center',\n",
        "                        fontsize=16, fontweight='bold')\n",
        "\n",
        "        ax1.set_title('Optimal Policy')\n",
        "        ax1.set_xticks(range(self.width))\n",
        "        ax1.set_yticks(range(self.height))\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Value function visualization\n",
        "        if values is not None:\n",
        "            value_grid = np.zeros((self.height, self.width))\n",
        "\n",
        "            for state in range(self.n_states):\n",
        "                row, col = self.state_to_coords(state)\n",
        "                value_grid[row, col] = values[state]\n",
        "\n",
        "            im = ax2.imshow(value_grid, cmap='viridis')\n",
        "\n",
        "            # Add value text\n",
        "            for i in range(self.height):\n",
        "                for j in range(self.width):\n",
        "                    ax2.text(j, i, f'{value_grid[i, j]:.2f}', ha='center', va='center',\n",
        "                            fontsize=10, color='white', fontweight='bold')\n",
        "\n",
        "            ax2.set_title('Value Function')\n",
        "            ax2.set_xticks(range(self.width))\n",
        "            ax2.set_yticks(range(self.height))\n",
        "            plt.colorbar(im, ax=ax2)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Create and solve MDP\n",
        "mdp = GridWorldMDP(height=4, width=4, goal_states=[(0, 3), (3, 3)],\n",
        "                   obstacle_states=[(1, 1), (2, 2)])\n",
        "\n",
        "print(\"Grid World MDP:\")\n",
        "print(f\"States: {mdp.n_states}\")\n",
        "print(f\"Actions: {mdp.n_actions}\")\n",
        "print(f\"Goal states: {mdp.goal_states}\")\n",
        "print(f\"Obstacle states: {mdp.obstacle_states}\")\n",
        "\n",
        "# Solve using Value Iteration\n",
        "print(\"\\nSolving with Value Iteration...\")\n",
        "V_vi, policy_vi, vi_history = mdp.value_iteration(gamma=0.9)\n",
        "print(f\"Converged in {len(vi_history)} iterations\")\n",
        "\n",
        "# Solve using Policy Iteration\n",
        "print(\"\\nSolving with Policy Iteration...\")\n",
        "V_pi, policy_pi = mdp.policy_iteration(gamma=0.9)\n",
        "\n",
        "# Visualize results\n",
        "print(\"\\nValue Iteration Results:\")\n",
        "mdp.visualize_policy(policy_vi, V_vi)\n",
        "\n",
        "print(\"\\nPolicy Iteration Results:\")\n",
        "mdp.visualize_policy(policy_pi, V_pi)\n",
        "\n",
        "# Compare convergence\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(vi_history, label='Value Iteration')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Max Value Change')\n",
        "plt.title('Value Iteration Convergence')\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Verify solutions are the same\n",
        "print(f\"\\nSolutions identical: {np.allclose(V_vi, V_pi) and np.array_equal(policy_vi, policy_pi)}\")\n",
        "print(f\"Max value difference: {np.max(np.abs(V_vi - V_pi)):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td-learning"
      },
      "source": [
        "# 8. Temporal Difference Learning\n",
        "\n",
        "## Theoretical Foundation\n",
        "\n",
        "**Temporal Difference (TD) Learning** combines ideas from Monte Carlo and Dynamic Programming. It learns directly from experience without a model and updates estimates based on other estimates.\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "#### TD Error:\n",
        "$$\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$$\n",
        "\n",
        "The TD error measures the difference between the estimated value and the actual observed reward plus discounted next state value.\n",
        "\n",
        "#### TD(0) Update Rule:\n",
        "$$V(S_t) \\leftarrow V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$$\n",
        "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\delta_t$$\n",
        "\n",
        "Where $\\alpha$ is the learning rate.\n",
        "\n",
        "### Advantages of TD Learning:\n",
        "\n",
        "1. **Model-Free**: No need for environment model\n",
        "2. **Online**: Can learn from incomplete episodes\n",
        "3. **Bootstrapping**: Uses current estimates to improve estimates\n",
        "4. **Sample Efficient**: Often faster than Monte Carlo\n",
        "\n",
        "### TD(λ) - Eligibility Traces (Continued):\n",
        "\n",
        "Where:\n",
        "- $\\lambda = 0$: TD(0) - only current state updated\n",
        "- $\\lambda = 1$: Equivalent to Monte Carlo\n",
        "- $\\lambda \\in (0,1)$: Balance between TD and MC\n",
        "\n",
        "### Comparison with Other Methods:\n",
        "\n",
        "| Method | Model Required | Bootstrapping | Online | Convergence |\n",
        "|--------|---------------|---------------|--------|-----------|\n",
        "| DP | Yes | Yes | No | Guaranteed |\n",
        "| MC | No | No | No | Guaranteed |\n",
        "| TD | No | Yes | Yes | Guaranteed* |\n",
        "\n",
        "*Under certain conditions (e.g., function approximation may not converge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td_learning_implementation"
      },
      "outputs": [],
      "source": [
        "# Temporal Difference Learning Implementation\n",
        "\n",
        "class TDAgent:\n",
        "    \"\"\"\n",
        "    Temporal Difference learning agent for state value estimation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_states, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.n_states = n_states\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "\n",
        "        # Initialize value function\n",
        "        self.V = np.zeros(n_states)\n",
        "\n",
        "        # For tracking learning progress\n",
        "        self.td_errors = []\n",
        "        self.value_history = []\n",
        "\n",
        "    def get_action(self, state, valid_actions):\n",
        "        \"\"\"\n",
        "        Epsilon-greedy action selection based on value function.\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(valid_actions)\n",
        "        else:\n",
        "            # Greedy action (this is simplified - normally we'd use Q-values)\n",
        "            return np.random.choice(valid_actions)\n",
        "\n",
        "    def td_update(self, state, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Perform TD(0) update.\n",
        "        \"\"\"\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            target = reward + self.gamma * self.V[next_state]\n",
        "\n",
        "        # TD error\n",
        "        td_error = target - self.V[state]\n",
        "\n",
        "        # Update value function\n",
        "        self.V[state] += self.alpha * td_error\n",
        "\n",
        "        # Store for analysis\n",
        "        self.td_errors.append(abs(td_error))\n",
        "\n",
        "        return td_error\n",
        "\n",
        "    def save_values(self):\n",
        "        \"\"\"Save current value function for tracking.\"\"\"\n",
        "        self.value_history.append(self.V.copy())\n",
        "\n",
        "class TDLambdaAgent(TDAgent):\n",
        "    \"\"\"\n",
        "    TD(λ) agent with eligibility traces.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_states, alpha=0.1, gamma=0.9, epsilon=0.1, lambda_=0.9):\n",
        "        super().__init__(n_states, alpha, gamma, epsilon)\n",
        "        self.lambda_ = lambda_\n",
        "        self.eligibility_traces = np.zeros(n_states)\n",
        "\n",
        "    def reset_traces(self):\n",
        "        \"\"\"Reset eligibility traces for new episode.\"\"\"\n",
        "        self.eligibility_traces = np.zeros(self.n_states)\n",
        "\n",
        "    def td_lambda_update(self, state, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Perform TD(λ) update with eligibility traces.\n",
        "        \"\"\"\n",
        "        # Compute TD error\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            target = reward + self.gamma * self.V[next_state]\n",
        "\n",
        "        td_error = target - self.V[state]\n",
        "\n",
        "        # Update eligibility traces\n",
        "        self.eligibility_traces *= self.gamma * self.lambda_\n",
        "        self.eligibility_traces[state] += 1\n",
        "\n",
        "        # Update all states proportional to their eligibility\n",
        "        self.V += self.alpha * td_error * self.eligibility_traces\n",
        "\n",
        "        self.td_errors.append(abs(td_error))\n",
        "\n",
        "        return td_error\n",
        "\n",
        "# Test TD learning on a simple chain MDP\n",
        "class ChainMDP:\n",
        "    \"\"\"\n",
        "    Simple chain MDP for testing TD learning.\n",
        "    States: 0 -> 1 -> 2 -> 3 -> 4 (terminal)\n",
        "    Reward only at the end.\n",
        "    \"\"\"\n",
        "    def __init__(self, length=5, reward=1.0):\n",
        "        self.length = length\n",
        "        self.reward = reward\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Only one action: move right\n",
        "        self.state += 1\n",
        "\n",
        "        if self.state >= self.length - 1:\n",
        "            reward = self.reward\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0.0\n",
        "            done = False\n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "# Compare TD(0), TD(λ), and Monte Carlo\n",
        "def compare_td_methods():\n",
        "    chain = ChainMDP(length=5)\n",
        "    n_episodes = 500\n",
        "\n",
        "    # Create agents\n",
        "    td0_agent = TDAgent(n_states=5, alpha=0.1)\n",
        "    td_lambda_agent = TDLambdaAgent(n_states=5, alpha=0.1, lambda_=0.9)\n",
        "    mc_values = np.zeros(5)  # Monte Carlo estimates\n",
        "\n",
        "    # True values for comparison (computed analytically)\n",
        "    gamma = 0.9\n",
        "    true_values = np.array([gamma**4, gamma**3, gamma**2, gamma**1, 0.0])\n",
        "\n",
        "    td0_errors = []\n",
        "    td_lambda_errors = []\n",
        "    mc_errors = []\n",
        "\n",
        "    # Track episode returns for MC\n",
        "    episode_returns = [[] for _ in range(5)]\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        # TD(0) learning\n",
        "        state = chain.reset()\n",
        "        while True:\n",
        "            next_state, reward, done = chain.step(0)\n",
        "            td0_agent.td_update(state, reward, next_state, done)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # TD(λ) learning\n",
        "        td_lambda_agent.reset_traces()\n",
        "        state = chain.reset()\n",
        "        while True:\n",
        "            next_state, reward, done = chain.step(0)\n",
        "            td_lambda_agent.td_lambda_update(state, reward, next_state, done)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Monte Carlo learning\n",
        "        state = chain.reset()\n",
        "        episode_states = [state]\n",
        "        episode_rewards = []\n",
        "\n",
        "        while True:\n",
        "            next_state, reward, done = chain.step(0)\n",
        "            episode_rewards.append(reward)\n",
        "            if not done:\n",
        "                episode_states.append(next_state)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # Compute returns and update MC estimates\n",
        "        G = 0\n",
        "        for i in reversed(range(len(episode_states))):\n",
        "            G = episode_rewards[i] + gamma * G\n",
        "            episode_returns[episode_states[i]].append(G)\n",
        "\n",
        "        # Update MC value estimates\n",
        "        for state in range(5):\n",
        "            if episode_returns[state]:\n",
        "                mc_values[state] = np.mean(episode_returns[state])\n",
        "\n",
        "        # Compute errors every 10 episodes\n",
        "        if episode % 10 == 0:\n",
        "            td0_error = np.mean(np.abs(td0_agent.V - true_values))\n",
        "            td_lambda_error = np.mean(np.abs(td_lambda_agent.V - true_values))\n",
        "            mc_error = np.mean(np.abs(mc_values - true_values))\n",
        "\n",
        "            td0_errors.append(td0_error)\n",
        "            td_lambda_errors.append(td_lambda_error)\n",
        "            mc_errors.append(mc_error)\n",
        "\n",
        "    return (td0_agent, td_lambda_agent, mc_values, true_values,\n",
        "            td0_errors, td_lambda_errors, mc_errors)\n",
        "\n",
        "# Run comparison\n",
        "print(\"Comparing TD(0), TD(λ), and Monte Carlo methods...\")\n",
        "results = compare_td_methods()\n",
        "td0_agent, td_lambda_agent, mc_values, true_values, td0_errors, td_lambda_errors, mc_errors = results\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Value function comparison\n",
        "plt.subplot(2, 3, 1)\n",
        "states = range(5)\n",
        "plt.plot(states, true_values, 'ko-', label='True Values', linewidth=2)\n",
        "plt.plot(states, td0_agent.V, 'bo-', label='TD(0)', linewidth=2)\n",
        "plt.plot(states, td_lambda_agent.V, 'ro-', label='TD(λ)', linewidth=2)\n",
        "plt.plot(states, mc_values, 'go-', label='Monte Carlo', linewidth=2)\n",
        "plt.xlabel('State')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Value Function Estimates')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Learning curves\n",
        "plt.subplot(2, 3, 2)\n",
        "episodes = range(0, 500, 10)\n",
        "plt.plot(episodes, td0_errors, 'b-', label='TD(0)', linewidth=2)\n",
        "plt.plot(episodes, td_lambda_errors, 'r-', label='TD(λ)', linewidth=2)\n",
        "plt.plot(episodes, mc_errors, 'g-', label='Monte Carlo', linewidth=2)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.title('Learning Curves')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# TD errors over time\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.plot(td0_agent.td_errors[:1000], alpha=0.7, label='TD(0)')\n",
        "plt.plot(td_lambda_agent.td_errors[:1000], alpha=0.7, label='TD(λ)')\n",
        "plt.xlabel('Update Step')\n",
        "plt.ylabel('|TD Error|')\n",
        "plt.title('TD Errors (First 1000 steps)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Error distribution\n",
        "plt.subplot(2, 3, 4)\n",
        "final_errors_td0 = np.abs(td0_agent.V - true_values)\n",
        "final_errors_td_lambda = np.abs(td_lambda_agent.V - true_values)\n",
        "final_errors_mc = np.abs(mc_values - true_values)\n",
        "\n",
        "x = np.arange(5)\n",
        "width = 0.25\n",
        "plt.bar(x - width, final_errors_td0, width, label='TD(0)', alpha=0.7)\n",
        "plt.bar(x, final_errors_td_lambda, width, label='TD(λ)', alpha=0.7)\n",
        "plt.bar(x + width, final_errors_mc, width, label='Monte Carlo', alpha=0.7)\n",
        "plt.xlabel('State')\n",
        "plt.ylabel('Absolute Error')\n",
        "plt.title('Final Estimation Errors')\n",
        "plt.legend()\n",
        "plt.xticks(x)\n",
        "\n",
        "# Convergence comparison\n",
        "plt.subplot(2, 3, 5)\n",
        "final_mae_td0 = np.mean(final_errors_td0)\n",
        "final_mae_td_lambda = np.mean(final_errors_td_lambda)\n",
        "final_mae_mc = np.mean(final_errors_mc)\n",
        "\n",
        "methods = ['TD(0)', 'TD(λ)', 'Monte Carlo']\n",
        "final_errors = [final_mae_td0, final_mae_td_lambda, final_mae_mc]\n",
        "colors = ['blue', 'red', 'green']\n",
        "\n",
        "plt.bar(methods, final_errors, color=colors, alpha=0.7)\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.title('Final Performance Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Learning efficiency\n",
        "plt.subplot(2, 3, 6)\n",
        "# Find episode where each method reaches within 10% of final error\n",
        "threshold_td0 = final_mae_td0 * 1.1\n",
        "threshold_td_lambda = final_mae_td_lambda * 1.1\n",
        "threshold_mc = final_mae_mc * 1.1\n",
        "\n",
        "conv_td0 = next((i for i, err in enumerate(td0_errors) if err <= threshold_td0), len(td0_errors))\n",
        "conv_td_lambda = next((i for i, err in enumerate(td_lambda_errors) if err <= threshold_td_lambda), len(td_lambda_errors))\n",
        "conv_mc = next((i for i, err in enumerate(mc_errors) if err <= threshold_mc), len(mc_errors))\n",
        "\n",
        "convergence_episodes = [conv_td0 * 10, conv_td_lambda * 10, conv_mc * 10]\n",
        "plt.bar(methods, convergence_episodes, color=colors, alpha=0.7)\n",
        "plt.ylabel('Episodes to Converge')\n",
        "plt.title('Convergence Speed')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFinal Results:\")\n",
        "print(f\"True Values:     {true_values}\")\n",
        "print(f\"TD(0) Values:    {td0_agent.V}\")\n",
        "print(f\"TD(λ) Values:    {td_lambda_agent.V}\")\n",
        "print(f\"MC Values:       {mc_values}\")\n",
        "\n",
        "print(f\"\\nFinal MAE:\")\n",
        "print(f\"TD(0):           {final_mae_td0:.4f}\")\n",
        "print(f\"TD(λ):           {final_mae_td_lambda:.4f}\")\n",
        "print(f\"Monte Carlo:     {final_mae_mc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-learning"
      },
      "source": [
        "# 9. Q-Learning\n",
        "\n",
        "## Theoretical Foundation\n",
        "\n",
        "**Q-Learning** is an off-policy TD control algorithm that learns the action-value function $Q(s,a)$ directly.\n",
        "\n",
        "### Key Innovation:\n",
        "\n",
        "Q-Learning learns the optimal action-value function regardless of the policy being followed:\n",
        "\n",
        "$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha [r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)]$$\n",
        "\n",
        "### Algorithm Properties:\n",
        "\n",
        "1. **Off-Policy**: Can learn optimal policy while following any policy\n",
        "2. **Model-Free**: Doesn't require environment model\n",
        "3. **Convergence**: Guaranteed to converge to $Q^*$ under certain conditions\n",
        "4. **Exploration**: Requires adequate exploration of state-action space\n",
        "\n",
        "### Q-Learning Algorithm:\n",
        "\n",
        "```\n",
        "Initialize Q(s,a) arbitrarily\n",
        "For each episode:\n",
        "    Initialize s\n",
        "    For each step of episode:\n",
        "        Choose a from s using policy derived from Q (e.g., ε-greedy)\n",
        "        Take action a, observe r, s'\n",
        "        Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]\n",
        "        s ← s'\n",
        "    Until s is terminal\n",
        "```\n",
        "\n",
        "### Exploration Strategies:\n",
        "\n",
        "#### ε-Greedy:\n",
        "$$a = \\begin{cases}\n",
        "\\arg\\max_a Q(s,a) & \\text{with probability } 1-\\epsilon \\\\\n",
        "\\text{random action} & \\text{with probability } \\epsilon\n",
        "\\end{cases}$$\n",
        "\n",
        "#### Boltzmann Exploration:\n",
        "$$P(a|s) = \\frac{e^{Q(s,a)/\\tau}}{\\sum_{a'} e^{Q(s,a')/\\tau}}$$\n",
        "\n",
        "Where $\\tau$ is the temperature parameter.\n",
        "\n",
        "#### Upper Confidence Bound (UCB):\n",
        "$$a = \\arg\\max_a \\left[Q(s,a) + c\\sqrt{\\frac{\\ln t}{N(s,a)}}\\right]$$\n",
        "\n",
        "Where $N(s,a)$ is the number of times action $a$ was taken in state $s$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_learning_implementation"
      },
      "outputs": [],
      "source": [
        "# Q-Learning Implementation\n",
        "\n",
        "class QLearningAgent:\n",
        "    \"\"\"\n",
        "    Q-Learning agent implementation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.95,\n",
        "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        # Initialize Q-table\n",
        "        self.Q = np.zeros((n_states, n_actions))\n",
        "\n",
        "        # For tracking\n",
        "        self.q_errors = []\n",
        "        self.episode_rewards = []\n",
        "        self.epsilon_history = []\n",
        "\n",
        "        # Action counts for exploration analysis\n",
        "        self.action_counts = np.zeros((n_states, n_actions))\n",
        "\n",
        "    def get_action(self, state, valid_actions=None):\n",
        "        \"\"\"\n",
        "        Choose action using ε-greedy policy.\n",
        "        \"\"\"\n",
        "        if valid_actions is None:\n",
        "            valid_actions = list(range(self.n_actions))\n",
        "\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore\n",
        "            action = np.random.choice(valid_actions)\n",
        "        else:\n",
        "            # Exploit\n",
        "            q_values = self.Q[state, valid_actions]\n",
        "            best_action_idx = np.argmax(q_values)\n",
        "            action = valid_actions[best_action_idx]\n",
        "\n",
        "        self.action_counts[state, action] += 1\n",
        "        return action\n",
        "\n",
        "    def get_action_greedy(self, state, valid_actions=None):\n",
        "        \"\"\"\n",
        "        Choose action greedily (for evaluation).\n",
        "        \"\"\"\n",
        "        if valid_actions is None:\n",
        "            valid_actions = list(range(self.n_actions))\n",
        "\n",
        "        q_values = self.Q[state, valid_actions]\n",
        "        best_action_idx = np.argmax(q_values)\n",
        "        return valid_actions[best_action_idx]\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Update Q-table using Q-learning rule.\n",
        "        \"\"\"\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # Q-learning update\n",
        "        old_q = self.Q[state, action]\n",
        "        self.Q[state, action] += self.alpha * (target - old_q)\n",
        "\n",
        "        # Track Q-value changes\n",
        "        q_error = abs(target - old_q)\n",
        "        self.q_errors.append(q_error)\n",
        "\n",
        "        return q_error\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"\n",
        "        Decay exploration rate.\n",
        "        \"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "        self.epsilon_history.append(self.epsilon)\n",
        "\n",
        "    def get_policy(self):\n",
        "        \"\"\"\n",
        "        Extract greedy policy from Q-table.\n",
        "        \"\"\"\n",
        "        return np.argmax(self.Q, axis=1)\n",
        "\n",
        "    def get_value_function(self):\n",
        "        \"\"\"\n",
        "        Extract value function from Q-table.\n",
        "        \"\"\"\n",
        "        return np.max(self.Q, axis=1)\n",
        "\n",
        "# Test on Grid World\n",
        "class SimpleGridWorld:\n",
        "    \"\"\"\n",
        "    Simple grid world for testing Q-learning.\n",
        "    \"\"\"\n",
        "    def __init__(self, height=4, width=4, goal_states=[(3,3)],\n",
        "                 obstacle_states=[(1,1)], start_state=(0,0)):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.goal_states = goal_states\n",
        "        self.obstacle_states = obstacle_states\n",
        "        self.start_state = start_state\n",
        "\n",
        "        self.n_states = height * width\n",
        "        self.n_actions = 4  # up, right, down, left\n",
        "\n",
        "        self.action_effects = {\n",
        "            0: (-1, 0),  # up\n",
        "            1: (0, 1),   # right\n",
        "            2: (1, 0),   # down\n",
        "            3: (0, -1)   # left\n",
        "        }\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def state_to_coords(self, state):\n",
        "        return (state // self.width, state % self.width)\n",
        "\n",
        "    def coords_to_state(self, row, col):\n",
        "        return row * self.width + col\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_pos = self.start_state\n",
        "        return self.coords_to_state(*self.current_pos)\n",
        "\n",
        "    def step(self, action):\n",
        "        dr, dc = self.action_effects[action]\n",
        "        new_row = self.current_pos[0] + dr\n",
        "        new_col = self.current_pos[1] + dc\n",
        "\n",
        "        # Check boundaries and obstacles\n",
        "        if (0 <= new_row < self.height and 0 <= new_col < self.width and\n",
        "            (new_row, new_col) not in self.obstacle_states):\n",
        "            self.current_pos = (new_row, new_col)\n",
        "\n",
        "        # Compute reward\n",
        "        if self.current_pos in self.goal_states:\n",
        "            reward = 10.0\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -0.1  # Small penalty for each step\n",
        "            done = False\n",
        "\n",
        "        next_state = self.coords_to_state(*self.current_pos)\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def render(self):\n",
        "        grid = np.zeros((self.height, self.width))\n",
        "\n",
        "        # Mark obstacles\n",
        "        for obs in self.obstacle_states:\n",
        "            grid[obs] = -1\n",
        "\n",
        "        # Mark goals\n",
        "        for goal in self.goal_states:\n",
        "            grid[goal] = 2\n",
        "\n",
        "        # Mark current position\n",
        "        grid[self.current_pos] = 1\n",
        "\n",
        "        return grid\n",
        "\n",
        "# Train Q-learning agent\n",
        "def train_q_learning(env, agent, n_episodes=1000, max_steps=100):\n",
        "    \"\"\"\n",
        "    Train Q-learning agent.\n",
        "    \"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action = agent.get_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            agent.update(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_lengths.append(steps)\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            print(f\"Episode {episode}, Avg Reward: {avg_reward:.2f}, ε: {agent.epsilon:.3f}\")\n",
        "\n",
        "    return episode_rewards, episode_lengths\n",
        "\n",
        "# Create environment and agent\n",
        "env = SimpleGridWorld(height=4, width=4, goal_states=[(3,3)],\n",
        "                     obstacle_states=[(1,1), (2,2)])\n",
        "\n",
        "agent = QLearningAgent(n_states=16, n_actions=4, alpha=0.1, gamma=0.95,\n",
        "                      epsilon=1.0, epsilon_decay=0.995)\n",
        "\n",
        "print(\"Training Q-Learning Agent...\")\n",
        "episode_rewards, episode_lengths = train_q_learning(env, agent, n_episodes=1000)\n",
        "\n",
        "print(\"\\nFinal Results:\")\n",
        "print(f\"Final ε: {agent.epsilon:.4f}\")\n",
        "print(f\"Final 100 episodes avg reward: {np.mean(episode_rewards[-100:]):.2f}\")\n",
        "print(f\"Final 100 episodes avg length: {np.mean(episode_lengths[-100:]):.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_learning_analysis"
      },
      "outputs": [],
      "source": [
        "# Analyze Q-Learning Results\n",
        "\n",
        "# Visualize Q-table and learned policy\n",
        "def visualize_q_learning_results(agent, env):\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    # Q-values for each action\n",
        "    action_names = ['Up', 'Right', 'Down', 'Left']\n",
        "\n",
        "    for i, action_name in enumerate(action_names):\n",
        "        ax = axes[0, i] if i < 2 else axes[1, i-2]\n",
        "\n",
        "        q_values = agent.Q[:, i].reshape(env.height, env.width)\n",
        "\n",
        "        im = ax.imshow(q_values, cmap='RdBu', interpolation='nearest')\n",
        "        ax.set_title(f'Q-values for {action_name}')\n",
        "\n",
        "        # Add text annotations\n",
        "        for row in range(env.height):\n",
        "            for col in range(env.width):\n",
        "                ax.text(col, row, f'{q_values[row, col]:.2f}',\n",
        "                       ha='center', va='center', fontsize=10)\n",
        "\n",
        "        plt.colorbar(im, ax=ax)\n",
        "        ax.set_xticks(range(env.width))\n",
        "        ax.set_yticks(range(env.height))\n",
        "\n",
        "    # Policy visualization\n",
        "    ax = axes[1, 2]\n",
        "    policy = agent.get_policy().reshape(env.height, env.width)\n",
        "    value_function = agent.get_value_function().reshape(env.height, env.width)\n",
        "\n",
        "    im = ax.imshow(value_function, cmap='viridis', interpolation='nearest')\n",
        "\n",
        "    # Add policy arrows\n",
        "    arrow_symbols = ['↑', '→', '↓', '←']\n",
        "    for row in range(env.height):\n",
        "        for col in range(env.width):\n",
        "            if (row, col) in env.obstacle_states:\n",
        "                symbol = '■'\n",
        "                color = 'red'\n",
        "            elif (row, col) in env.goal_states:\n",
        "                symbol = 'G'\n",
        "                color = 'gold'\n",
        "            else:\n",
        "                symbol = arrow_symbols[policy[row, col]]\n",
        "                color = 'white'\n",
        "\n",
        "            ax.text(col, row, symbol, ha='center', va='center',\n",
        "                   fontsize=16, color=color, fontweight='bold')\n",
        "\n",
        "    ax.set_title('Learned Policy & Value Function')\n",
        "    plt.colorbar(im, ax=ax)\n",
        "    ax.set_xticks(range(env.width))\n",
        "    ax.set_yticks(range(env.height))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize training progress\n",
        "def plot_training_progress(agent, episode_rewards, episode_lengths):\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "    # Episode rewards\n",
        "    ax = axes[0, 0]\n",
        "    window_size = 50\n",
        "    moving_avg = pd.Series(episode_rewards).rolling(window_size).mean()\n",
        "    ax.plot(episode_rewards, alpha=0.3, color='blue')\n",
        "    ax.plot(moving_avg, color='red', linewidth=2)\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Total Reward')\n",
        "    ax.set_title('Episode Rewards')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Episode lengths\n",
        "    ax = axes[0, 1]\n",
        "    moving_avg_length = pd.Series(episode_lengths).rolling(window_size).mean()\n",
        "    ax.plot(episode_lengths, alpha=0.3, color='green')\n",
        "    ax.plot(moving_avg_length, color='red', linewidth=2)\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Episode Length')\n",
        "    ax.set_title('Episode Lengths')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Epsilon decay\n",
        "    ax = axes[0, 2]\n",
        "    ax.plot(agent.epsilon_history)\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Epsilon')\n",
        "    ax.set_title('Exploration Rate Decay')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Q-value changes\n",
        "    ax = axes[1, 0]\n",
        "    q_error_ma = pd.Series(agent.q_errors).rolling(1000).mean()\n",
        "    ax.plot(agent.q_errors[:5000], alpha=0.3, color='orange')\n",
        "    ax.plot(q_error_ma[:5000], color='red', linewidth=2)\n",
        "    ax.set_xlabel('Update Step')\n",
        "    ax.set_ylabel('Q-value Change')\n",
        "    ax.set_title('Q-value Updates (First 5000)')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Action distribution\n",
        "    ax = axes[1, 1]\n",
        "    total_actions = np.sum(agent.action_counts, axis=0)\n",
        "    action_names = ['Up', 'Right', 'Down', 'Left']\n",
        "    ax.bar(action_names, total_actions)\n",
        "    ax.set_ylabel('Action Count')\n",
        "    ax.set_title('Total Action Distribution')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # State visitation\n",
        "    ax = axes[1, 2]\n",
        "    state_visits = np.sum(agent.action_counts, axis=1).reshape(env.height, env.width)\n",
        "    im = ax.imshow(state_visits, cmap='Blues', interpolation='nearest')\n",
        "\n",
        "    for row in range(env.height):\n",
        "        for col in range(env.width):\n",
        "            ax.text(col, row, f'{int(state_visits[row, col])}',\n",
        "                   ha='center', va='center', fontsize=10)\n",
        "\n",
        "    ax.set_title('State Visitation Counts')\n",
        "    plt.colorbar(im, ax=ax)\n",
        "    ax.set_xticks(range(env.width))\n",
        "    ax.set_yticks(range(env.height))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Test learned policy\n",
        "def test_learned_policy(agent, env, n_episodes=10):\n",
        "    \"\"\"\n",
        "    Test the learned policy without exploration.\n",
        "    \"\"\"\n",
        "    test_rewards = []\n",
        "    test_lengths = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "        path = [env.state_to_coords(state)]\n",
        "\n",
        "        for step in range(100):  # Max steps\n",
        "            action = agent.get_action_greedy(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            path.append(env.state_to_coords(next_state))\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        test_rewards.append(episode_reward)\n",
        "        test_lengths.append(steps)\n",
        "\n",
        "        if episode == 0:  # Show first episode path\n",
        "            print(f\"Sample path: {path}\")\n",
        "\n",
        "    print(f\"\\nTest Results ({n_episodes} episodes):\")\n",
        "    print(f\"Average reward: {np.mean(test_rewards):.2f} ± {np.std(test_rewards):.2f}\")\n",
        "    print(f\"Average length: {np.mean(test_lengths):.1f} ± {np.std(test_lengths):.1f}\")\n",
        "    print(f\"Success rate: {np.mean([r > 5 for r in test_rewards]) * 100:.1f}%\")\n",
        "\n",
        "    return test_rewards, test_lengths\n",
        "\n",
        "# Run analysis\n",
        "print(\"\\nAnalyzing Q-Learning Results...\")\n",
        "visualize_q_learning_results(agent, env)\n",
        "plot_training_progress(agent, episode_rewards, episode_lengths)\n",
        "\n",
        "# Test final policy\n",
        "print(\"\\nTesting Learned Policy...\")\n",
        "test_rewards, test_lengths = test_learned_policy(agent, env, n_episodes=20)\n",
        "\n",
        "# Compare with random policy\n",
        "print(\"\\nComparing with Random Policy...\")\n",
        "random_rewards = []\n",
        "for _ in range(20):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    for step in range(100):\n",
        "        action = np.random.randint(4)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "    random_rewards.append(episode_reward)\n",
        "\n",
        "print(f\"Random policy average reward: {np.mean(random_rewards):.2f} ± {np.std(random_rewards):.2f}\")\n",
        "print(f\"Q-learning improvement: {np.mean(test_rewards) - np.mean(random_rewards):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deep-q-learning"
      },
      "source": [
        "# 10. Deep Q-Learning (DQN)\n",
        "\n",
        "## Theoretical Foundation\n",
        "\n",
        "**Deep Q-Networks (DQN)** combine Q-learning with deep neural networks to handle high-dimensional state spaces.\n",
        "\n",
        "### Key Innovations:\n",
        "\n",
        "#### 1. Function Approximation:\n",
        "Instead of a Q-table, use a neural network:\n",
        "$$Q(s,a;θ) ≈ Q^*(s,a)$$\n",
        "\n",
        "#### 2. Experience Replay:\n",
        "Store experiences $(s_t, a_t, r_t, s_{t+1})$ in a replay buffer and sample randomly for training.\n",
        "\n",
        "#### 3. Target Network:\n",
        "Use a separate target network $Q(s,a;θ^-)$ for computing targets:\n",
        "$$y_t = r_t + \\gamma \\max_{a'} Q(s_{t+1}, a'; θ^-)$$\n",
        "\n",
        "### DQN Loss Function:\n",
        "$$L(θ) = E_{(s,a,r,s') ∼ D}[(y - Q(s,a;θ))^2]$$\n",
        "\n",
        "Where $y = r + \\gamma \\max_{a'} Q(s',a';θ^-)$\n",
        "\n",
        "### Algorithm:\n",
        "```\n",
        "Initialize replay buffer D\n",
        "Initialize Q-network with random weights θ\n",
        "Initialize target Q-network with weights θ^- = θ\n",
        "\n",
        "For episode = 1 to M:\n",
        "    Initialize state s_1\n",
        "    For t = 1 to T:\n",
        "        Select action a_t using ε-greedy policy\n",
        "        Execute action a_t, observe r_t and s_{t+1}\n",
        "        Store (s_t, a_t, r_t, s_{t+1}) in D\n",
        "        Sample random minibatch from D\n",
        "        Perform gradient descent on DQN loss\n",
        "        Every C steps: θ^- ← θ\n",
        "```\n",
        "\n",
        "### Advantages:\n",
        "- Handles continuous/high-dimensional state spaces\n",
        "- Learns efficient representations\n",
        "- Can generalize to unseen states\n",
        "\n",
        "### Challenges:\n",
        "- Training instability\n",
        "- Hyperparameter sensitivity\n",
        "- Sample efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqn_implementation"
      },
      "outputs": [],
      "source": [
        "# Deep Q-Network Implementation\n",
        "\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Deep Q-Network agent implementation.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, action_size, lr=0.001, gamma=0.99,\n",
        "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01,\n",
        "                 buffer_size=10000, batch_size=32, target_update_freq=100):\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update_freq = target_update_freq\n",
        "\n",
        "        # Experience replay buffer\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "\n",
        "        # Neural networks\n",
        "        self.q_network = self._build_model()\n",
        "        self.target_network = self._build_model()\n",
        "        self.update_target_network()\n",
        "\n",
        "        # Training tracking\n",
        "        self.losses = []\n",
        "        self.episode_rewards = []\n",
        "        self.epsilon_history = []\n",
        "        self.training_step = 0\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"\n",
        "        Build the deep Q-network.\n",
        "        \"\"\"\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Dense(64, activation='relu', input_shape=(self.state_size,)),\n",
        "            keras.layers.Dense(64, activation='relu'),\n",
        "            keras.layers.Dense(32, activation='relu'),\n",
        "            keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=self.lr),\n",
        "                     loss='mse')\n",
        "        return model\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"\n",
        "        Copy weights from main network to target network.\n",
        "        \"\"\"\n",
        "        self.target_network.set_weights(self.q_network.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Store experience in replay buffer.\n",
        "        \"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Choose action using ε-greedy policy.\n",
        "        \"\"\"\n",
        "        if np.random.random() <= self.epsilon:\n",
        "            return np.random.choice(self.action_size)\n",
        "\n",
        "        state = np.array([state])\n",
        "        q_values = self.q_network.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def act_greedy(self, state):\n",
        "        \"\"\"\n",
        "        Choose action greedily (for evaluation).\n",
        "        \"\"\"\n",
        "        state = np.array([state])\n",
        "        q_values = self.q_network.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def replay(self):\n",
        "        \"\"\"\n",
        "        Train the model on a batch of experiences.\n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample random batch\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states = np.array([e[0] for e in batch])\n",
        "        actions = np.array([e[1] for e in batch])\n",
        "        rewards = np.array([e[2] for e in batch])\n",
        "        next_states = np.array([e[3] for e in batch])\n",
        "        dones = np.array([e[4] for e in batch])\n",
        "\n",
        "        # Compute current Q-values\n",
        "        current_q_values = self.q_network.predict(states, verbose=0)\n",
        "\n",
        "        # Compute next Q-values using target network\n",
        "        next_q_values = self.target_network.predict(next_states, verbose=0)\n",
        "\n",
        "        # Compute targets\n",
        "        targets = current_q_values.copy()\n",
        "        for i in range(self.batch_size):\n",
        "            if dones[i]:\n",
        "                targets[i][actions[i]] = rewards[i]\n",
        "            else:\n",
        "                targets[i][actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])\n",
        "\n",
        "        # Train the model\n",
        "        history = self.q_network.fit(states, targets, epochs=1, verbose=0)\n",
        "        loss = history.history['loss'][0]\n",
        "        self.losses.append(loss)\n",
        "\n",
        "        # Update target network periodically\n",
        "        self.training_step += 1\n",
        "        if self.training_step % self.target_update_freq == 0:\n",
        "            self.update_target_network()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"\n",
        "        Decay exploration rate.\n",
        "        \"\"\"\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        self.epsilon_history.append(self.epsilon)\n",
        "\n",
        "# Train DQN on CartPole\n",
        "def train_dqn_cartpole(episodes=1000):\n",
        "    \"\"\"\n",
        "    Train DQN agent on CartPole environment.\n",
        "    \"\"\"\n",
        "    # Create environment\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # Create DQN agent\n",
        "    agent = DQNAgent(state_size=state_size, action_size=action_size,\n",
        "                    lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995,\n",
        "                    epsilon_min=0.01, buffer_size=10000, batch_size=32)\n",
        "\n",
        "    scores = []\n",
        "    scores_window = deque(maxlen=100)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "\n",
        "        for step in range(500):  # Max steps per episode\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Store experience\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Train the agent\n",
        "        if len(agent.memory) > agent.batch_size:\n",
        "            agent.replay()\n",
        "\n",
        "        # Update exploration rate\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        scores.append(score)\n",
        "        scores_window.append(score)\n",
        "        agent.episode_rewards.append(score)\n",
        "\n",
        "        # Print progress\n",
        "        if episode % 100 == 0:\n",
        "            mean_score = np.mean(scores_window)\n",
        "            print(f\"Episode {episode}, Average Score: {mean_score:.2f}, \"\n",
        "                  f\"ε: {agent.epsilon:.3f}, Buffer Size: {len(agent.memory)}\")\n",
        "\n",
        "            # Check if solved\n",
        "            if mean_score >= 195.0:\n",
        "                print(f\"\\nEnvironment solved in {episode} episodes!\")\n",
        "                print(f\"Average Score: {mean_score:.2f}\")\n",
        "                break\n",
        "\n",
        "    env.close()\n",
        "    return agent, scores\n",
        "\n",
        "# Train the DQN agent\n",
        "print(\"Training DQN Agent on CartPole...\")\n",
        "dqn_agent, dqn_scores = train_dqn_cartpole(episodes=1000)\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Final ε: {dqn_agent.epsilon:.4f}\")\n",
        "print(f\"Buffer size: {len(dqn_agent.memory)}\")\n",
        "print(f\"Training steps: {dqn_agent.training_step}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqn_analysis"
      },
      "outputs": [],
      "source": [
        "# Analyze DQN Training Results\n",
        "\n",
        "def analyze_dqn_performance(agent, scores):\n",
        "    \"\"\"\n",
        "    Comprehensive analysis of DQN training.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    # Training progress\n",
        "    ax = axes[0, 0]\n",
        "    window_size = 100\n",
        "    moving_avg = pd.Series(scores).rolling(window_size).mean()\n",
        "    ax.plot(scores, alpha=0.3, color='blue', label='Episode Score')\n",
        "    ax.plot(moving_avg, color='red', linewidth=2, label=f'{window_size}-Episode Average')\n",
        "    ax.axhline(y=195, color='green', linestyle='--', label='Solved Threshold')\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('DQN Training Progress')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Epsilon decay\n",
        "    ax = axes[0, 1]\n",
        "    ax.plot(agent.epsilon_history)\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Epsilon')\n",
        "    ax.set_title('Exploration Rate Decay')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Loss progression\n",
        "    ax = axes[0, 2]\n",
        "    if agent.losses:\n",
        "        loss_ma = pd.Series(agent.losses).rolling(100).mean()\n",
        "        ax.plot(agent.losses, alpha=0.3, color='orange', label='Loss')\n",
        "        ax.plot(loss_ma, color='red', linewidth=2, label='100-Step Average')\n",
        "        ax.set_xlabel('Training Step')\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.set_title('Training Loss')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Score distribution over time\n",
        "    ax = axes[1, 0]\n",
        "    early_scores = scores[:len(scores)//3]\n",
        "    middle_scores = scores[len(scores)//3:2*len(scores)//3]\n",
        "    late_scores = scores[2*len(scores)//3:]\n",
        "\n",
        "    ax.hist(early_scores, bins=20, alpha=0.5, label='Early Training', color='red')\n",
        "    ax.hist(middle_scores, bins=20, alpha=0.5, label='Middle Training', color='orange')\n",
        "    ax.hist(late_scores, bins=20, alpha=0.5, label='Late Training', color='blue')\n",
        "    ax.set_xlabel('Score')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_title('Score Distribution Evolution')\n",
        "    ax.legend()\n",
        "\n",
        "    # Learning curve segments\n",
        "    ax = axes[1, 1]\n",
        "    segment_size = 50\n",
        "    segments = len(scores) // segment_size\n",
        "    segment_means = []\n",
        "    segment_stds = []\n",
        "\n",
        "    for i in range(segments):\n",
        "        start_idx = i * segment_size\n",
        "        end_idx = (i + 1) * segment_size\n",
        "        segment_scores = scores[start_idx:end_idx]\n",
        "        segment_means.append(np.mean(segment_scores))\n",
        "        segment_stds.append(np.std(segment_scores))\n",
        "\n",
        "    segment_episodes = [(i + 0.5) * segment_size for i in range(segments)]\n",
        "    ax.errorbar(segment_episodes, segment_means, yerr=segment_stds,\n",
        "                fmt='o-', capsize=5, capthick=2)\n",
        "    ax.axhline(y=195, color='green', linestyle='--', label='Solved Threshold')\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Mean Score ± Std')\n",
        "    ax.set_title('Learning Curve with Variance')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Performance comparison\n",
        "    ax = axes[1, 2]\n",
        "    final_performance = np.mean(scores[-100:])\n",
        "    random_performance = 20  # Approximate random performance\n",
        "\n",
        "    methods = ['Random', 'DQN']\n",
        "    performances = [random_performance, final_performance]\n",
        "    colors = ['red', 'blue']\n",
        "\n",
        "    bars = ax.bar(methods, performances, color=colors, alpha=0.7)\n",
        "    ax.axhline(y=195, color='green', linestyle='--', label='Solved Threshold')\n",
        "    ax.set_ylabel('Average Score')\n",
        "    ax.set_title('Final Performance Comparison')\n",
        "    ax.legend()\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, performances):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
        "               f'{value:.1f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nTraining Summary:\")\n",
        "    print(f\"Total Episodes: {len(scores)}\")\n",
        "    print(f\"Final 100 Episode Average: {np.mean(scores[-100:]):.2f} ± {np.std(scores[-100:]):.2f}\")\n",
        "    print(f\"Best Episode Score: {np.max(scores)}\")\n",
        "    print(f\"Episodes to solve (>195): {next((i for i, score in enumerate(pd.Series(scores).rolling(100).mean()) if score >= 195), 'Not solved')}\")\n",
        "\n",
        "    if agent.losses:\n",
        "        print(f\"Final Training Loss: {np.mean(agent.losses[-100:]):.4f}\")\n",
        "\n",
        "    print(f\"Memory Buffer Utilization: {len(agent.memory)}/{agent.memory.maxlen} ({len(agent.memory)/agent.memory.maxlen*100:.1f}%)\")\n",
        "\n",
        "# Test DQN agent performance\n",
        "def test_dqn_agent(agent, n_episodes=20):\n",
        "    \"\"\"\n",
        "    Test the trained DQN agent.\n",
        "    \"\"\"\n",
        "    env = gym.make('CartPole-v1')\n",
        "    test_scores = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "\n",
        "        for step in range(500):\n",
        "            action = agent.act_greedy(state)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            score += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        test_scores.append(score)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(f\"\\nTest Results ({n_episodes} episodes):\")\n",
        "    print(f\"Average Score: {np.mean(test_scores):.2f} ± {np.std(test_scores):.2f}\")\n",
        "    print(f\"Min Score: {np.min(test_scores)}\")\n",
        "    print(f\"Max Score: {np.max(test_scores)}\")\n",
        "    print(f\"Success Rate (>195): {np.mean([score >= 195 for score in test_scores]) * 100:.1f}%\")\n",
        "\n",
        "    return test_scores\n",
        "\n",
        "# Analyze Q-network predictions\n",
        "def analyze_q_network(agent):\n",
        "    \"\"\"\n",
        "    Analyze what the Q-network has learned.\n",
        "    \"\"\"\n",
        "    # Generate test states\n",
        "    n_samples = 1000\n",
        "    test_states = []\n",
        "\n",
        "    # Sample random states\n",
        "    for _ in range(n_samples):\n",
        "        cart_pos = np.random.uniform(-2.4, 2.4)\n",
        "        cart_vel = np.random.uniform(-3.0, 3.0)\n",
        "        pole_angle = np.random.uniform(-0.2, 0.2)\n",
        "        pole_vel = np.random.uniform(-3.0, 3.0)\n",
        "        test_states.append([cart_pos, cart_vel, pole_angle, pole_vel])\n",
        "\n",
        "    test_states = np.array(test_states)\n",
        "\n",
        "    # Get Q-values\n",
        "    q_values = agent.q_network.predict(test_states, verbose=0)\n",
        "\n",
        "    # Analyze Q-values\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    # Q-values vs cart position\n",
        "    ax = axes[0, 0]\n",
        "    ax.scatter(test_states[:, 0], q_values[:, 0], alpha=0.5, label='Left Action', s=10)\n",
        "    ax.scatter(test_states[:, 0], q_values[:, 1], alpha=0.5, label='Right Action', s=10)\n",
        "    ax.set_xlabel('Cart Position')\n",
        "    ax.set_ylabel('Q-Value')\n",
        "    ax.set_title('Q-Values vs Cart Position')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Q-values vs pole angle\n",
        "    ax = axes[0, 1]\n",
        "    ax.scatter(test_states[:, 2], q_values[:, 0], alpha=0.5, label='Left Action', s=10)\n",
        "    ax.scatter(test_states[:, 2], q_values[:, 1], alpha=0.5, label='Right Action', s=10)\n",
        "    ax.set_xlabel('Pole Angle (rad)')\n",
        "    ax.set_ylabel('Q-Value')\n",
        "    ax.set_title('Q-Values vs Pole Angle')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Action preferences\n",
        "    ax = axes[1, 0]\n",
        "    preferred_actions = np.argmax(q_values, axis=1)\n",
        "    action_counts = np.bincount(preferred_actions)\n",
        "    ax.bar(['Left (0)', 'Right (1)'], action_counts)\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_title('Preferred Action Distribution')\n",
        "\n",
        "    # Q-value distribution\n",
        "    ax = axes[1, 1]\n",
        "    ax.hist(q_values[:, 0], bins=30, alpha=0.5, label='Left Action Q-Values')\n",
        "    ax.hist(q_values[:, 1], bins=30, alpha=0.5, label='Right Action Q-Values')\n",
        "    ax.set_xlabel('Q-Value')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_title('Q-Value Distribution')\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"\\nQ-Network Analysis:\")\n",
        "    print(f\"Mean Q-Value (Left): {np.mean(q_values[:, 0]):.3f} ± {np.std(q_values[:, 0]):.3f}\")\n",
        "    print(f\"Mean Q-Value (Right): {np.mean(q_values[:, 1]):.3f} ± {np.std(q_values[:, 1]):.3f}\")\n",
        "    print(f\"Action Preference: {action_counts[0]/(action_counts[0]+action_counts[1])*100:.1f}% Left, {action_counts[1]/(action_counts[0]+action_counts[1])*100:.1f}% Right\")\n",
        "\n",
        "# Run all analyses\n",
        "print(\"\\nAnalyzing DQN Performance...\")\n",
        "analyze_dqn_performance(dqn_agent, dqn_scores)\n",
        "\n",
        "print(\"\\nTesting DQN Agent...\")\n",
        "test_scores = test_dqn_agent(dqn_agent, n_episodes=50)\n",
        "\n",
        "print(\"\\nAnalyzing Q-Network...\")\n",
        "analyze_q_network(dqn_agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqn-variants"
      },
      "source": [
        "# 11. DQN Variants and Improvements\n",
        "\n",
        "## Theoretical Foundation\n",
        "\n",
        "Several important improvements have been made to the basic DQN algorithm:\n",
        "\n",
        "### 1. Double DQN (DDQN)\n",
        "\n",
        "**Problem**: DQN tends to overestimate Q-values due to the max operator.\n",
        "\n",
        "**Solution**: Use the main network to select actions, target network to evaluate them:\n",
        "$$y = r + \\gamma Q(s', \\arg\\max_{a'} Q(s', a'; θ); θ^-)$$\n",
        "\n",
        "### 2. Dueling DQN\n",
        "\n",
        "**Innovation**: Separate value and advantage estimation:\n",
        "$$Q(s,a) = V(s) + A(s,a) - \\frac{1}{|A|}\\sum_{a'} A(s,a')$$\n",
        "\n",
        "Where:\n",
        "- $V(s)$: State value function\n",
        "- $A(s,a)$: Advantage function\n",
        "\n",
        "### 3. Prioritized Experience Replay (PER)\n",
        "\n",
        "**Innovation**: Sample experiences based on their TD error:\n",
        "$$P(i) = \\frac{p_i^α}{\\sum_k p_k^α}$$\n",
        "\n",
        "Where $p_i = |\\delta_i| + ε$ and $α$ controls prioritization strength.\n",
        "\n",
        "**Importance Sampling**: Correct for bias with weights:\n",
        "$$w_i = \\left(\\frac{1}{N} \\cdot \\frac{1}{P(i)}\\right)^β$$\n",
        "\n",
        "### 4. Multi-Step Learning\n",
        "\n",
        "**Innovation**: Use n-step returns instead of 1-step:\n",
        "$$y = \\sum_{k=0}^{n-1} \\gamma^k r_{t+k+1} + \\gamma^n \\max_{a'} Q(s_{t+n}, a'; θ^-)$$\n",
        "\n",
        "### 5. Noisy Networks\n",
        "\n",
        "**Innovation**: Add learnable noise to network weights for exploration:\n",
        "$$y = (μ^w + σ^w ⊙ ε^w)x + μ^b + σ^b ⊙ ε^b$$\n",
        "\n",
        "### 6. Rainbow DQN\n",
        "\n",
        "**Combination**: Integrates multiple improvements:\n",
        "- Double DQN\n",
        "- Dueling Networks\n",
        "- Prioritized Experience Replay\n",
        "- Multi-step Learning\n",
        "- Noisy Networks\n",
        "- Distributional RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqn_variants_implementation"
      },
      "outputs": [],
      "source": [
        "# Implementation of DQN Variants\n",
        "\n",
        "class DoubleDQNAgent(DQNAgent):\n",
        "    \"\"\"\n",
        "    Double DQN implementation.\n",
        "    \"\"\"\n",
        "    def replay(self):\n",
        "        \"\"\"\n",
        "        Train using Double DQN update rule.\n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample random batch\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states = np.array([e[0] for e in batch])\n",
        "        actions = np.array([e[1] for e in batch])\n",
        "        rewards = np.array([e[2] for e in batch])\n",
        "        next_states = np.array([e[3] for e in batch])\n",
        "        dones = np.array([e[4] for e in batch])\n",
        "\n",
        "        # Compute current Q-values\n",
        "        current_q_values = self.q_network.predict(states, verbose=0)\n",
        "\n",
        "        # Double DQN: Use main network to select actions\n",
        "        next_q_values_main = self.q_network.predict(next_states, verbose=0)\n",
        "        next_actions = np.argmax(next_q_values_main, axis=1)\n",
        "\n",
        "        # Use target network to evaluate selected actions\n",
        "        next_q_values_target = self.target_network.predict(next_states, verbose=0)\n",
        "\n",
        "        # Compute targets\n",
        "        targets = current_q_values.copy()\n",
        "        for i in range(self.batch_size):\n",
        "            if dones[i]:\n",
        "                targets[i][actions[i]] = rewards[i]\n",
        "            else:\n",
        "                # Double DQN target\n",
        "                targets[i][actions[i]] = rewards[i] + self.gamma * next_q_values_target[i][next_actions[i]]\n",
        "\n",
        "        # Train the model\n",
        "        history = self.q_network.fit(states, targets, epochs=1, verbose=0)\n",
        "        loss = history.history['loss'][0]\n",
        "        self.losses.append(loss)\n",
        "\n",
        "        # Update target network periodically\n",
        "        self.training_step += 1\n",
        "        if self.training_step % self.target_update_freq == 0:\n",
        "            self.update_target_network()\n",
        "\n",
        "        return loss\n",
        "\n",
        "class DuelingDQNAgent(DQNAgent):\n",
        "    \"\"\"\n",
        "    Dueling DQN implementation.\n",
        "    \"\"\"\n",
        "    def _build_model(self):\n",
        "        \"\"\"\n",
        "        Build dueling network architecture.\n",
        "        \"\"\"\n",
        "        # Input layer\n",
        "        inputs = keras.layers.Input(shape=(self.state_size,))\n",
        "\n",
        "        # Shared layers\n",
        "        shared = keras.layers.Dense(64, activation='relu')(inputs)\n",
        "        shared = keras.layers.Dense(64, activation='relu')(shared)\n",
        "\n",
        "        # Value stream\n",
        "        value_stream = keras.layers.Dense(32, activation='relu')(shared)\n",
        "        value = keras.layers.Dense(1, activation='linear')(value_stream)\n",
        "\n",
        "        # Advantage stream\n",
        "        advantage_stream = keras.layers.Dense(32, activation='relu')(shared)\n",
        "        advantage = keras.layers.Dense(self.action_size, activation='linear')(advantage_stream)\n",
        "\n",
        "        # Combine value and advantage\n",
        "        # Q(s,a) = V(s) + A(s,a) - mean(A(s,a))\n",
        "        mean_advantage = keras.layers.Lambda(\n",
        "            lambda x: tf.reduce_mean(x, axis=1, keepdims=True)\n",
        "        )(advantage)\n",
        "\n",
        "        q_values = keras.layers.Add()([value, advantage])\n",
        "        q_values = keras.layers.Subtract()([q_values, mean_advantage])\n",
        "\n",
        "        model = keras.Model(inputs=inputs, outputs=q_values)\n",
        "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=self.lr),\n",
        "                     loss='mse')\n",
        "\n",
        "        return model\n",
        "\n",
        "class PrioritizedReplayBuffer:\n",
        "    \"\"\"\n",
        "    Prioritized Experience Replay buffer.\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity, alpha=0.6, beta=0.4, beta_increment=0.001):\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.beta_increment = beta_increment\n",
        "\n",
        "        self.buffer = []\n",
        "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
        "        self.position = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add experience with maximum priority.\"\"\"\n",
        "        max_priority = np.max(self.priorities) if self.size > 0 else 1.0\n",
        "\n",
        "        if self.size < self.capacity:\n",
        "            self.buffer.append((state, action, reward, next_state, done))\n",
        "            self.size += 1\n",
        "        else:\n",
        "            self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "\n",
        "        self.priorities[self.position] = max_priority\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample batch with prioritized sampling.\"\"\"\n",
        "        if self.size < batch_size:\n",
        "            return None, None, None\n",
        "\n",
        "        # Compute sampling probabilities\n",
        "        priorities = self.priorities[:self.size]\n",
        "        probabilities = priorities ** self.alpha\n",
        "        probabilities /= probabilities.sum()\n",
        "\n",
        "        # Sample indices\n",
        "        indices = np.random.choice(self.size, batch_size, p=probabilities)\n",
        "\n",
        "        # Sample experiences\n",
        "        experiences = [self.buffer[i] for i in indices]\n",
        "\n",
        "        # Compute importance sampling weights\n",
        "        weights = (self.size * probabilities[indices]) ** (-self.beta)\n",
        "        weights /= weights.max()  # Normalize\n",
        "\n",
        "        # Update beta\n",
        "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
        "\n",
        "        return experiences, indices, weights\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        \"\"\"Update priorities for sampled experiences.\"\"\"\n",
        "        for i, priority in zip(indices, priorities):\n",
        "            self.priorities[i] = priority + 1e-6  # Add small epsilon\n",
        "\n",
        "# Compare DQN variants\n",
        "def compare_dqn_variants(episodes=500):\n",
        "    \"\"\"\n",
        "    Compare different DQN variants.\n",
        "    \"\"\"\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # Create different agents\n",
        "    agents = {\n",
        "        'DQN': DQNAgent(state_size, action_size, lr=0.001),\n",
        "        'Double DQN': DoubleDQNAgent(state_size, action_size, lr=0.001),\n",
        "        'Dueling DQN': DuelingDQNAgent(state_size, action_size, lr=0.001)\n",
        "    }\n",
        "\n",
        "    results = {name: [] for name in agents.keys()}\n",
        "\n",
        "    # Train each agent\n",
        "    for name, agent in agents.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        scores = []\n",
        "\n",
        "        for episode in range(episodes):\n",
        "            state = env.reset()\n",
        "            score = 0\n",
        "\n",
        "            for step in range(500):\n",
        "                action = agent.act(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "                if len(agent.memory) > agent.batch_size:\n",
        "                    agent.replay()\n",
        "\n",
        "                state = next_state\n",
        "                score += reward\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "agent.decay_epsilon()\n",
        "scores.append(score)\n",
        "\n",
        "if episode % 100 == 0:\n",
        "    mean_score = np.mean(scores[-100:]) if len(scores) >= 100 else np.mean(scores)\n",
        "    print(f\"Episode {episode}, Average Score: {mean_score:.2f}\")\n",
        "\n",
        "results[name] = scores\n",
        "env.close()\n",
        "return results\n",
        "# Visualize comparison results\n",
        "def plot_dqn_comparison(results):\n",
        "    \"\"\"Plot comparison of different DQN variants.\"\"\"\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    colors = ['blue', 'red', 'green', 'orange']\n",
        "    window_size = 50\n",
        "\n",
        "    for i, (name, scores) in enumerate(results.items()):\n",
        "        moving_avg = pd.Series(scores).rolling(window_size).mean()\n",
        "        plt.plot(scores, alpha=0.3, color=colors[i])\n",
        "        plt.plot(moving_avg, color=colors[i], linewidth=2, label=f'{name} (MA)')\n",
        "\n",
        "    plt.axhline(y=195, color='black', linestyle='--', label='Solved Threshold')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('DQN Variants Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # Performance summary\n",
        "    print(\"\\nFinal Performance Summary:\")\n",
        "    for name, scores in results.items():\n",
        "        final_performance = np.mean(scores[-100:])\n",
        "        episodes_to_solve = next((i for i, score in enumerate(pd.Series(scores).rolling(100).mean())\n",
        "                                if score >= 195), len(scores))\n",
        "        print(f\"{name:12}: {final_performance:.1f} ± {np.std(scores[-100:]):.1f} \"\n",
        "              f\"(solved in {episodes_to_solve} episodes)\")\n",
        "print(\"\\nComparing DQN Variants...\")\n",
        "variant_results = compare_dqn_variants(episodes=300)\n",
        "plot_dqn_comparison(variant_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzerFMhKXZaG"
      },
      "source": [
        "# 12. TF-Agents Library\n",
        "\n",
        "## Theoretical Foundation\n",
        "\n",
        "**TF-Agents** is Google's reinforcement learning library built on TensorFlow. It provides:\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "1. **Environments**: Standardized interfaces for RL environments\n",
        "2. **Agents**: Pre-implemented RL algorithms (DQN, PPO, SAC, etc.)\n",
        "3. **Policies**: Action selection strategies\n",
        "4. **Replay Buffers**: Experience storage and sampling\n",
        "5. **Metrics**: Training and evaluation metrics\n",
        "6. **Drivers**: Training loop orchestration\n",
        "\n",
        "### Architecture Benefits:\n",
        "\n",
        "- **Modular Design**: Mix and match components\n",
        "- **TensorFlow Integration**: GPU acceleration and distributed training\n",
        "- **Production Ready**: Scalable and robust implementations\n",
        "- **Research Friendly**: Easy to extend and experiment\n",
        "\n",
        "### Typical Workflow:\n",
        "\n",
        "```python\n",
        "# 1. Create environment\n",
        "env = suite_gym.load('CartPole-v1')\n",
        "\n",
        "# 2. Create agent\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    env.time_step_spec(),\n",
        "    env.action_spec(),\n",
        "    q_network=q_net)\n",
        "\n",
        "# 3. Create replay buffer\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=env.batch_size,\n",
        "    max_length=100000)\n",
        "\n",
        "# 4. Create driver for data collection\n",
        "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
        "    env, agent.collect_policy, [replay_buffer.add_batch])\n",
        "\n",
        "# 5. Training loop\n",
        "for _ in range(num_iterations):\n",
        "    collect_driver.run()\n",
        "    experience = replay_buffer.gather_all()\n",
        "    train_loss = agent.train(experience)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk75w-VaXZaG"
      },
      "outputs": [],
      "source": [
        "# Install TF-Agents (uncomment if needed)\n",
        "# !pip install tf-agents[reverb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE2ZocQxXZaG"
      },
      "outputs": [],
      "source": [
        "# TF-Agents imports\n",
        "import tensorflow as tf\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.eval import metric_utils\n",
        "# TF-Agents DQN Implementation\n",
        "class TFAgentsDQN:\n",
        "    def __init__(self, env_name='CartPole-v1'):\n",
        "        # Create environments\n",
        "        self.train_py_env = suite_gym.load(env_name)\n",
        "        self.eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "        self.train_env = tf_py_environment.TFPyEnvironment(self.train_py_env)\n",
        "        self.eval_env = tf_py_environment.TFPyEnvironment(self.eval_py_env)\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.num_iterations = 20000\n",
        "        self.initial_collect_steps = 100\n",
        "        self.collect_steps_per_iteration = 1\n",
        "        self.replay_buffer_max_length = 100000\n",
        "        self.batch_size = 64\n",
        "        self.learning_rate = 1e-3\n",
        "        self.log_interval = 200\n",
        "        self.num_eval_episodes = 10\n",
        "        self.eval_interval = 1000\n",
        "\n",
        "        self._setup_agent()\n",
        "        self._setup_replay_buffer()\n",
        "        self._setup_drivers()\n",
        "\n",
        "    def _setup_agent(self):\n",
        "        \"\"\"Setup the DQN agent.\"\"\"\n",
        "        # Q-Network\n",
        "        fc_layer_params = (100, 50)\n",
        "\n",
        "        self.q_net = q_network.QNetwork(\n",
        "            self.train_env.observation_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            fc_layer_params=fc_layer_params)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "        # DQN Agent\n",
        "        self.agent = dqn_agent.DqnAgent(\n",
        "            self.train_env.time_step_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            q_network=self.q_net,\n",
        "            optimizer=optimizer,\n",
        "            td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "            train_step_counter=tf.Variable(0))\n",
        "\n",
        "        self.agent.initialize()\n",
        "\n",
        "    def _setup_replay_buffer(self):\n",
        "        \"\"\"Setup replay buffer.\"\"\"\n",
        "        self.replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "            data_spec=self.agent.collect_data_spec,\n",
        "            batch_size=self.train_env.batch_size,\n",
        "            max_length=self.replay_buffer_max_length)\n",
        "\n",
        "    def _setup_drivers(self):\n",
        "        \"\"\"Setup data collection drivers.\"\"\"\n",
        "        # Random policy for initial data collection\n",
        "        self.random_policy = random_tf_policy.RandomTFPolicy(\n",
        "            self.train_env.time_step_spec(),\n",
        "            self.train_env.action_spec())\n",
        "\n",
        "        # Collect driver\n",
        "        self.collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
        "            self.train_env,\n",
        "            self.agent.collect_policy,\n",
        "            observers=[self.replay_buffer.add_batch],\n",
        "            num_steps=self.collect_steps_per_iteration)\n",
        "\n",
        "        # Initial data collection\n",
        "        self.initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
        "            self.train_env,\n",
        "            self.random_policy,\n",
        "            observers=[self.replay_buffer.add_batch],\n",
        "            num_steps=self.initial_collect_steps)\n",
        "\n",
        "    def collect_initial_data(self):\n",
        "        \"\"\"Collect initial random data.\"\"\"\n",
        "        print(\"Collecting initial data...\")\n",
        "        self.initial_collect_driver.run()\n",
        "\n",
        "    def compute_avg_return(self, num_episodes=10):\n",
        "        \"\"\"Compute average return over episodes.\"\"\"\n",
        "        total_return = 0.0\n",
        "        for _ in range(num_episodes):\n",
        "            time_step = self.eval_env.reset()\n",
        "            episode_return = 0.0\n",
        "\n",
        "            while not time_step.is_last():\n",
        "                action_step = self.agent.policy.action(time_step)\n",
        "                time_step = self.eval_env.step(action_step.action)\n",
        "                episode_return += time_step.reward\n",
        "            total_return += episode_return\n",
        "\n",
        "        avg_return = total_return / num_episodes\n",
        "        return avg_return.numpy()[0]\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the agent.\"\"\"\n",
        "        # Collect initial data\n",
        "        self.collect_initial_data()\n",
        "\n",
        "        # Create dataset\n",
        "        dataset = self.replay_buffer.as_dataset(\n",
        "            num_parallel_calls=3,\n",
        "            sample_batch_size=self.batch_size,\n",
        "            num_steps=2).prefetch(3)\n",
        "\n",
        "        iterator = iter(dataset)\n",
        "\n",
        "        # Training metrics\n",
        "        returns = []\n",
        "        losses = []\n",
        "\n",
        "        # Training loop\n",
        "        print(\"Starting training...\")\n",
        "        for iteration in range(self.num_iterations):\n",
        "            # Collect data\n",
        "            self.collect_driver.run()\n",
        "\n",
        "            # Train\n",
        "            experience, unused_info = next(iterator)\n",
        "            train_loss = self.agent.train(experience).loss\n",
        "\n",
        "            step = self.agent.train_step_counter.numpy()\n",
        "\n",
        "            if step % self.log_interval == 0:\n",
        "                losses.append(train_loss.numpy())\n",
        "                print(f'Step {step}: loss = {train_loss.numpy():.3f}')\n",
        "\n",
        "            if step % self.eval_interval == 0:\n",
        "                avg_return = self.compute_avg_return(self.num_eval_episodes)\n",
        "                returns.append(avg_return)\n",
        "                print(f'Step {step}: Average Return = {avg_return:.2f}')\n",
        "\n",
        "        return returns, losses\n",
        "# Train TF-Agents DQN\n",
        "print(\"Training TF-Agents DQN...\")\n",
        "tf_agents_dqn = TFAgentsDQN()\n",
        "returns, losses = tf_agents_dqn.train()\n",
        "# Plot TF-Agents results\n",
        "def plot_tf_agents_results(returns, losses):\n",
        "    \"\"\"Plot TF-Agents training results.\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Returns plot\n",
        "    iterations = range(0, len(returns) * 1000, 1000)\n",
        "    ax1.plot(iterations, returns, 'b-', linewidth=2)\n",
        "    ax1.axhline(y=195, color='red', linestyle='--', label='Solved Threshold')\n",
        "    ax1.set_xlabel('Training Steps')\n",
        "    ax1.set_ylabel('Average Return')\n",
        "    ax1.set_title('TF-Agents DQN Performance')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Loss plot\n",
        "    loss_iterations = range(0, len(losses) * 200, 200)\n",
        "    ax2.plot(loss_iterations, losses, 'r-', linewidth=2)\n",
        "    ax2.set_xlabel('Training Steps')\n",
        "    ax2.set_ylabel('Training Loss')\n",
        "    ax2.set_title('TF-Agents DQN Training Loss')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Performance summary\n",
        "    final_return = returns[-1] if returns else 0\n",
        "    solved_step = next((i * 1000 for i, ret in enumerate(returns) if ret >= 195), None)\n",
        "\n",
        "    print(f\"\\nTF-Agents DQN Results:\")\n",
        "    print(f\"Final Average Return: {final_return:.2f}\")\n",
        "    if solved_step:\n",
        "        print(f\"Environment solved at step: {solved_step}\")\n",
        "    else:\n",
        "        print(\"Environment not solved in training period\")\n",
        "\n",
        "plot_tf_agents_results(returns, losses)\n",
        "# Advanced TF-Agents Features\n",
        "class AdvancedTFAgentsDQN(TFAgentsDQN):\n",
        "    def __init__(self, env_name='CartPole-v1', use_double_dqn=True):\n",
        "        self.use_double_dqn = use_double_dqn\n",
        "        super().__init__(env_name)\n",
        "\n",
        "    def _setup_agent(self):\n",
        "        \"\"\"Setup advanced DQN agent with more features.\"\"\"\n",
        "        # Improved Q-Network with dropout\n",
        "        fc_layer_params = (128, 64, 32)\n",
        "        dropout_layer_params = (0.2, 0.2, 0.1)\n",
        "\n",
        "        self.q_net = q_network.QNetwork(\n",
        "            self.train_env.observation_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            fc_layer_params=fc_layer_params,\n",
        "            dropout_layer_params=dropout_layer_params)\n",
        "\n",
        "        # Target Q-Network for Double DQN\n",
        "        if self.use_double_dqn:\n",
        "            self.target_q_net = q_network.QNetwork(\n",
        "                self.train_env.observation_spec(),\n",
        "                self.train_env.action_spec(),\n",
        "                fc_layer_params=fc_layer_params,\n",
        "                dropout_layer_params=dropout_layer_params)\n",
        "\n",
        "        # Learning rate schedule\n",
        "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            initial_learning_rate=1e-3,\n",
        "            decay_steps=1000,\n",
        "            decay_rate=0.96)\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "        # Advanced DQN Agent\n",
        "        self.agent = dqn_agent.DqnAgent(\n",
        "            self.train_env.time_step_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            q_network=self.q_net,\n",
        "            target_q_network=self.target_q_net if self.use_double_dqn else None,\n",
        "            optimizer=optimizer,\n",
        "            epsilon_greedy=0.1,\n",
        "            target_update_tau=0.005,\n",
        "            target_update_period=100,\n",
        "            td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "            gamma=0.99,\n",
        "            reward_scale_factor=1.0,\n",
        "            gradient_clipping=None,\n",
        "            train_step_counter=tf.Variable(0))\n",
        "\n",
        "        self.agent.initialize()\n",
        "# Compare basic vs advanced TF-Agents implementation\n",
        "print(\"Training Advanced TF-Agents DQN...\")\n",
        "advanced_dqn = AdvancedTFAgentsDQN(use_double_dqn=True)\n",
        "advanced_returns, advanced_losses = advanced_dqn.train()\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "iterations = range(0, len(returns) * 1000, 1000)\n",
        "plt.plot(iterations, returns, 'b-', label='Basic DQN', linewidth=2)\n",
        "plt.plot(iterations, advanced_returns, 'r-', label='Advanced DQN', linewidth=2)\n",
        "plt.axhline(y=195, color='black', linestyle='--', label='Solved')\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Average Return')\n",
        "plt.title('TF-Agents DQN Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "loss_iterations = range(0, len(losses) * 200, 200)\n",
        "plt.plot(loss_iterations, losses, 'b-', label='Basic DQN', linewidth=2)\n",
        "plt.plot(loss_iterations, advanced_losses, 'r-', label='Advanced DQN', linewidth=2)\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nComparison Results:\")\n",
        "print(f\"Basic DQN Final Return: {returns[-1]:.2f}\")\n",
        "print(f\"Advanced DQN Final Return: {advanced_returns[-1]:.2f}\")\n",
        "# TF-Agents Benefits Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TF-AGENTS BENEFITS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"✓ Production-Ready Implementation\")\n",
        "print(\"  - Optimized TensorFlow operations\")\n",
        "print(\"  - GPU acceleration support\")\n",
        "print(\"  - Distributed training capabilities\")\n",
        "\n",
        "print(\"\\n✓ Modular Architecture\")\n",
        "print(\"  - Easy to swap components\")\n",
        "print(\"  - Extensible for research\")\n",
        "print(\"  - Clean separation of concerns\")\n",
        "\n",
        "print(\"\\n✓ Advanced Features\")\n",
        "print(\"  - Multiple RL algorithms (DQN, PPO, SAC, etc.)\")\n",
        "print(\"  - Sophisticated replay buffers\")\n",
        "print(\"  - Built-in metrics and logging\")\n",
        "print(\"  - Policy evaluation utilities\")\n",
        "\n",
        "print(\"\\n✓ Robust Implementation\")\n",
        "print(\"  - Handles edge cases\")\n",
        "print(\"  - Numerical stability\")\n",
        "print(\"  - Memory efficient\")\n",
        "print(\"  - Well-tested codebase\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXERCISE COMPLETE!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "437d6hjiXZaH"
      },
      "outputs": [],
      "source": [
        "# DQN Training Loop continuation\n",
        "agent.decay_epsilon()\n",
        "scores.append(score)\n",
        "\n",
        "if episode % 100 == 0:\n",
        "    mean_score = np.mean(scores[-100:]) if len(scores) >= 100 else np.mean(scores)\n",
        "    print(f\"Episode {episode}, Average Score: {mean_score:.2f}\")\n",
        "\n",
        "results[name] = scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rhADH0fXZaH"
      },
      "outputs": [],
      "source": [
        "env.close()\n",
        "return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSh9SkCFXZaH"
      },
      "outputs": [],
      "source": [
        "# Visualize comparison results\n",
        "def plot_dqn_comparison(results):\n",
        "    \"\"\"Plot comparison of different DQN variants.\"\"\"\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    colors = ['blue', 'red', 'green', 'orange']\n",
        "    window_size = 50\n",
        "\n",
        "    for i, (name, scores) in enumerate(results.items()):\n",
        "        moving_avg = pd.Series(scores).rolling(window_size).mean()\n",
        "        plt.plot(scores, alpha=0.3, color=colors[i])\n",
        "        plt.plot(moving_avg, color=colors[i], linewidth=2, label=f'{name} (MA)')\n",
        "\n",
        "    plt.axhline(y=195, color='black', linestyle='--', label='Solved Threshold')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('DQN Variants Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # Performance summary\n",
        "    print(\"\\nFinal Performance Summary:\")\n",
        "    for name, scores in results.items():\n",
        "        final_performance = np.mean(scores[-100:])\n",
        "        episodes_to_solve = next((i for i, score in enumerate(pd.Series(scores).rolling(100).mean())\n",
        "                                if score >= 195), len(scores))\n",
        "        print(f\"{name:12}: {final_performance:.1f} ± {np.std(scores[-100:]):.1f} \"\n",
        "              f\"(solved in {episodes_to_solve} episodes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZRVHJpcXZaH"
      },
      "outputs": [],
      "source": [
        "print(\"\\nComparing DQN Variants...\")\n",
        "variant_results = compare_dqn_variants(episodes=300)\n",
        "plot_dqn_comparison(variant_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fybLgTffXZaH"
      },
      "source": [
        "# 12. TF-Agents Library\n",
        "\n",
        "## Theoretical Foundation\n",
        "\n",
        "**TF-Agents** is Google's reinforcement learning library built on TensorFlow. It provides:\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "1. **Environments**: Standardized interfaces for RL environments\n",
        "2. **Agents**: Pre-implemented RL algorithms (DQN, PPO, SAC, etc.)\n",
        "3. **Policies**: Action selection strategies\n",
        "4. **Replay Buffers**: Experience storage and sampling\n",
        "5. **Metrics**: Training and evaluation metrics\n",
        "6. **Drivers**: Training loop orchestration\n",
        "\n",
        "### Architecture Benefits:\n",
        "\n",
        "- **Modular Design**: Mix and match components\n",
        "- **TensorFlow Integration**: GPU acceleration and distributed training\n",
        "- **Production Ready**: Scalable and robust implementations\n",
        "- **Research Friendly**: Easy to extend and experiment\n",
        "\n",
        "### Typical Workflow:\n",
        "\n",
        "```python\n",
        "# 1. Create environment\n",
        "env = suite_gym.load('CartPole-v1')\n",
        "\n",
        "# 2. Create agent\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    env.time_step_spec(),\n",
        "    env.action_spec(),\n",
        "    q_network=q_net)\n",
        "\n",
        "# 3. Create replay buffer\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=env.batch_size,\n",
        "    max_length=100000)\n",
        "\n",
        "# 4. Create driver for data collection\n",
        "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
        "    env, agent.collect_policy, [replay_buffer.add_batch])\n",
        "\n",
        "# 5. Training loop\n",
        "for _ in range(num_iterations):\n",
        "    collect_driver.run()\n",
        "    experience = replay_buffer.gather_all()\n",
        "    train_loss = agent.train(experience)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj-4EEcYXZaH"
      },
      "outputs": [],
      "source": [
        "# Install TF-Agents (uncomment if needed)\n",
        "# !pip install tf-agents[reverb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQI8Z8XFXZaH"
      },
      "outputs": [],
      "source": [
        "# TF-Agents imports\n",
        "import tensorflow as tf\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.eval import metric_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JClfJgOYXZaI"
      },
      "outputs": [],
      "source": [
        "# TF-Agents DQN Implementation\n",
        "class TFAgentsDQN:\n",
        "    def __init__(self, env_name='CartPole-v1'):\n",
        "        # Create environments\n",
        "        self.train_py_env = suite_gym.load(env_name)\n",
        "        self.eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "        self.train_env = tf_py_environment.TFPyEnvironment(self.train_py_env)\n",
        "        self.eval_env = tf_py_environment.TFPyEnvironment(self.eval_py_env)\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.num_iterations = 20000\n",
        "        self.initial_collect_steps = 100\n",
        "        self.collect_steps_per_iteration = 1\n",
        "        self.replay_buffer_max_length = 100000\n",
        "        self.batch_size = 64\n",
        "        self.learning_rate = 1e-3\n",
        "        self.log_interval = 200\n",
        "        self.num_eval_episodes = 10\n",
        "        self.eval_interval = 1000\n",
        "\n",
        "        self._setup_agent()\n",
        "        self._setup_replay_buffer()\n",
        "        self._setup_drivers()\n",
        "\n",
        "    def _setup_agent(self):\n",
        "        \"\"\"Setup the DQN agent.\"\"\"\n",
        "        # Q-Network\n",
        "        fc_layer_params = (100, 50)\n",
        "\n",
        "        self.q_net = q_network.QNetwork(\n",
        "            self.train_env.observation_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            fc_layer_params=fc_layer_params)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "        # DQN Agent\n",
        "        self.agent = dqn_agent.DqnAgent(\n",
        "            self.train_env.time_step_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            q_network=self.q_net,\n",
        "            optimizer=optimizer,\n",
        "            td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "            train_step_counter=tf.Variable(0))\n",
        "\n",
        "        self.agent.initialize()\n",
        "\n",
        "    def _setup_replay_buffer(self):\n",
        "        \"\"\"Setup replay buffer.\"\"\"\n",
        "        self.replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "            data_spec=self.agent.collect_data_spec,\n",
        "            batch_size=self.train_env.batch_size,\n",
        "            max_length=self.replay_buffer_max_length)\n",
        "\n",
        "    def _setup_drivers(self):\n",
        "        \"\"\"Setup data collection drivers.\"\"\"\n",
        "        # Random policy for initial data collection\n",
        "        self.random_policy = random_tf_policy.RandomTFPolicy(\n",
        "            self.train_env.time_step_spec(),\n",
        "            self.train_env.action_spec())\n",
        "\n",
        "        # Collect driver\n",
        "        self.collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
        "            self.train_env,\n",
        "            self.agent.collect_policy,\n",
        "            observers=[self.replay_buffer.add_batch],\n",
        "            num_steps=self.collect_steps_per_iteration)\n",
        "\n",
        "        # Initial data collection\n",
        "        self.initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
        "            self.train_env,\n",
        "            self.random_policy,\n",
        "            observers=[self.replay_buffer.add_batch],\n",
        "            num_steps=self.initial_collect_steps)\n",
        "\n",
        "    def collect_initial_data(self):\n",
        "        \"\"\"Collect initial random data.\"\"\"\n",
        "        print(\"Collecting initial data...\")\n",
        "        self.initial_collect_driver.run()\n",
        "\n",
        "    def compute_avg_return(self, num_episodes=10):\n",
        "        \"\"\"Compute average return over episodes.\"\"\"\n",
        "        total_return = 0.0\n",
        "        for _ in range(num_episodes):\n",
        "            time_step = self.eval_env.reset()\n",
        "            episode_return = 0.0\n",
        "\n",
        "            while not time_step.is_last():\n",
        "                action_step = self.agent.policy.action(time_step)\n",
        "                time_step = self.eval_env.step(action_step.action)\n",
        "                episode_return += time_step.reward\n",
        "            total_return += episode_return\n",
        "\n",
        "        avg_return = total_return / num_episodes\n",
        "        return avg_return.numpy()[0]\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the agent.\"\"\"\n",
        "        # Collect initial data\n",
        "        self.collect_initial_data()\n",
        "\n",
        "        # Create dataset\n",
        "        dataset = self.replay_buffer.as_dataset(\n",
        "            num_parallel_calls=3,\n",
        "            sample_batch_size=self.batch_size,\n",
        "            num_steps=2).prefetch(3)\n",
        "\n",
        "        iterator = iter(dataset)\n",
        "\n",
        "        # Training metrics\n",
        "        returns = []\n",
        "        losses = []\n",
        "\n",
        "        # Training loop\n",
        "        print(\"Starting training...\")\n",
        "        for iteration in range(self.num_iterations):\n",
        "            # Collect data\n",
        "            self.collect_driver.run()\n",
        "\n",
        "            # Train\n",
        "            experience, unused_info = next(iterator)\n",
        "            train_loss = self.agent.train(experience).loss\n",
        "\n",
        "            step = self.agent.train_step_counter.numpy()\n",
        "\n",
        "            if step % self.log_interval == 0:\n",
        "                losses.append(train_loss.numpy())\n",
        "                print(f'Step {step}: loss = {train_loss.numpy():.3f}')\n",
        "\n",
        "            if step % self.eval_interval == 0:\n",
        "                avg_return = self.compute_avg_return(self.num_eval_episodes)\n",
        "                returns.append(avg_return)\n",
        "                print(f'Step {step}: Average Return = {avg_return:.2f}')\n",
        "\n",
        "        return returns, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArP6ONhgXZaI"
      },
      "outputs": [],
      "source": [
        "# Train TF-Agents DQN\n",
        "print(\"Training TF-Agents DQN...\")\n",
        "tf_agents_dqn = TFAgentsDQN()\n",
        "returns, losses = tf_agents_dqn.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0S49VnLXZaI"
      },
      "outputs": [],
      "source": [
        "# Plot TF-Agents results\n",
        "def plot_tf_agents_results(returns, losses):\n",
        "    \"\"\"Plot TF-Agents training results.\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Returns plot\n",
        "    iterations = range(0, len(returns) * 1000, 1000)\n",
        "    ax1.plot(iterations, returns, 'b-', linewidth=2)\n",
        "    ax1.axhline(y=195, color='red', linestyle='--', label='Solved Threshold')\n",
        "    ax1.set_xlabel('Training Steps')\n",
        "    ax1.set_ylabel('Average Return')\n",
        "    ax1.set_title('TF-Agents DQN Performance')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Loss plot\n",
        "    loss_iterations = range(0, len(losses) * 200, 200)\n",
        "    ax2.plot(loss_iterations, losses, 'r-', linewidth=2)\n",
        "    ax2.set_xlabel('Training Steps')\n",
        "    ax2.set_ylabel('Training Loss')\n",
        "    ax2.set_title('TF-Agents DQN Training Loss')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Performance summary\n",
        "    final_return = returns[-1] if returns else 0\n",
        "    solved_step = next((i * 1000 for i, ret in enumerate(returns) if ret >= 195), None)\n",
        "\n",
        "    print(f\"\\nTF-Agents DQN Results:\")\n",
        "    print(f\"Final Average Return: {final_return:.2f}\")\n",
        "    if solved_step:\n",
        "        print(f\"Environment solved at step: {solved_step}\")\n",
        "    else:\n",
        "        print(\"Environment not solved in training period\")\n",
        "\n",
        "plot_tf_agents_results(returns, losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUE9E-xtXZaI"
      },
      "outputs": [],
      "source": [
        "# Advanced TF-Agents Features\n",
        "class AdvancedTFAgentsDQN(TFAgentsDQN):\n",
        "    def __init__(self, env_name='CartPole-v1', use_double_dqn=True):\n",
        "        self.use_double_dqn = use_double_dqn\n",
        "        super().__init__(env_name)\n",
        "\n",
        "    def _setup_agent(self):\n",
        "        \"\"\"Setup advanced DQN agent with more features.\"\"\"\n",
        "        # Improved Q-Network with dropout\n",
        "        fc_layer_params = (128, 64, 32)\n",
        "        dropout_layer_params = (0.2, 0.2, 0.1)\n",
        "\n",
        "        self.q_net = q_network.QNetwork(\n",
        "            self.train_env.observation_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            fc_layer_params=fc_layer_params,\n",
        "            dropout_layer_params=dropout_layer_params)\n",
        "\n",
        "        # Target Q-Network for Double DQN\n",
        "        if self.use_double_dqn:\n",
        "            self.target_q_net = q_network.QNetwork(\n",
        "                self.train_env.observation_spec(),\n",
        "                self.train_env.action_spec(),\n",
        "                fc_layer_params=fc_layer_params,\n",
        "                dropout_layer_params=dropout_layer_params)\n",
        "\n",
        "        # Learning rate schedule\n",
        "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            initial_learning_rate=1e-3,\n",
        "            decay_steps=1000,\n",
        "            decay_rate=0.96)\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "        # Advanced DQN Agent\n",
        "        self.agent = dqn_agent.DqnAgent(\n",
        "            self.train_env.time_step_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            q_network=self.q_net,\n",
        "            target_q_network=self.target_q_net if self.use_double_dqn else None,\n",
        "            optimizer=optimizer,\n",
        "            epsilon_greedy=0.1,\n",
        "            target_update_tau=0.005,\n",
        "            target_update_period=100,\n",
        "            td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "            gamma=0.99,\n",
        "            reward_scale_factor=1.0,\n",
        "            gradient_clipping=None,\n",
        "            train_step_counter=tf.Variable(0))\n",
        "\n",
        "        self.agent.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6bMMdgnXZaI"
      },
      "outputs": [],
      "source": [
        "# Compare basic vs advanced TF-Agents implementation\n",
        "print(\"Training Advanced TF-Agents DQN...\")\n",
        "advanced_dqn = AdvancedTFAgentsDQN(use_double_dqn=True)\n",
        "advanced_returns, advanced_losses = advanced_dqn.train()\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "iterations = range(0, len(returns) * 1000, 1000)\n",
        "plt.plot(iterations, returns, 'b-', label='Basic DQN', linewidth=2)\n",
        "plt.plot(iterations, advanced_returns, 'r-', label='Advanced DQN', linewidth=2)\n",
        "plt.axhline(y=195, color='black', linestyle='--', label='Solved')\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Average Return')\n",
        "plt.title('TF-Agents DQN Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "loss_iterations = range(0, len(losses) * 200, 200)\n",
        "plt.plot(loss_iterations, losses, 'b-', label='Basic DQN', linewidth=2)\n",
        "plt.plot(loss_iterations, advanced_losses, 'r-', label='Advanced DQN', linewidth=2)\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nComparison Results:\")\n",
        "print(f\"Basic DQN Final Return: {returns[-1]:.2f}\")\n",
        "print(f\"Advanced DQN Final Return: {advanced_returns[-1]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sG3Vj_mIXZaI"
      },
      "outputs": [],
      "source": [
        "# TF-Agents Benefits Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TF-AGENTS BENEFITS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"✓ Production-Ready Implementation\")\n",
        "print(\"  - Optimized TensorFlow operations\")\n",
        "print(\"  - GPU acceleration support\")\n",
        "print(\"  - Distributed training capabilities\")\n",
        "\n",
        "print(\"\\n✓ Modular Architecture\")\n",
        "print(\"  - Easy to swap components\")\n",
        "print(\"  - Extensible for research\")\n",
        "print(\"  - Clean separation of concerns\")\n",
        "\n",
        "print(\"\\n✓ Advanced Features\")\n",
        "print(\"  - Multiple RL algorithms (DQN, PPO, SAC, etc.)\")\n",
        "print(\"  - Sophisticated replay buffers\")\n",
        "print(\"  - Built-in metrics and logging\")\n",
        "print(\"  - Policy evaluation utilities\")\n",
        "\n",
        "print(\"\\n✓ Robust Implementation\")\n",
        "print(\"  - Handles edge cases\")\n",
        "print(\"  - Numerical stability\")\n",
        "print(\"  - Memory efficient\")\n",
        "print(\"  - Well-tested codebase\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXERCISE COMPLETE!\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}